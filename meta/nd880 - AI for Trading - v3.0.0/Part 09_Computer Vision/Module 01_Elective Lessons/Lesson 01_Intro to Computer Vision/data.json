{
  "data": {
    "lesson": {
      "id": 279926,
      "key": "260fb8ce-eb1d-4ea2-864b-d8ed31b7082f",
      "title": "Intro to Computer Vision",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Learn what computer vision is all about, its applications in the field of artificial and emotional intelligence.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/260fb8ce-eb1d-4ea2-864b-d8ed31b7082f/279926/1544283754495/Intro+to+Computer+Vision+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/260fb8ce-eb1d-4ea2-864b-d8ed31b7082f/279926/1544283750184/Intro+to+Computer+Vision+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 279927,
          "key": "f9c56a94-7db0-496a-a876-8f5060e84216",
          "title": "Welcome to Computer Vision",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f9c56a94-7db0-496a-a876-8f5060e84216",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 336101,
              "key": "8ae4113d-2d1a-402f-a8a4-7cc45f4fb750",
              "title": "Welcome to Computer Vision",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GgA3_-MMT_I",
                "china_cdn_id": "GgA3_-MMT_I.mp4"
              }
            },
            {
              "id": 283368,
              "key": "9c00c894-2c0e-4b8d-8e64-db486b74d47d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Welcome to Computer Vision!\n\nComputer vision is the art and science of perceiving and understanding the world around you through images. Throughout this term, we'll be learning about a variety of computer vision applications. **This lesson will focus on a high-level overview of computer vision systems** and where they fit into the world of artificial intelligence. Later on, we'll be diving into the details of image analysis techniques, programming and computer vision tools and libraries (such as OpenCV), and we'll learn more about the math behind certain applications.\n\nOften, more detail will be provided in text below videos, so make sure to check out these notes as you progress through the videos. Let's get learning!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 279930,
          "key": "2c3ecd48-37d1-48aa-adf2-12e4fee1512f",
          "title": "What is Vision?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2c3ecd48-37d1-48aa-adf2-12e4fee1512f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291788,
              "key": "14f4324e-da28-423c-9533-0375a7b84021",
              "title": "02. What Is Vision?",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_99V1rUNFa4",
                "china_cdn_id": "_99V1rUNFa4.mp4"
              }
            },
            {
              "id": 283367,
              "key": "78a828da-87d2-4867-a2ce-be0285517196",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Vision Systems\n\nVisual perception is the act of observing patterns and objects through sight or visual input, and visual systems allow us to build a model of the physical world.\n\nAs mentioned in the above video, the structure of animal and insect eyes differs based on how their vision systems have evolved and adapted to it's environment and behavior; vision systems change to help a creature fulfill the tasks it needs to survive. ",
              "instructor_notes": ""
            },
            {
              "id": 283363,
              "key": "b147d66e-4359-4632-80dd-eeaf441da872",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Compound Eyes\n\nBees and many other insects have **compound eyes** that consist of multiple lenses (as many as 30,000 lenses in a single compound eye). Each lens is responsible for focusing light and forming a small section of an image; this allows an insect to see a composite image that’s pieced together from the input that each lens receives! If you’d like to learn more about compound eyes, take a look at [this reference](https://www.reference.com/science/compound-eyes-362b8e2642846797#).\n",
              "instructor_notes": ""
            },
            {
              "id": 283364,
              "key": "80fd6fc3-2133-406a-a752-01d7a80eab4d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc6b70_screen-shot-2017-03-17-at-4.03.20-pm/screen-shot-2017-03-17-at-4.03.20-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/80fd6fc3-2133-406a-a752-01d7a80eab4d",
              "caption": "A close up of the compound eyes of a bee; to the right are many tightly-spaced lenses that form a single compound eye.",
              "alt": null,
              "width": 926,
              "height": 334,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 279932,
          "key": "23d5ea6a-f442-4a8d-81e2-e21b0de80c39",
          "title": "Role in AI",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "23d5ea6a-f442-4a8d-81e2-e21b0de80c39",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 331547,
              "key": "cd9e83a5-ecb4-4b58-9b56-cc7d12dd4fbd",
              "title": "03. Role In AI Render",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "xm1TXnNe5Pw",
                "china_cdn_id": "xm1TXnNe5Pw.mp4"
              }
            },
            {
              "id": 283369,
              "key": "3ecc5a76-abb7-4c3a-ab19-bb36a6a71035",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Computer Vision's Role in AI\n\nThe basis of any AI system is that it can: 1) perceive its environment and 2) take actions, based on those perceptions. And computer vision is used to **perceive** and construct a physical model of the world, so that an AI system can then take the appropriate **action**.\n\nIt's important to note that vision is only one aspect of perception. Just think of how you observe the world: through sight, but also through smell, sound, and many other \"sensors\" that humans have. It's the same with AI systems; computer vision is just one - visual - way to perceive physical surroundings.",
              "instructor_notes": ""
            },
            {
              "id": 283374,
              "key": "7ad9b13d-14af-418f-b4db-83eb2526c1c3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### More on the Self-Driving Car\n\nIn the case of self-driving cars, computer vision is used to analyze the visual input from cameras mounted on the car (computer vision is not used to analyze data from other sensors like [radar](https://en.wikipedia.org/wiki/Radar), and [LiDAR](http://oceanservice.noaa.gov/facts/lidar.html), which uses\\ radio waves and lasers respectively). Computer vision is used to look at *images and video data* to intelligently detect lane markings, vehicles, pedestrians, and other elements in the environment, in order to navigate safely!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 279934,
          "key": "6fe47ae8-d86a-4eb2-9d2a-2bc203cda08c",
          "title": "Computer Vision Applications",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6fe47ae8-d86a-4eb2-9d2a-2bc203cda08c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291794,
              "key": "56f2aa62-1bdd-462a-a78e-57d396493b65",
              "title": "04. Computer Vision Applications",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "aFJKp2NltCY",
                "china_cdn_id": "aFJKp2NltCY.mp4"
              }
            },
            {
              "id": 283375,
              "key": "6063210c-bd15-4df9-b92f-7e3cb22b9fe7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Examples of Computer Vision Applications\n\nIn general, computer vision is used in many applications to recognize objects and their behavior. Below are some examples.",
              "instructor_notes": ""
            },
            {
              "id": 283377,
              "key": "e5519352-1b79-430f-b532-08dd70336a0f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "** Self-Driving Car**\n\nComputer vision is used for vehicle and pedestrian recognition and tracking (to see their speed and predict movement).\n\n",
              "instructor_notes": ""
            },
            {
              "id": 283385,
              "key": "a546bd8e-d56b-48a0-9af7-a2053b424a1c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc7aec_screen-shot-2017-03-17-at-5.09.53-pm/screen-shot-2017-03-17-at-5.09.53-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a546bd8e-d56b-48a0-9af7-a2053b424a1c",
              "caption": "Udacity self-driving car",
              "alt": null,
              "width": 300,
              "height": 190,
              "instructor_notes": null
            },
            {
              "id": 283379,
              "key": "1e490e86-2911-4fbd-b347-9eeb571cfa64",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "** Medical Image Analysis and Diagnosis**\n\nAn AI system, using computer vision, can learn to recognize images of cancerous tissue and help with early detection and diagnoses.",
              "instructor_notes": ""
            },
            {
              "id": 283386,
              "key": "89be72b5-7668-4205-8be5-1cc66859de33",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc7b97_screen-shot-2017-03-17-at-5.12.46-pm/screen-shot-2017-03-17-at-5.12.46-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/89be72b5-7668-4205-8be5-1cc66859de33",
              "caption": "Brain MRI, in which a tumor is recognized and colorized in an image",
              "alt": null,
              "width": 380,
              "height": 160,
              "instructor_notes": null
            },
            {
              "id": 283381,
              "key": "2837957b-c32b-44be-9706-839c22736bd2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Photo Tagging and Face Recognition**\n\nComputer vision can be trained to recognize and tag (or label) faces or different features in any given photo library. This is already a feature that many of our phones have!",
              "instructor_notes": ""
            },
            {
              "id": 283391,
              "key": "ac7ec215-5d8f-4b11-a6f8-bbec8b53518c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc7e3f_screen-shot-2017-03-17-at-5.20.52-pm/screen-shot-2017-03-17-at-5.20.52-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ac7ec215-5d8f-4b11-a6f8-bbec8b53518c",
              "caption": "Face recognition with labels for emotions",
              "alt": null,
              "width": 300,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 283384,
              "key": "bf5558a7-db02-49c1-82da-5e8dffea4a62",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "** Image Retrieval**\n\nAs a compliment to image recognition, computer vision is also used to retrieve relevant images based on some search text/given label; this is called image retrieval. For example, searching the term \"sunflower\" in Google images, should return relevant images of sunflowers!",
              "instructor_notes": ""
            },
            {
              "id": 283392,
              "key": "dd8bbfc7-3b72-4d2d-b556-4594673cba5c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc7e7b_screen-shot-2017-03-17-at-5.25.16-pm/screen-shot-2017-03-17-at-5.25.16-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/dd8bbfc7-3b72-4d2d-b556-4594673cba5c",
              "caption": "Image retrieval; searching for images of  a \"sunflower\"",
              "alt": null,
              "width": 300,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 283382,
              "key": "e2eb70c0-b1d9-4a94-9713-66dbcc5eae47",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Automatic Image Captioning**\n\nUsing AI techniques and computer vision, a system can recognize behavior in images and caption it correctly.\n",
              "instructor_notes": ""
            },
            {
              "id": 283387,
              "key": "c3c6f8f6-9b77-476b-b800-69248496e5d9",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58cc7cd2_screen-shot-2017-03-17-at-5.18.10-pm/screen-shot-2017-03-17-at-5.18.10-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c3c6f8f6-9b77-476b-b800-69248496e5d9",
              "caption": "Automatically captioned image; caption reads: \"a man is eating a hot dog in a crowd\"",
              "alt": null,
              "width": 300,
              "height": 220,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 279935,
          "key": "84aa57b8-4949-4b0d-87b3-d3b4a91e5d28",
          "title": "Emotional Intelligence",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "84aa57b8-4949-4b0d-87b3-d3b4a91e5d28",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291805,
              "key": "3f4635a1-22e7-4714-b64f-0466d5fed0ce",
              "title": "05. Emotional Intelligence",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "D_LzJsJH5qk",
                "china_cdn_id": "D_LzJsJH5qk.mp4"
              }
            },
            {
              "id": 283685,
              "key": "94e8eb72-861e-43b8-b97d-81fc332753ca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Cognitive and Emotional Intelligence\n\n**Cognitive intelligence** is the ability to reason and understand the world based on observations and facts. It's often what is measured on academic tests and what's measured to calculate a person's IQ.\n\n**Emotional intelligence** is the ability to understand and influence human emotion. For example, observing that someone looks sad based on their facial expression, body language, and what you know about them - then acting to comfort them or asking them if they want to talk, etc. For humans, this kind of intelligence allows us to form meaningful connections and build a trustworthy network of friends and family. It's also often thought of as *only* a human quality and is not yet a part of traditional AI systems.",
              "instructor_notes": ""
            },
            {
              "id": 279985,
              "key": "b408762f-27c1-4cec-80fb-79241bcd130d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you'd like to learn more about Affectiva and emotion AI, check out [their website](http://www.affectiva.com/).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 279936,
          "key": "477f5ded-97ed-4178-8c44-e8b9fbc662d2",
          "title": "Vision-based Emotion AI",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "477f5ded-97ed-4178-8c44-e8b9fbc662d2",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291806,
              "key": "e85435c6-5678-4766-b4f2-3e310e2a639b",
              "title": "Vision-based Emotion AI",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "7nKKWWn1sAc",
                "china_cdn_id": "7nKKWWn1sAc.mp4"
              }
            }
          ]
        },
        {
          "id": 279937,
          "key": "419af5fa-27bd-49ea-9c07-1abaa0670856",
          "title": "Computer Vision Pipeline",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "419af5fa-27bd-49ea-9c07-1abaa0670856",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291807,
              "key": "ca10cbbd-e692-41f1-be45-90d34b8c283d",
              "title": "08. Computer Vision Pipeline",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "64hFcqhnNow",
                "china_cdn_id": "64hFcqhnNow.mp4"
              }
            },
            {
              "id": 283715,
              "key": "74eaa9fd-8a6e-4d63-9d41-d09abbc49a73",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Computer Vision Pipeline\n\nA computer vision pipeline is a series of steps that most computer vision applications will go through. Many vision applications start off by acquiring images and data, then processing that data, performing some analysis and recognition steps, then finally performing an action. The general pipeline is pictured below!",
              "instructor_notes": ""
            },
            {
              "id": 283716,
              "key": "8f9fe646-26be-4452-9501-77b4d4cec58c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58d03bda_screen-shot-2017-03-13-at-12.36.54-pm/screen-shot-2017-03-13-at-12.36.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8f9fe646-26be-4452-9501-77b4d4cec58c",
              "caption": "General computer vision processing pipeline",
              "alt": null,
              "width": 2134,
              "height": 626,
              "instructor_notes": null
            },
            {
              "id": 814511,
              "key": "f569a9bd-53fb-4b67-a1e0-2f6f4a529599",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now, let's take a look at a specific example of a pipeline applied to facial expression recognition.",
              "instructor_notes": ""
            },
            {
              "id": 614799,
              "key": "b3c7efd9-72f5-46b7-abd6-3464335d8306",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ade67ef_screen-shot-2018-04-23-at-4.10.20-pm/screen-shot-2018-04-23-at-4.10.20-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/b3c7efd9-72f5-46b7-abd6-3464335d8306",
              "caption": "Facial recognition pipeline.",
              "alt": "",
              "width": 1486,
              "height": 802,
              "instructor_notes": null
            },
            {
              "id": 614782,
              "key": "b48f01aa-7b8d-4402-8d94-8e2033aaeed1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Standardizing Data\n\nPre-processing images is all about **standardizing** input images so that you can move further along the pipeline and analyze images in the same way. In machine learning tasks, the pre-processing step is often one of the most important. \n\nFor example, imagine that you've created a simple algorithm to distinguish between stop signs and other traffic lights. ",
              "instructor_notes": ""
            },
            {
              "id": 614797,
              "key": "59351e53-c3bb-469f-b8c0-dc5ce19be6e3",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ade672f_screen-shot-2018-04-23-at-4.05.20-pm/screen-shot-2018-04-23-at-4.05.20-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/59351e53-c3bb-469f-b8c0-dc5ce19be6e3",
              "caption": "Images of traffic signs; a stop sign is on top and a hiking sign is on the bottom.",
              "alt": "",
              "width": 1268,
              "height": 556,
              "instructor_notes": null
            },
            {
              "id": 614798,
              "key": "02ddf8ed-e771-4b2f-b285-b6710ffd5b10",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If the images are different sizes, or even cropped differently, then this counting tactic will likely fail! So, it's important to pre-process these images so that they are standardized before they move along the pipeline. In the example below, you can see that the images are pre-processed into a standard square size.\n\nThe algorithm counts up the number of red pixels in a given image and if there are enough of them, it classifies an image as a stop sign. In this example, we are just extracting a color feature and skipping over selecting an area of interest (we are looking at the _whole_ image). In practice, you'll often see a classification pipeline that looks like this.",
              "instructor_notes": ""
            },
            {
              "id": 614789,
              "key": "9b76370b-bd5e-486b-91af-e459d2ba28e1",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ade6541_stop-sign-classification/stop-sign-classification.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9b76370b-bd5e-486b-91af-e459d2ba28e1",
              "caption": "",
              "alt": "",
              "width": 1292,
              "height": 658,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 280005,
          "key": "da41a87f-41dc-4549-919a-606a8a777a69",
          "title": "Quiz: Pipeline Steps",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "da41a87f-41dc-4549-919a-606a8a777a69",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 280007,
              "key": "622d0f97-1938-4e8c-b139-ec5461f12a4a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Computer Vision Pipeline\n\nA standard computer vision pipeline uses the following steps to extract information from any set of images; eventually going from *perception to action*.",
              "instructor_notes": ""
            },
            {
              "id": 280008,
              "key": "27cbe001-91d8-4665-a38f-e18a1ccb5fd8",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58c6f735_screen-shot-2017-03-13-at-12.36.54-pm/screen-shot-2017-03-13-at-12.36.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/27cbe001-91d8-4665-a38f-e18a1ccb5fd8",
              "caption": "Computer Vision Pipeline",
              "alt": null,
              "width": 2134,
              "height": 626,
              "instructor_notes": null
            },
            {
              "id": 280011,
              "key": "dc887751-2b79-4f42-8a4b-c62b17f1e236",
              "title": "",
              "semantic_type": "MatchingQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "dc887751-2b79-4f42-8a4b-c62b17f1e236",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "complex_prompt": {
                  "text": "These examples are for a facial expression recognition application. This is a computer vision application that can recognize a face and what kind of emotion it is displaying based on features, such as eyebrow angle and mouth curvature. **Match the pipeline step to the correct example.**"
                },
                "concepts_label": "Example",
                "answers_label": "Pipeline step",
                "concepts": [
                  {
                    "text": "Transforming an image from color to grayscale",
                    "correct_answer": {
                      "id": "a1489434450597",
                      "text": "Pre-processing"
                    }
                  },
                  {
                    "text": "Given a face, determining the curvature of the mouth ",
                    "correct_answer": {
                      "id": "a1489434753587",
                      "text": "Feature extraction"
                    }
                  },
                  {
                    "text": "Identifying a face in an image",
                    "correct_answer": {
                      "id": "a1489434799532",
                      "text": "Selecting areas of interest"
                    }
                  },
                  {
                    "text": "Extracting image data from a video stream",
                    "correct_answer": {
                      "id": "a1490047505926",
                      "text": "Input data"
                    }
                  },
                  {
                    "text": "Predicting that a face is expressing anger",
                    "correct_answer": {
                      "id": "a1490047707992",
                      "text": "Prediction/Recognition"
                    }
                  }
                ],
                "answers": [
                  {
                    "id": "a1490047505926",
                    "text": "Input data"
                  },
                  {
                    "id": "a1489434799532",
                    "text": "Selecting areas of interest"
                  },
                  {
                    "id": "a1489434450597",
                    "text": "Pre-processing"
                  },
                  {
                    "id": "a1489434753587",
                    "text": "Feature extraction"
                  },
                  {
                    "id": "a1490047707992",
                    "text": "Prediction/Recognition"
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 279938,
          "key": "8672d5b9-9abd-4baf-a55d-8efa3d052a80",
          "title": "Training a Model",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8672d5b9-9abd-4baf-a55d-8efa3d052a80",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291808,
              "key": "215973d5-e81e-46b5-9461-61932db845fc",
              "title": "09. Training a Model",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "m4GVfwVkj74",
                "china_cdn_id": "m4GVfwVkj74.mp4"
              }
            },
            {
              "id": 283764,
              "key": "c5008e78-8281-4cc0-b52e-8ffe17fad435",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Training a Neural Network\n\nTo train a computer vision neural network, we typically provide sets of **labelled images**, which we can compare to the **predicted output** label or recognition measurements. The neural network then monitors any errors it makes (by comparing the correct label to the output label) and corrects for them by modifying how it finds and prioritizes patterns and differences among the image data. Eventually, given enough labelled data, the model should be able to characterize any new, unlabeled, image data it sees!\n\nA training flow is pictured below. This is a convolutional neural network that *learns* to recognize and distinguish between images of a smile and a smirk.\n\nThis is a very high-level view of training a neural network, and we'll be diving more into how this works later on in this course. For now, we are explaining this so that you'll be able to jump into coding a computer vision application soon!",
              "instructor_notes": ""
            },
            {
              "id": 614800,
              "key": "c93e9449-1234-4a07-8cf8-a553c53b8b32",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ade68dd_screen-shot-2018-04-23-at-4.14.19-pm/screen-shot-2018-04-23-at-4.14.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c93e9449-1234-4a07-8cf8-a553c53b8b32",
              "caption": "Example of a convolutional neural network being trained to distinguish between images of a smile and a smirk.",
              "alt": "",
              "width": 1072,
              "height": 672,
              "instructor_notes": null
            },
            {
              "id": 283767,
              "key": "cc7893d2-6c34-426e-ac42-f8eaddef7735",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Gradient descent** is a a mathematical way to minimize error in a neural network. More information on this minimization method can be found [here](https://en.wikipedia.org/wiki/Gradient_descent).\n\n**Convolutional neural networks** are a specific type of neural network that are commonly used in computer vision applications. They learn to recognize patterns among a given set of images. If you want to learn more, refer to [this resource](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/), and we'll be learning more about these types of networks, and how they work step-by-step, at a different point in this course!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 279939,
          "key": "49948b27-0173-41ee-91ea-61d27e1768aa",
          "title": "AffdexMe Demo",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "49948b27-0173-41ee-91ea-61d27e1768aa",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": {
            "files": [
              {
                "name": "AffdexMe_desktop_demo",
                "uri": "http://video.udacity-data.com.s3.amazonaws.com/topher/2017/June/5941ce66_affdexme.app/affdexme.app.zip"
              }
            ],
            "google_plus_link": null,
            "career_resource_center_link": null,
            "coaching_appointments_link": null,
            "office_hours_link": null,
            "aws_provisioning_link": null
          },
          "atoms": [
            {
              "id": 291810,
              "key": "8c6cf1d3-f2ff-42f7-a2df-bff67b83a868",
              "title": "AffdexMe Demo",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "dpFtXDqakvY",
                "china_cdn_id": "dpFtXDqakvY.mp4"
              }
            },
            {
              "id": 283770,
              "key": "feb8ed16-326e-40e3-ac8d-a15a489ea87f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**AffdexMe** is currently available for download on multiple platforms. \n\nIf you'd like to try this out yourself, you can find the link to download the demo in the Supporting Materials section below!",
              "instructor_notes": ""
            },
            {
              "id": 814518,
              "key": "bacf1acf-b3f0-40ca-b1a4-682221dc6807",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 279940,
          "key": "7db107c2-d658-4dff-ad36-271dac2a11d8",
          "title": "Emotion as a Service",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7db107c2-d658-4dff-ad36-271dac2a11d8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 291812,
              "key": "6e968edc-28c8-4ba1-ba83-161b10c87543",
              "title": "Emotion as a Service",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "2jAP3rP3USM",
                "china_cdn_id": "2jAP3rP3USM.mp4"
              }
            },
            {
              "id": 283771,
              "key": "3c28da38-3235-4183-a46b-80fcdaf4328a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Next, let's incorporate **emotion AI** into our own projects!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 330048,
          "key": "a06449f4-0ca8-4751-88b4-f3538b02d9ce",
          "title": "[Preview] Project: Mimic Me!",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a06449f4-0ca8-4751-88b4-f3538b02d9ce",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 330057,
              "key": "97954abe-b54b-4531-98ac-bb2d71654b64",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Overview\n\nIn this project, you will learn to track faces in a video and identify facial expressions using Affectiva. As a fun visualization, you will tag each face with an appropriate emoji next to it. You will then turn this into a game where the player needs to mimic a random emoji displayed by the computer!\n\n_Ready?_",
              "instructor_notes": ""
            },
            {
              "id": 330075,
              "key": "22ec1fda-355d-4ed0-9d50-cef565e57f10",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58c6ea32_arpan-shocked/arpan-shocked.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/22ec1fda-355d-4ed0-9d50-cef565e57f10",
              "caption": "",
              "alt": null,
              "width": 1280,
              "height": 960,
              "instructor_notes": null
            },
            {
              "id": 330063,
              "key": "14608c04-eaed-4e0a-b405-899a4fa73975",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "---\n\n## Getting Started\n\nClone the [AIND-CV-Mimic](https://github.com/udacity/AIND-CV-Mimic) repo on GitHub, and follow the instructions below.\n\nWe’ll be using [Affectiva](http://www.affectiva.com/)’s Emotion-as-a-Service API for this project. Visit their [Developer Portal](http://developer.affectiva.com/) and try out some of the sample apps. Affectiva makes it really easy to extract detailed information about faces in an image or video stream. To get a sense for what information you can obtain, check out the [Metrics](http://developer.affectiva.com/metrics/) page.\n\n### Project files\n\nTo start working on the project, open the following files in your favorite text editor:\n\n- **mimic.js**: Javascript file with code that connects to the Affectiva API and processes results.\n- **index.html**: Dynamic webpage that displays the video feed and results.\n- **mimic.css**: Stylesheet file that defines the layout and presentation for HTML elements.\n\n_You only need to implement the TODOs in mimic.js to complete the project. But feel free to modify the HTML and/or CSS file to change the look and feel of your game!_\n\nThere are two additional files provided for serving your project as a local web application - you do not need to make any changes to them:\n\n- **serve.py**: A lightweight Python webserver required to serve the webpage over HTTPS, so that we can access the webcam feed.\n- **generate-pemfile.sh**: A shell script you’ll need to run once to generate an SSL certificate for the webserver.\n\n### Serving locally over HTTPS\n\nIn order to access the webcam stream, modern browsers require you to serve your web app over HTTPS. To run locally, you will need to general an SSL certificate (this is a one-time step):\n\n- Open a terminal or command-prompt, and ensure you are inside the `AIND-CV-Mimic/` directory.\n- Run the following shell script: `generate-pemfile.sh`\n\nThis creates an SSL certificate file named `my-ssl-cert.pem` that is used to serve over https.\n\nNow you can launch the server using:\n\n```\npython serve.py\n```\n\n_Note: The `serve.py` script uses Python 3._\n\nAlternately, you can put your HTML, JS and CSS files on an online platform (such as [JSFiddle](https://jsfiddle.net/)) and develop your project there.\n\n### Running and implementing the game\n\nOpen a web browser and go to: [https://localhost:4443/](https://localhost:4443/)\n\n- Hit the Start button to initiate face tracking. You may have to give permission for the app to access your webcam.\n- Hit the Stop button to stop tracking and Reset to reset the detector (in case it becomes stuck or unstable).\n- Modify the Javascript code to implement TODOs as indicated in inline comments. Then refresh the page in your browser (_you may need to do a \"hard-refresh\" for the changes to show up, e.g. `Cmd+Shift+R` on a Mac), or use an auto-reload solution._\n- When you’re done, you can shutdown the server by pressing `Ctrl+C` at the terminal.\n\n_Note: Your browser may notify you that your connection is not secure - that is because the SSL certificate you just created is not signed by an SSL Certificate Authority‎. This is okay, because we are using it only as a workaround to access the webcam. You can suppress the warning or choose \"Proceed Anyway\" to open the page._\n\nAs you work on your code, you may have to refer to resources in Affectiva's [JS SDK documentation](https://affectiva.readme.io/docs/getting-started-with-the-emotion-sdk-for-javascript).",
              "instructor_notes": ""
            },
            {
              "id": 330073,
              "key": "ee72f07d-d779-42b4-969e-c26817e22240",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "---\n\n## Tasks\n\nThe starter code sends frames from your webcam to Affectiva’s cloud-based API and fetches the results. You can see several metrics being reported, including emotions, expressions and the dominant emoji!",
              "instructor_notes": ""
            },
            {
              "id": 330076,
              "key": "ee364513-2d89-40f3-99c7-beff31cd7713",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58c6ef46_arpan-happy-results/arpan-happy-results.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ee364513-2d89-40f3-99c7-beff31cd7713",
              "caption": "",
              "alt": null,
              "width": 2200,
              "height": 960,
              "instructor_notes": null
            },
            {
              "id": 330078,
              "key": "e2a84502-8740-4817-9d8d-e03cd884e50c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### 1. Display Feature Points\n\nYour first task is to display the feature points on top of the webcam image that are returned along with the metrics. To do this, open up mimic.js, and scroll down to the `drawFeaturePoints()` function near the bottom:\n\n```javascript\nfunction drawFeaturePoints(canvas, img, face) {\n    ...\n}\n```\n\nIt accepts three parameters: `canvas` (HTML DOM element to draw on), `img` (image frame that was processed), and `face` (an object with all the detected feature points and metrics for a face). The most important object to consider here is `face` - you can even print it to the Javascript console using `console.log()` to see what it contains (tip: the console in your web browser maybe under Developer Tools; in Chrome you can open it using `Cmd+Option+J`).\n\nImplement the `drawFeaturePoints()` function as per instructions and save the file. To run your code, stop the face tracking app using the Stop button (if you haven’t done so already), refresh the page and hit Start again.\n\nYour output should look something like this:",
              "instructor_notes": ""
            },
            {
              "id": 330079,
              "key": "3ec189b4-56e4-4e16-ac0c-656370ceaa8c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58c6eff8_arpan-happy-features/arpan-happy-features.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3ec189b4-56e4-4e16-ac0c-656370ceaa8c",
              "caption": "",
              "alt": null,
              "width": 2200,
              "height": 960,
              "instructor_notes": null
            },
            {
              "id": 330080,
              "key": "93118de5-2693-4970-a87b-38f426a30b45",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### 2. Show Dominant Emoji\n\nIn addition to feature points and metrics that capture facial expressions and emotions, the Affectiva API also reports back what emoji best represents the current emotional state of a face. This is referred to as the _dominant emoji_. In mimic.js, scroll down to the `drawEmoji()` function:\n\n```javascript\nfunction drawEmoji(canvas, img, face) {\n    ...\n}\n```\n\nIt has the same interface as `drawFeaturePoints()`, accepting the same `canvas`, `img` and `face` objects. Implement it as per given instructions. You can access the dominant emoji as: `face.emojis.dominantEmoji`\n\nYour output should now look like:",
              "instructor_notes": ""
            },
            {
              "id": 330082,
              "key": "cbb37331-4530-4180-89d7-5484c828a0ff",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/March/58c6f08b_arpan-happy-emoji/arpan-happy-emoji.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cbb37331-4530-4180-89d7-5484c828a0ff",
              "caption": "",
              "alt": null,
              "width": 2200,
              "height": 960,
              "instructor_notes": null
            },
            {
              "id": 330083,
              "key": "36fce3db-7ddf-4698-9279-813ca162c06b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Note that the emoji should be located close to the face, so you may want to use one of the facial feature points from the step above as the anchor point (or a combination of them). You can also try varying the size of the emoji based on how big the detected face is (you can compute this, for instance, by looking at the distance between two opposing feature points).\n\n### 3. Implement Mimic Me!\n\nNow it's your turn to implement the game mechanics and make it as fun as possible! Scroll down to the bottom of mimic.js for more instructions. Feel free to modify the HTML and/or CSS files to change the look and feel of the game as well.\n\nIn this game, the computer should display an emoji at random, and the goal of the human player would be to mimic that emoji as best as they can. The computer should continually monitor the player’s face, and as soon as they are able to mimic the face (or optionally after some timeout), the game should move on to the next random emoji.\n\nAffectiva's SDK can recognize 13 different emojis. The unicode values for these emojis are provided as a list in mimic.js:\n\n```javascript\n// Unicode values for all emojis Affectiva can detect\nvar emojis = [ 128528, 9786, ..., 128561 ];\n```\n\nEach value corresponds to an HTML entity, e.g. `&#128527;` for a smirk: &#128527;\n\nTo display a desired emoji as the next target for the player, you can use `setTargetEmoji()`, passing in the respective value. The dominant emoji returned by Affectiva is supplied in a Javascript string. In order to reliably compare it with the desired emoji code, you will need to convert it to unicode using `toUnicode()`. Both of these are provided as utility functions.\n\nSome things to keep in mind:\n\n- You can keep track of how many emojis the player is able to mimic in a certain amount of time (say, 1 minute) and give them a score based on that, or show a set number of emojis in a sequence and see how many they are able to mimic within a timeout for each.\n- Remember to give the API some time to warm up and find the face before you start showing the emojis.\n- Provide some audio/visual feedback whenever the player is able to mimic an emoji successfully.\n- You may want to limit the possible emojis to a small set that you know that the system can recognize reliably (or ones that you can actually express!).",
              "instructor_notes": ""
            },
            {
              "id": 330084,
              "key": "117f61b0-83d6-4d7d-b8fd-d941259f91cc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "---\n\n## Extensions\n\nSky’s the limit on where you can take this project! Feel free to share with your friends and family. You can host it online to make it available to everyone.\n\nSome ideas for extensions:\n\n- Make it a 2 player game, like Guitar Hero, where you compete with someone to mimic as many emojis as you can out of a streaming sequence of them.\n- Pair a stream of emojis with a script and have the player read the script, interspersed with emotional expressions that are checked by the computer. Great for some acting practice!",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}