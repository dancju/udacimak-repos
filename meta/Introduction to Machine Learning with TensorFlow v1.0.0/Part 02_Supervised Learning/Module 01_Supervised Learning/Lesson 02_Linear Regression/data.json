{
  "data": {
    "lesson": {
      "id": 909511,
      "key": "6b653b4d-38c3-44e0-a0f1-4ddb2005305a",
      "title": "Linear Regression",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Linear regression is one of the most fundamental algorithms in machine learning. In this lesson, learn how linear regression works!",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/6b653b4d-38c3-44e0-a0f1-4ddb2005305a/909511/1581973718981/Linear+Regression+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/6b653b4d-38c3-44e0-a0f1-4ddb2005305a/909511/1581973713332/Linear+Regression+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 439309,
          "key": "10a4b9d5-623c-474e-be81-fb1f33d08ece",
          "title": "Intro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "10a4b9d5-623c-474e-be81-fb1f33d08ece",
            "completed_at": "2020-04-14T20:27:46.023Z",
            "last_viewed_at": "2020-04-14T20:27:44.326Z",
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461326,
              "key": "d8ea4e66-7264-4e13-815f-698fbbd41836",
              "title": "Welcome To Linear Regression",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "zxZkTkM34BY",
                "china_cdn_id": "zxZkTkM34BY.mp4"
              }
            }
          ]
        },
        {
          "id": 439310,
          "key": "12f6331c-0bb3-48e2-b0a2-31f852e117ca",
          "title": "Quiz: Housing Prices",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "12f6331c-0bb3-48e2-b0a2-31f852e117ca",
            "completed_at": "2020-04-14T20:27:49.042Z",
            "last_viewed_at": "2020-04-14T20:27:47.431Z",
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461327,
              "key": "917c5e8c-e470-4543-9a57-c50c9d20012e",
              "title": "DLND REG 01 Quiz Housing Prices V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "8CSBiVKu35Q",
                "china_cdn_id": "8CSBiVKu35Q.mp4"
              }
            },
            {
              "id": 462406,
              "key": "83656a98-033b-4e33-9bf8-8d2fcbba31bc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a0a88f8_house/house.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/83656a98-033b-4e33-9bf8-8d2fcbba31bc",
              "caption": "",
              "alt": "",
              "width": 1895,
              "height": 1043,
              "instructor_notes": null
            },
            {
              "id": 439818,
              "key": "d85f7e02-fe9e-4e77-9c21-5fa0932584c4",
              "title": "Housing Price Quiz",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "d85f7e02-fe9e-4e77-9c21-5fa0932584c4",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the best estimate for the price of the house?",
                "answers": [
                  {
                    "id": "a1508882980407",
                    "text": "$80,000",
                    "is_correct": false
                  },
                  {
                    "id": "a1508882997712",
                    "text": "$120,000",
                    "is_correct": true
                  },
                  {
                    "id": "a1508882998363",
                    "text": "$190,000",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 439311,
          "key": "eb2ba02e-e89f-4973-825d-5eff662071fa",
          "title": "Solution: Housing Prices",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "eb2ba02e-e89f-4973-825d-5eff662071fa",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461328,
              "key": "dcb248c5-9667-4a9c-85ab-3c919e8c6b50",
              "title": "Solution  Housing Prices",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "uhdTulw9-Nc",
                "china_cdn_id": "uhdTulw9-Nc.mp4"
              }
            }
          ]
        },
        {
          "id": 439314,
          "key": "e5d39837-1c4d-49c2-9884-97edcec7fb4a",
          "title": "Fitting a Line Through Data",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e5d39837-1c4d-49c2-9884-97edcec7fb4a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461329,
              "key": "7db47c10-8635-4663-8fff-f1638ea3d0b0",
              "title": "Fitting A Line",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "gkdoknEEcaI",
                "china_cdn_id": "gkdoknEEcaI.mp4"
              }
            }
          ]
        },
        {
          "id": 439312,
          "key": "c10e0bdd-6099-4923-982f-0d40c0d86302",
          "title": "Moving a Line",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c10e0bdd-6099-4923-982f-0d40c0d86302",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461330,
              "key": "c6775f68-1192-4eaa-8edf-5d164393732d",
              "title": "Moving A Line",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "8EIHFyL2Log",
                "china_cdn_id": "8EIHFyL2Log.mp4"
              }
            }
          ]
        },
        {
          "id": 439313,
          "key": "73940af7-d358-4b4a-b854-02f2888ded01",
          "title": "Absolute Trick",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "73940af7-d358-4b4a-b854-02f2888ded01",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461555,
              "key": "726a5e2f-4918-4a66-9868-cc7d0e3b4df2",
              "title": "Absolute Trick",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "DJWjBAqSkZw",
                "china_cdn_id": "DJWjBAqSkZw.mp4"
              }
            }
          ]
        },
        {
          "id": 439315,
          "key": "229a6078-0e20-4b83-8bb6-a4fe6114df04",
          "title": "Square Trick",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "229a6078-0e20-4b83-8bb6-a4fe6114df04",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461332,
              "key": "dea2ebba-00a5-480b-b482-6751d3f1ef35",
              "title": "Square Trick",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "AGZEq-yQgRM",
                "china_cdn_id": "AGZEq-yQgRM.mp4"
              }
            }
          ]
        },
        {
          "id": 608339,
          "key": "d0613474-b71f-435e-8ef7-730cab6b7790",
          "title": "Quiz: Absolute and Square Trick",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d0613474-b71f-435e-8ef7-730cab6b7790",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 608340,
              "key": "f163eba0-ef8b-4168-99fb-d51a02ca6af8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Quiz for \"Absolute Trick\"**",
              "instructor_notes": ""
            },
            {
              "id": 613082,
              "key": "0efe0d5a-5427-4b0a-933f-2abe43688b4c",
              "title": "",
              "semantic_type": "ValidatedQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "0efe0d5a-5427-4b0a-933f-2abe43688b4c",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Let's say that we have a line whose equation is **y = -0.6x + 4**. For the point **(x,y) = (-5, 3)**, apply the **absolute trick** to get the new equation for the line, using a learning rate of <span class=\"mathquill\">alpha = 0.1</span>.\n\nReport your answer in the form **y = w_1x + w_2**, substituting appropriate values for **w_1** and **w_2**.",
                "matchers": [
                  {
                    "expression": "y\\s*=\\s*-0?.1x\\s*\\+\\s*3.9"
                  },
                  {
                    "expression": "y\\s*=\\s*-1.1x\\s*\\+\\s*3.9"
                  },
                  {
                    "expression": "y\\s*=\\s*-1.1x\\s*\\+\\s*4.1"
                  },
                  {
                    "expression": "y\\s*=\\s*-0?.1x\\s*\\+\\s*4.1"
                  }
                ]
              }
            },
            {
              "id": 613095,
              "key": "af748fef-5beb-497e-8840-fd54beb4ffff",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "** Quiz for \"Square Trick\"**",
              "instructor_notes": ""
            },
            {
              "id": 613123,
              "key": "590acec4-8ca3-4f28-98e7-f38bbaf7b6b2",
              "title": "",
              "semantic_type": "ValidatedQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "590acec4-8ca3-4f28-98e7-f38bbaf7b6b2",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Let's say that we have a line whose equation is **y = -0.6x + 4**. For the point **(x,y) = (-5, 3)**, apply the **square trick** to get the new equation for the line, using a learning rate of <span class=\"mathquill\">alpha = 0.01</span>.\n\nReport your answer in the form **y = w_1x + w_2**, substituting appropriate values for **w_1** and **w_2**.",
                "matchers": [
                  {
                    "expression": "y\\s*=\\s*-0?.4x\\s*\\+\\s*3.96"
                  },
                  {
                    "expression": "y\\s*=\\s*-0?.8x\\s*\\+\\s*3.96"
                  },
                  {
                    "expression": "y\\s*=\\s*-0?.4x\\s*\\+\\s*4.04"
                  },
                  {
                    "expression": "y\\s*=\\s*-0?.8x\\s*\\+\\s*4.04"
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 543042,
          "key": "ec3e518c-19b3-4b3c-a711-89dfb2347265",
          "title": "Gradient Descent",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ec3e518c-19b3-4b3c-a711-89dfb2347265",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 543043,
              "key": "376e0bae-d0c3-4fc9-a888-807b627d0890",
              "title": "Gradient Descent",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "4s4x9h6AN5Y",
                "china_cdn_id": "4s4x9h6AN5Y.mp4"
              }
            }
          ]
        },
        {
          "id": 439318,
          "key": "6bc778b2-3c29-4b29-a59d-4522c9ae0f71",
          "title": "Mean Absolute Error",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6bc778b2-3c29-4b29-a59d-4522c9ae0f71",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461334,
              "key": "25e3eb1b-03ec-4488-bdfb-5240cedbec79",
              "title": "Mean Absolute Error",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "vLKiY0Ehors",
                "china_cdn_id": "vLKiY0Ehors.mp4"
              }
            }
          ]
        },
        {
          "id": 439319,
          "key": "e38f1034-2e10-4c52-85bf-e2889da458eb",
          "title": "Mean Squared Error",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e38f1034-2e10-4c52-85bf-e2889da458eb",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461335,
              "key": "a2fee92f-a9ba-4536-b7a0-fc083cc901df",
              "title": "Mean Squared Error",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "MRyxmZDngI4",
                "china_cdn_id": "MRyxmZDngI4.mp4"
              }
            }
          ]
        },
        {
          "id": 613124,
          "key": "9958b882-f9d7-42a4-9f7a-61e32ff530f5",
          "title": "Quiz: Mean Absolute & Squared Errors",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9958b882-f9d7-42a4-9f7a-61e32ff530f5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 613125,
              "key": "2afce90b-0267-4477-9be4-b6c09645156e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Quiz for Mean Absolute Error**",
              "instructor_notes": ""
            },
            {
              "id": 613151,
              "key": "9ac702fe-992a-46e3-9b95-8851ca63c719",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "9ac702fe-992a-46e3-9b95-8851ca63c719",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "prompt": "Compute the **mean absolute error** for the following line and points:\n\nline: y = 1.2x + 2\n\npoints: (2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)",
                "semantic_type": "CodeGradedQuestion",
                "evaluation_id": "5247166273159168"
              },
              "answer": null
            },
            {
              "id": 613156,
              "key": "c34a5ecb-9141-4178-91bc-4819a4fbfb96",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Quiz for Mean Squared Error**",
              "instructor_notes": ""
            },
            {
              "id": 613157,
              "key": "c8f9100c-a525-4800-89de-495e6ecb0334",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "c8f9100c-a525-4800-89de-495e6ecb0334",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "prompt": "Compute the **mean squared error** for the following line and points:\n\nline: y = 1.2x + 2\n\npoints: (2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)",
                "semantic_type": "CodeGradedQuestion",
                "evaluation_id": "6709963716689920"
              },
              "answer": null
            }
          ]
        },
        {
          "id": 439320,
          "key": "5b5b3ff6-5d7c-409f-ac2c-6b6652288ce5",
          "title": "Minimizing Error Functions",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5b5b3ff6-5d7c-409f-ac2c-6b6652288ce5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461336,
              "key": "f30616cd-1d32-4e8e-8a1e-3e3654ec6686",
              "title": "Minimizing Error Functions",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "_NOTE:_ From 2:22 onward, the slide title should say \"Mean Absolute Error\".",
              "video": {
                "youtube_id": "RbT2TXN_6tY",
                "china_cdn_id": "RbT2TXN_6tY.mp4"
              }
            },
            {
              "id": 477818,
              "key": "dd44f7a8-ddd2-4fe9-8073-4cddcebf2393",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Development of the derivative of the error function\n\nNotice that we've defined the squared error to be\n\n<span class=\"mathquill\">\nError = \\frac{1}{2} (y - \\hat{y})^2.\n</span>\n\nAlso, we've defined the prediction to be\n\n<span class=\"mathquill\">\n\\hat{y} = w_1 x + w_2.\n</span>\n\nSo to calculate the derivative of the Error with respect to\n<span class=\"mathquill\">\nw_1\n</span>\n, we simply use the chain rule:\n\n<span class=\"mathquill\">\n\\frac{\\partial}{\\partial w_1} Error = \\frac{\\partial Error}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_i}. \n</span>\n\nThe first factor of the right hand side is the derivative of the Error with respect to the prediction\n<span class=\"mathquill\">\n\\hat{y}\n</span>, which is\n<span class=\"mathquill\">\n-(y-\\hat{y}).\n</span>\n\nThe second factor is the derivative of the prediction with respect to\n<span class=\"mathquill\">\nw_1\n</span>, which is simply\n<span class=\"mathquill\">\nx\n</span>.\n\nTherefore, the derivative is",
              "instructor_notes": ""
            },
            {
              "id": 477821,
              "key": "f1aea651-ab46-4ba6-9162-a9292216586d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a3068ed_gif-1/gif-1.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f1aea651-ab46-4ba6-9162-a9292216586d",
              "caption": "",
              "alt": "",
              "width": 220,
              "height": 51,
              "instructor_notes": null
            },
            {
              "id": 477823,
              "key": "6e09eb77-4bdc-4ca7-92d5-11d61e330c10",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Exercise\nCalculate the derivative of the Error with respect to\n<span class=\"mathquill\">\nw_2</span>\nand verify that it is precisely\n<span class=\"mathquill\">\n-(y-\\hat{y}).\n</span>\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 464528,
          "key": "fba8dd74-5e19-4d6d-b8af-f5a6580e7870",
          "title": "Mean vs Total Error",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fba8dd74-5e19-4d6d-b8af-f5a6580e7870",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 464529,
              "key": "76ae754b-006d-41a7-8e25-9865199b2ddd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Mean vs Total Squared (or Absolute) Error\n\nA potential confusion is the following: How do we know if we should use the mean or the total squared (or absolute) error?\n\nThe total squared error is the sum of errors at each point, given by the following equation:\n\n<span class=\"mathquill\">\nM = \\sum_{i=1}^m \\frac{1}{2} (y - \\hat{y})^2,\n</span>\n\nwhereas the mean squared error is the average of these errors, given by the equation, where\n<span class=\"mathquill\">\nm\n</span>\nis the number of points:\n\n<span class=\"mathquill\">\nT = \\sum_{i=1}^m \\frac{1}{2m}(y - \\hat{y})^2.\n</span>\n\nThe good news is, it doesn't really matter. As we can see, the total squared error is just a multiple of the mean squared error, since\n\n<span class=\"mathquill\">\nM = mT.\n</span>\n\nTherefore, since derivatives are linear functions, the gradient of\n<span class=\"mathquill\">\nT\n</span>\nis also\n<span class=\"mathquill\">\nm\n</span>\ntimes the gradient of\n<span class=\"mathquill\">\nM\n</span>.\n\nHowever, the gradient descent step consists of subtracting the gradient of the error times the learning rate\n<span class=\"mathquill\">\n\\alpha\n</span>. Therefore, choosing between the mean squared error and the total squared error really just amounts to picking a different learning rate.\n\nIn real life, we'll have algorithms that will help us determine a good learning rate to work with. Therefore, if we use the mean error or the total error, the algorithm will just end up picking a different learning rate.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 464510,
          "key": "1b0ed370-9d37-4b2f-a8e9-d4082ec73b77",
          "title": "Mini-batch Gradient Descent",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1b0ed370-9d37-4b2f-a8e9-d4082ec73b77",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 464522,
              "key": "4ab96907-76b0-471d-9692-90897f254ca3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Batch vs Stochastic Gradient Descent\n\nAt this point, it seems that we've seen two ways of doing linear regression.\n- By applying the squared (or absolute) trick at every point in our data _one by one_, and repeating this process many times.\n- By applying the squared (or absolute) trick at every point in our data _all at the same time_, and repeating this process many times.\n\nMore specifically, the squared (or absolute) trick, when applied to a point, gives us some values to add to the weights of the model. We can add these values, update our weights, and then apply the squared (or absolute) trick on the next point. Or we can calculate these values for all the points, add them, and then update the weights with the sum of these values.\n\nThe latter is called _batch gradient descent_. The former is called _stochastic gradient descent_. ",
              "instructor_notes": ""
            },
            {
              "id": 464525,
              "key": "51ab3801-33ae-4561-8c84-676158d23ca7",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a151a19_batch-stochastic/batch-stochastic.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/51ab3801-33ae-4561-8c84-676158d23ca7",
              "caption": "",
              "alt": "",
              "width": 2560,
              "height": 971,
              "instructor_notes": null
            },
            {
              "id": 464526,
              "key": "e5766220-8cf8-4f64-914b-059fd2c5d76a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The question is, which one is used in practice?\n\nActually, in most cases, neither. Think about this: If your data is huge, both are a bit slow, computationally. The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called _mini-batch gradient descent_.",
              "instructor_notes": ""
            },
            {
              "id": 464534,
              "key": "773b018c-539d-4d08-a272-e7a32f1cded4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a152b4b_minibatch/minibatch.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/773b018c-539d-4d08-a272-e7a32f1cded4",
              "caption": "",
              "alt": "",
              "width": 1962,
              "height": 1244,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 613158,
          "key": "2d949317-4987-48f5-a81a-2a8113a4ca3e",
          "title": "Quiz: Mini-Batch Gradient Descent",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2d949317-4987-48f5-a81a-2a8113a4ca3e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 613159,
              "key": "4e17c049-0477-40c6-92b5-dba2fd498f44",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Programming Quiz for \"Mini-Batch Gradient Descent\"**\n\n## Mini-Batch Gradient Descent Quiz\n\nIn this quiz, you'll be given the following sample dataset (as in data.csv), and your goal is to write a function that executes mini-batch gradient descent to find a best-fitting regression line. You might consider looking into numpy's `matmul` function for this!",
              "instructor_notes": ""
            },
            {
              "id": 615403,
              "key": "dad998f4-2985-4d8e-a2fa-c5290df567f0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5adfa32d_l2-gradient-descent-data/l2-gradient-descent-data.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/dad998f4-2985-4d8e-a2fa-c5290df567f0",
              "caption": "",
              "alt": "",
              "width": 432,
              "height": 288,
              "instructor_notes": null
            },
            {
              "id": 613162,
              "key": "be1f4673-3015-4353-889a-42b082ece356",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "be1f4673-3015-4353-889a-42b082ece356",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6721257467412480",
                "initial_code_files": [
                  {
                    "text": "import numpy as np\n# Setting a random seed, feel free to change it and see different solutions.\nnp.random.seed(42)\n\n\n# TODO: Fill in code in the function below to implement a gradient descent\n# step for linear regression, following a squared error rule. See the docstring\n# for parameters and returned variables.\ndef MSEStep(X, y, W, b, learn_rate = 0.005):\n    \"\"\"\n    This function implements the gradient descent step for squared error as a\n    performance metric.\n    \n    Parameters\n    X : array of predictor features\n    y : array of outcome values\n    W : predictor feature coefficients\n    b : regression function intercept\n    learn_rate : learning rate\n\n    Returns\n    W_new : predictor feature coefficients following gradient descent step\n    b_new : intercept following gradient descent step\n    \"\"\"\n    \n    # Fill in code\n    \n    return W_new, b_new\n\n\n# The parts of the script below will be run when you press the \"Test Run\"\n# button. The gradient descent step will be performed multiple times on\n# the provided dataset, and the returned list of regression coefficients\n# will be plotted.\ndef miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):\n    \"\"\"\n    This function performs mini-batch gradient descent on a given dataset.\n\n    Parameters\n    X : array of predictor features\n    y : array of outcome values\n    batch_size : how many data points will be sampled for each iteration\n    learn_rate : learning rate\n    num_iter : number of batches used\n\n    Returns\n    regression_coef : array of slopes and intercepts generated by gradient\n      descent procedure\n    \"\"\"\n    n_points = X.shape[0]\n    W = np.zeros(X.shape[1]) # coefficients\n    b = 0 # intercept\n    \n    # run iterations\n    regression_coef = [np.hstack((W,b))]\n    for _ in range(num_iter):\n        batch = np.random.choice(range(n_points), batch_size)\n        X_batch = X[batch,:]\n        y_batch = y[batch]\n        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n        regression_coef.append(np.hstack((W,b)))\n    \n    return regression_coef\n\n\nif __name__ == \"__main__\":\n    # perform gradient descent\n    data = np.loadtxt('data.csv', delimiter = ',')\n    X = data[:,:-1]\n    y = data[:,-1]\n    regression_coef = miniBatchGD(X, y)\n    \n    # plot the results\n    import matplotlib.pyplot as plt\n    \n    plt.figure()\n    X_min = X.min()\n    X_max = X.max()\n    counter = len(regression_coef)\n    for W, b in regression_coef:\n        counter -= 1\n        color = [1 - 0.92 ** counter for _ in range(3)]\n        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)\n    plt.scatter(X, y, zorder = 3)\n    plt.show()",
                    "name": "batch_graddesc.py"
                  },
                  {
                    "text": "-0.72407,2.23863\n-2.40724,-0.00156\n2.64837,3.01665\n0.36092,2.31019\n0.67312,2.05950\n-0.45460,1.24736\n2.20168,2.82497\n1.15605,2.21802\n0.50694,1.43644\n-0.85952,1.74980\n-0.59970,1.63259\n1.46804,2.43461\n-1.05659,1.02226\n1.29177,3.11769\n-0.74565,0.81194\n0.15033,2.81910\n-1.49627,0.53105\n-0.72071,1.64845\n0.32924,1.91416\n-0.28053,2.11376\n-1.36115,1.70969\n0.74678,2.92253\n0.10621,3.29827\n0.03256,1.58565\n-0.98290,2.30455\n-1.15661,1.79169\n0.09024,1.54723\n-1.03816,1.06893\n-0.00604,1.78802\n0.16278,1.84746\n-0.69869,1.58732\n1.03857,1.94799\n-0.11783,3.09324\n-0.95409,1.86155\n-0.81839,1.88817\n-1.28802,1.39474\n0.62822,1.71526\n-2.29674,1.75695\n-0.85601,1.12981\n-1.75223,1.67000\n-1.19662,0.66711\n0.97781,3.11987\n-1.17110,0.56924\n0.15835,2.28231\n-0.58918,1.23798\n-1.79678,1.35803\n-0.95727,1.75579\n0.64556,1.91470\n0.24625,2.33029\n0.45917,3.25263\n1.21036,2.07602\n-0.60116,1.54254\n0.26851,2.79202\n0.49594,1.96178\n-2.67877,0.95898\n0.49402,1.96690\n1.18643,3.06144\n-0.17741,1.85984\n0.57938,1.82967\n-2.14926,0.62285\n2.27700,3.63838\n-1.05695,1.11807\n1.68288,2.91735\n-1.53513,1.99668\n0.00099,1.76149\n0.45520,2.31938\n-0.37855,0.90172\n1.35638,3.49432\n0.01763,1.87838\n2.21725,2.61171\n-0.44442,2.06623\n0.89583,3.04041\n1.30499,2.42824\n0.10883,0.63190\n1.79466,2.95265\n-0.00733,1.87546\n0.79862,3.44953\n-0.12353,1.53740\n-1.34999,1.59958\n-0.67825,1.57832\n-0.17901,1.73312\n0.12577,2.00244\n1.11943,2.08990\n-3.02296,1.09255\n0.64965,1.28183\n1.05994,2.32358\n0.53360,1.75136\n-0.73591,1.43076\n-0.09569,2.81376\n1.04694,2.56597\n0.46511,2.36401\n-0.75463,2.30161\n-0.94159,1.94500\n-0.09314,1.87619\n-0.98641,1.46602\n-0.92159,1.21538\n0.76953,2.39377\n0.03283,1.55730\n-1.07619,0.70874\n0.20174,1.76894",
                    "name": "data.csv"
                  },
                  {
                    "text": "def MSEStep(X, y, W, b, learn_rate = 0.001):\n    \"\"\"\n    This function implements the gradient descent step for squared error as a\n    performance metric.\n    \n    Parameters\n    X : array of predictor features\n    y : array of outcome values\n    W : predictor feature coefficients\n    b : regression function intercept\n    learn_rate : learning rate\n\n    Returns\n    W_new : predictor feature coefficients following gradient descent step\n    b_new : intercept following gradient descent step\n    \"\"\"\n    \n    # compute errors\n    y_pred = np.matmul(X, W) + b\n    error = y - y_pred\n    \n    # compute steps\n    W_new = W + learn_rate * np.matmul(error, X)\n    b_new = b + learn_rate * error.sum()\n    return W_new, b_new",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 439321,
          "key": "229237eb-f9fc-4d62-b708-561f162dd857",
          "title": "Absolute Error vs Squared Error",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "229237eb-f9fc-4d62-b708-561f162dd857",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461337,
              "key": "b0a83885-cd4a-4157-8b60-059fc2728a31",
              "title": "Absolute Vs Squared Error",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "csvdjaqt1GM",
                "china_cdn_id": "csvdjaqt1GM.mp4"
              }
            },
            {
              "id": 462404,
              "key": "8f4cde33-5e60-4cb1-9b3c-5b1259ec415f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a0a8887_quiz/quiz.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8f4cde33-5e60-4cb1-9b3c-5b1259ec415f",
              "caption": "",
              "alt": "",
              "width": 961,
              "height": 750,
              "instructor_notes": null
            },
            {
              "id": 439867,
              "key": "7b640d2f-a950-4d52-9e06-3bb28a9fd6dc",
              "title": "Absolute Error Quiz",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "7b640d2f-a950-4d52-9e06-3bb28a9fd6dc",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the three lines gives you a smaller Mean Absolute Error?",
                "answers": [
                  {
                    "id": "a1508889763921",
                    "text": "A",
                    "is_correct": false
                  },
                  {
                    "id": "a1508889784462",
                    "text": "B",
                    "is_correct": false
                  },
                  {
                    "id": "a1508889788646",
                    "text": "C",
                    "is_correct": false
                  },
                  {
                    "id": "a1508889790902",
                    "text": "They all give the same error",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 461338,
              "key": "abb5abe3-9d65-447e-a644-07efde275162",
              "title": "DLND REG 12 Absolute Vs Squared Error 2 V1 (1)",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "7El1OH17Oi4",
                "china_cdn_id": "7El1OH17Oi4.mp4"
              }
            },
            {
              "id": 462405,
              "key": "47afdcd8-800e-4886-9239-10fdcfbacad2",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a0a8887_quiz/quiz.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/47afdcd8-800e-4886-9239-10fdcfbacad2",
              "caption": "",
              "alt": "",
              "width": 961,
              "height": 750,
              "instructor_notes": null
            },
            {
              "id": 439871,
              "key": "1cdc459d-2797-458b-8015-114981c61d50",
              "title": "Squared Error quiz",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "1cdc459d-2797-458b-8015-114981c61d50",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the three lines gives you a smaller Mean Squared Error?",
                "answers": [
                  {
                    "id": "a1508889876971",
                    "text": "A",
                    "is_correct": false
                  },
                  {
                    "id": "a1508889902337",
                    "text": "B",
                    "is_correct": true
                  },
                  {
                    "id": "a1508889903817",
                    "text": "C",
                    "is_correct": false
                  },
                  {
                    "id": "a1508889905281",
                    "text": "They all give the same error",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 461339,
              "key": "0ba86c77-add6-4d99-9172-e1bb33f6a83c",
              "title": "DLND REG 13 Absolute Vs Squared Error 3 V1 (1)",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "bIVGf_dDkrY",
                "china_cdn_id": "bIVGf_dDkrY.mp4"
              }
            }
          ]
        },
        {
          "id": 464504,
          "key": "7ae99885-7fe4-4b29-8690-f381d2439b79",
          "title": "Linear Regression in scikit-learn",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7ae99885-7fe4-4b29-8690-f381d2439b79",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 464505,
              "key": "58e34971-2eff-4ddb-9ab6-5ef83feeaabe",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Linear Regression\n\nIn this section, you'll use linear regression to predict life expectancy from [body mass index (BMI)](https://en.wikipedia.org/wiki/Body_mass_index).  Before you do that, let's go over the tools required to build this model.\n\nFor your linear regression model, you'll be using scikit-learn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class.  This class provides the function [`fit()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) to fit the model to your data.\n\n```python\n>>> from sklearn.linear_model import LinearRegression\n>>> model = LinearRegression()\n>>> model.fit(x_values, y_values)\n```\n\nIn the example above, the `model` variable is a linear regression model that has been fitted to the data `x_values` and `y_values`.  Fitting the model means finding the best line that fits the training data.  Let's make two predictions using the model's [`predict()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) function.\n\n```python\n>>> print(model.predict([ [127], [248] ]))\n[[ 438.94308857, 127.14839521]]\n```\n\nThe model returned an array of predictions, one prediction for each input array.  The first input, `[127]`, got a prediction of `438.94308857`.  The second input, `[248]`, got a prediction of `127.14839521`.  The reason for predicting on an array like `[127]` and not just `127`, is because you can have a model that makes a prediction using multiple features.  We'll go over using multiple variables in linear regression  later in this lesson.  For now, let's stick to a single value.\n\n## Linear Regression Quiz\n\nIn this quiz, you'll be working with data on the average life expectancy at birth and the average BMI for males across the world. The data comes from [Gapminder](https://www.gapminder.org/).\n\nThe data file can be found under the \"bmi_and_life_expectancy.csv\" tab in the quiz below. It includes three columns, containing the following data:\n* **Country** – The country the person was born in.  \n* **Life expectancy** – The average life expectancy at birth for a person in that country.\n* **BMI** – The mean BMI of males in that country.\n\n### You'll need to complete each of the following steps:\n\n**1. Load the data**\n* The data is in the file called \"bmi_and_life_expectancy.csv\".\n* Use pandas [`read_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) to load the data into a dataframe (don't forget to import pandas!)\n* Assign the dataframe to the variable `bmi_life_data`.\n\n**2. Build a linear regression model**\n* Create a regression model using scikit-learn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and assign it to `bmi_life_model`.\n* Fit the model to the data.\n\n**3. Predict using the model**\n* Predict using a BMI of 21.07931 and assign it to the variable `laos_life_exp`.",
              "instructor_notes": ""
            },
            {
              "id": 464506,
              "key": "2a422903-d9be-42ca-9f83-fe0aa9d2fa3d",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "2a422903-d9be-42ca-9f83-fe0aa9d2fa3d",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6178299265548288",
                "initial_code_files": [
                  {
                    "text": "# TODO: Add import statements\n\n\n# Assign the dataframe to this variable.\n# TODO: Load the data\nbmi_life_data = None \n\n# Make and fit the linear regression model\n#TODO: Fit the model and Assign it to bmi_life_model\nbmi_life_model = None\n\n# Make a prediction using the model\n# TODO: Predict life expectancy for a BMI value of 21.07931\nlaos_life_exp = None\n",
                    "name": "gapminder1.py"
                  },
                  {
                    "text": "Country,Life expectancy,BMI\nAfghanistan,52.8,20.62058\nAlbania,76.8,26.44657\nAlgeria,75.5,24.59620\nAndorra,84.6,27.63048\nAngola,56.7,22.25083\nArmenia,72.3,25.355420000000002\nAustralia,81.6,27.56373\nAustria,80.4,26.467409999999997\nAzerbaijan,69.2,25.65117\nBahamas,72.2,27.24594\nBangladesh,68.3,20.39742\nBarbados,75.3,26.38439\nBelarus,70.0,26.16443\nBelgium,79.6,26.75915\nBelize,70.7,27.02255\nBenin,59.7,22.41835\nBhutan,70.7,22.82180\nBolivia,71.2,24.43335\nBosnia and Herzegovina,77.5,26.61163\nBotswana,53.2,22.12984\nBrazil,73.2,25.78623\nBulgaria,73.2,26.54286\nBurkina Faso,58.0,21.27157\nBurundi,59.1,21.50291\nCambodia,66.1,20.80496\nCameroon,56.6,23.68173\nCanada,80.8,27.45210\nCape Verde,70.4,23.51522\nChad,54.3,21.48569\nChile,78.5,27.01542\nChina,73.4,22.92176\nColombia,76.2,24.94041\nComoros,67.1,22.06131\n\"Congo, Dem. Rep.\",57.5,19.86692\n\"Congo, Rep.\",58.8,21.87134\nCosta Rica,79.8,26.47897\nCote d'Ivoire,55.4,22.56469\nCroatia,76.2,26.59629\nCuba,77.6,25.06867\nCyprus,80.0,27.41899\nDenmark,78.9,26.13287\nDjibouti,61.8,23.38403\nEcuador,74.7,25.58841\nEgypt,70.2,26.73243\nEl Salvador,73.7,26.36751\nEritrea,60.1,20.88509\nEstonia,74.2,26.26446\nEthiopia,60.0,20.24700\nFiji,64.9,26.53078\nFinland,79.6,26.73339\nFrance,81.1,25.85329\nFrench Polynesia,75.11,30.86752\nGabon,61.7,24.07620\nGambia,65.7,21.65029\nGeorgia,71.8,25.54942\nGermany,80.0,27.16509\nGhana,62.0,22.84247\nGreece,80.2,26.33786\nGreenland,70.3,26.01359\nGrenada,70.8,25.17988\nGuatemala,71.2,25.29947\nGuinea,57.1,22.52449\nGuinea-Bissau,53.6,21.64338\nGuyana,65.0,23.68465\nHaiti,61.0,23.66302\nHonduras,71.8,25.10872\nHungary,73.9,27.11568\nIceland,82.4,27.20687\nIndia,64.7,20.95956\nIndonesia,69.4,21.85576\nIran,73.1,25.31003\nIraq,66.6,26.71017\nIreland,80.1,27.65325\nIsrael,80.6,27.13151\nJamaica,75.1,24.00421\nJapan,82.5,23.50004\nJordan,76.9,27.47362\nKazakhstan,67.1,26.29078\nKenya,60.8,21.59258\nKuwait,77.3,29.17211\nLatvia,72.4,26.45693\nLesotho,44.5,21.90157\nLiberia,59.9,21.89537\nLibya,75.6,26.54164\nLithuania,72.1,26.86102\nLuxembourg,81.0,27.43404\n\"Macedonia, FYR\",74.5,26.34473\nMadagascar,62.2,21.40347\nMalawi,52.4,22.03468\nMalaysia,74.5,24.73069\nMaldives,78.5,23.21991\nMali,58.5,21.78881\nMalta,80.7,27.68361\nMarshall Islands,65.3,29.37337\nMauritania,67.9,22.62295\nMauritius,72.9,25.15669\nMexico,75.4,27.42468\nMoldova,70.4,24.23690\nMongolia,64.8,24.88385\nMontenegro,76.0,26.55412\nMorocco,73.3,25.63182\nMozambique,54.0,21.93536\nMyanmar,59.4,21.44932\nNamibia,59.1,22.65008\nNepal,68.4,20.76344\nNetherlands,80.3,26.01541\nNicaragua,77.0,25.77291\nNiger,58.0,21.21958\nNigeria,59.2,23.03322\nNorway,80.8,26.93424\nOman,76.2,26.24109\nPakistan,64.1,22.29914\nPanama,77.3,26.26959\nPapua New Guinea,58.6,25.01506\nParaguay,74.0,25.54223\nPeru,78.2,24.77041\nPhilippines,69.8,22.87263\nPoland,75.4,26.67380\nPortugal,79.4,26.68445\nQatar,77.9,28.13138\nRomania,73.2,25.41069\nRussia,67.9,26.01131\nRwanda,64.1,22.55453\nSamoa,72.3,30.42475\nSao Tome and Principe,66.0,23.51233\nSenegal,63.5,21.92743\nSerbia,74.3,26.51495\nSierra Leone,53.6,22.53139\nSingapore,80.6,23.83996\nSlovak Republic,74.9,26.92717\nSlovenia,78.7,27.43983\nSomalia,52.6,21.96917\nSouth Africa,53.4,26.85538\nSpain,81.1,27.49975\nSri Lanka,74.0,21.96671\nSudan,65.5,22.40484\nSuriname,70.2,25.49887\nSwaziland,45.1,23.16969\nSweden,81.1,26.37629\nSwitzerland,82.0,26.20195\nSyria,76.1,26.91969\nTajikistan,69.6,23.77966\nTanzania,60.4,22.47792\nThailand,73.9,23.00803\nTimor-Leste,69.9,20.59082\nTogo,57.5,21.87875\nTonga,70.3,30.99563\nTrinidad and Tobago,71.7,26.39669\nTunisia,76.8,25.15699\nTurkey,77.8,26.70371\nTurkmenistan,67.2,25.24796\nUganda,56.0,22.35833\nUkraine,67.8,25.42379\nUnited Arab Emirates,75.6,28.05359\nUnited Kingdom,79.7,27.39249\nUnited States,78.3,28.45698\nUruguay,76.0,26.39123\nUzbekistan,69.6,25.32054\nVanuatu,63.4,26.78926\nWest Bank and Gaza,74.1,26.57750\nVietnam,74.1,20.91630\nZambia,51.1,20.68321\nZimbabwe,47.3,22.02660\n",
                    "name": "bmi_and_life_expectancy.csv"
                  },
                  {
                    "text": "# TODO: Add import statements\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Assign the dataframe to this variable.\n# TODO: Load the data\nbmi_life_data = pd.read_csv(\"bmi_and_life_expectancy.csv\")\n\n# Make and fit the linear regression model\n#TODO: Fit the model and Assign it to bmi_life_model\nbmi_life_model = LinearRegression()\nbmi_life_model.fit(bmi_life_data[['BMI']], bmi_life_data[['Life expectancy']])\n\n# Mak a prediction using the model\n# TODO: Predict life expectancy for a BMI value of 21.07931\nlaos_life_exp = bmi_life_model.predict(21.07931)\n",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 439322,
          "key": "d0ddf90a-4d89-4d63-a3df-cadcd54abc4c",
          "title": "Higher Dimensions",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d0ddf90a-4d89-4d63-a3df-cadcd54abc4c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461556,
              "key": "fa2249e8-1fe9-415b-bc18-07c8008f73f9",
              "title": "Higher Dimensions",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "-UvpQV1qmiE",
                "china_cdn_id": "-UvpQV1qmiE.mp4"
              }
            }
          ]
        },
        {
          "id": 464511,
          "key": "81f1671b-b376-49c0-84d4-8ea18eab8852",
          "title": "Multiple Linear Regression",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "81f1671b-b376-49c0-84d4-8ea18eab8852",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 464512,
              "key": "bee39358-62c6-4f51-ab91-bfaf543cb504",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Multiple Linear Regression\n\nIn the last section, you saw how we can predict life expectancy using BMI. Here, BMI was the **predictor**, also known as an independent variable. A predictor is a variable you're looking at in order to make predictions about other variables, while the values you are trying to predict are known as dependent variables. In this case, life expectancy was the dependent variable.\n\nNow, let’s say we get new data on each person’s heart rate as well. Can we create a prediction of life expectancy using both BMI and heart rate?\n\nAbsolutely! As we saw in the previous video, we can do that using multiple linear regression.\n\nIf the outcome you want to predict depends on more than one variable, you can make a more complicated model that takes this into account. As long as they're relevant to the situation, using more independent/predictor variables can help you get a better prediction.\n\nWhen there's just one predictor, the linear regression model is a line, but as you add more predictor variables, you're adding more dimensions to the picture.",
              "instructor_notes": ""
            },
            {
              "id": 464515,
              "key": "b9511c71-adb3-4bb1-8b62-57bb12106095",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "When you have one predictor variable, the equation of the line is\n\n<span class='mathquill'> y = m x + b </span>\n\nand the plot might look something like this:",
              "instructor_notes": ""
            },
            {
              "id": 464514,
              "key": "5c6ccdef-fccc-4fb7-8a79-add33acebf06",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/January/5886b877_just-a-simple-lin-reg/just-a-simple-lin-reg.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5c6ccdef-fccc-4fb7-8a79-add33acebf06",
              "caption": "Linear regression with one predictor variable",
              "alt": "",
              "width": 528,
              "height": 435,
              "instructor_notes": null
            },
            {
              "id": 464513,
              "key": "ac6fe596-3347-4dc3-ae34-425f87d87cd9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Adding a predictor variable to go to two predictor variables means that the predicting equation is:\n\n<span class=\"mathquill\"> y = m_1 x_1 + m_2 x_2 + b</span>\n\nTo represent this graphically, we'll need a three-dimensional plot, with the linear regression model represented as a plane:",
              "instructor_notes": ""
            },
            {
              "id": 464516,
              "key": "8c2c7436-30ec-49ff-9319-42b156c7633f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/January/5886b8bc_just-a-2d-reg/just-a-2d-reg.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8c2c7436-30ec-49ff-9319-42b156c7633f",
              "caption": "Linear regression with two predictor variables",
              "alt": "",
              "width": 484,
              "height": 407,
              "instructor_notes": null
            },
            {
              "id": 464517,
              "key": "e68cfc38-74d4-4a56-9f5e-a468d46f90a9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You can use more than two predictor variables - in fact, you should use as many as is useful! If you use <span class=\"mathquill\">n</span> predictor variables, then the model can be represented by the equation  \n\n<span class=\"mathquill\">y = m_{1} x_{1} + m_{2} x_{2} + m_{3} x_{3}+ ... +m_{n} x_{n} + b </span>\n\nAs you make a model with more predictor variables, it becomes harder to visualise, but luckily, everything else about linear regression stays the same. We can still fit models and make predictions in exactly the same way - time to try it!\n\n# Programming Quiz: Multiple Linear Regression\n\nIn this quiz, you'll be using the [Boston house-prices dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).  The dataset consists of 13 features of 506 houses and the median home value in $1000's.  You'll fit a model on the 13 features to predict the value of the houses.\n\nYou'll need to complete each of the following steps:\n\n**1. Build a linear regression model**\n* Create a regression model using scikit-learn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and assign it to `model`.\n* Fit the model to the data.\n\n**2. Predict using the model**\n* Predict the value of `sample_house`.\n",
              "instructor_notes": ""
            },
            {
              "id": 464518,
              "key": "40cf5f84-dbc7-48b7-84d3-0261ce70d2d2",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "40cf5f84-dbc7-48b7-84d3-0261ce70d2d2",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "4966760126742528",
                "initial_code_files": [
                  {
                    "text": "from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\n# Load the data from the boston house-prices dataset \nboston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\n\n# Make and fit the linear regression model\n# TODO: Fit the model and assign it to the model variable\nmodel = None\n\n# Make a prediction using the model\nsample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n# TODO: Predict housing price for the sample_house\nprediction = None",
                    "name": "quiz.py"
                  },
                  {
                    "text": "from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\n# Load the data from the boston house-prices dataset \nboston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\n\n# Make and fit the linear regression model\n# TODO: Fit the model and Assign it to the model variable\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Make a prediction using the model\nsample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n# TODO: Predict housing price for the sample_house\nprediction = model.predict(sample_house)",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 439323,
          "key": "ba95d332-6bae-45be-a642-649772712340",
          "title": "Closed Form Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ba95d332-6bae-45be-a642-649772712340",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461341,
              "key": "ba56cad5-f013-47c3-b01d-df81f647a976",
              "title": "Closed Form Solution",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "Full derivation is available on the next page.",
              "video": {
                "youtube_id": "G3fRVgLa5gI",
                "china_cdn_id": "G3fRVgLa5gI.mp4"
              }
            }
          ]
        },
        {
          "id": 440742,
          "key": "fcb495b1-c94e-4a37-b9f8-247584fa735f",
          "title": "(Optional) Closed form Solution Math",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fcb495b1-c94e-4a37-b9f8-247584fa735f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 440744,
              "key": "1d5b32f6-7545-4433-a481-7797f7ec2b8d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# (Optional) Closed form solution math\n\nIn this optional section, we'll develop the math of the closed form solution, which we introduced in the last video. First, we'll do it for the 2-dimensional case, and then for the general case.\n\n### 2-Dimensional solution\n\nOur data will be the values\n<span class=\"mathquill\">x_1, x_2, \\ldots, x_m,</span>\nand our labels will be the values\n<span class=\"mathquill\">y_1,y_2, \\ldots, y_n.</span>\nLet's call our weights\n<span class=\"mathquill\">w_1,</span>\nand\n<span class=\"mathquill\">w_2.</span>\nTherefore, our predictions are\n<span class=\"mathquill\">\\hat{y_i} = w_1x_i + w_2.</span>\nThe mean squared error is",
              "instructor_notes": ""
            },
            {
              "id": 440751,
              "key": "e5fc4004-4a84-4fc7-bb5f-95a7e575aaaf",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7df05_f4/f4.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e5fc4004-4a84-4fc7-bb5f-95a7e575aaaf",
              "caption": "",
              "alt": "",
              "width": 219,
              "height": 51,
              "instructor_notes": null
            },
            {
              "id": 440748,
              "key": "fd8d7bd0-884b-44cd-ac0b-5cb42bd6e6c3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We need to minimize this error function. Therefore, the factor of\n<span class=\"mathquill\">\\frac{1}{m}</span>\ncan be ignored. Now, replacing the value of\n<span class=\"mathquill\">\\hat{y},</span>\nwe get",
              "instructor_notes": ""
            },
            {
              "id": 440754,
              "key": "c634e2ce-0919-4e9c-adfc-cc688811bd16",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7e881_f6/f6.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c634e2ce-0919-4e9c-adfc-cc688811bd16",
              "caption": "",
              "alt": "",
              "width": 308,
              "height": 66,
              "instructor_notes": null
            },
            {
              "id": 440752,
              "key": "24fccbec-85a9-45c5-b4c6-58ad7753d151",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now, in order to minimize this error function, we need to take the derivatives with respect to\n<span class=\"mathquill\">w_1</span>\nand\n<span class=\"mathquill\">w_2</span>\nand set them equal to\n<span class=\"mathquill\">0.</span>\n\nUsing the chain rule, we get",
              "instructor_notes": ""
            },
            {
              "id": 440761,
              "key": "fbd68184-08a5-4ca8-97a6-a7a4bf7bec81",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7eddf_f1/f1.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fbd68184-08a5-4ca8-97a6-a7a4bf7bec81",
              "caption": "",
              "alt": "",
              "width": 353,
              "height": 68,
              "instructor_notes": null
            },
            {
              "id": 440756,
              "key": "24247c17-e735-4f5b-8d49-ec6971d8cdcd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "and",
              "instructor_notes": ""
            },
            {
              "id": 440762,
              "key": "8215622c-8b89-45b7-a930-d3aa6c549636",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7ee2a_f2/f2.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8215622c-8b89-45b7-a930-d3aa6c549636",
              "caption": "",
              "alt": "",
              "width": 329,
              "height": 68,
              "instructor_notes": null
            },
            {
              "id": 440759,
              "key": "7ab9f7cd-48ec-4ee5-9880-a46b088c3e99",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Setting the two equations to zero gives us the following system of two equations and two variables (where the variables are\n<span class=\"mathquill\">w_1</span>\nand\n<span class=\"mathquill\">w_2</span>).",
              "instructor_notes": ""
            },
            {
              "id": 464724,
              "key": "46fbb662-3662-49da-a843-f63d54185734",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a175b5f_codecogseqn-61/codecogseqn-61.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/46fbb662-3662-49da-a843-f63d54185734",
              "caption": "",
              "alt": "",
              "width": 333,
              "height": 66,
              "instructor_notes": null
            },
            {
              "id": 440764,
              "key": "d44b335f-e82a-4e00-a49c-37236037f732",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We can use any method to solve 2 equations and 2 variables. For example, if we multiply the first equation by\n<span class=\"mathquill\">\\sum_{i=1}^m x_i</span>,\nthe second one by\n<span class=\"mathquill\">m</span>,\nsubtract them to obtain a value for\n<span class=\"mathquill\">w_1</span>,\nand then replace this value in the first equation, we get the following:\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 440766,
              "key": "fc52614c-4fee-456a-b1f4-17712de60fdc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7f2ae_f4/f4.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fc52614c-4fee-456a-b1f4-17712de60fdc",
              "caption": "",
              "alt": "",
              "width": 390,
              "height": 110,
              "instructor_notes": null
            },
            {
              "id": 440767,
              "key": "0a61ca9b-2b6c-4119-9b31-2ff69aa7c354",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "That's our desired solution.\n\n### n-Dimensional solution\n\nNow, let's do this when our data has n dimensions, instead of 2. In order to do this, we'll introduce the following notation. Our matrix\n<span class=\"mathquill\">X</span>\ncontaining the data is the following, where each row is one of our datapoints, and\n<span class=\"mathquill\">x_0^{(i)} =1</span>\nto represent the bias.",
              "instructor_notes": ""
            },
            {
              "id": 440768,
              "key": "0c24e176-6291-48c0-8cb5-2673ecd22f68",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7f63c_m/m.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0c24e176-6291-48c0-8cb5-2673ecd22f68",
              "caption": "",
              "alt": "",
              "width": 307,
              "height": 229,
              "instructor_notes": null
            },
            {
              "id": 440769,
              "key": "707be177-11b6-4b84-bd8a-bbefdf7b99d5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Our labels are the vector",
              "instructor_notes": ""
            },
            {
              "id": 440770,
              "key": "6a296540-0609-48e7-9b23-ca2a4f9ec86b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7f716_y/y.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6a296540-0609-48e7-9b23-ca2a4f9ec86b",
              "caption": "",
              "alt": "",
              "width": 122,
              "height": 129,
              "instructor_notes": null
            },
            {
              "id": 440771,
              "key": "1dffa1f5-1ec6-4f06-94e1-c9e03f68fcf2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "and our weight matrix is the following:",
              "instructor_notes": ""
            },
            {
              "id": 464842,
              "key": "f27b4566-b1cb-48ee-a81d-ddf167ba7a3a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1b2275_codecogseqn-62/codecogseqn-62.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f27b4566-b1cb-48ee-a81d-ddf167ba7a3a",
              "caption": "",
              "alt": "",
              "width": 120,
              "height": 109,
              "instructor_notes": null
            },
            {
              "id": 440773,
              "key": "a52ffbe4-c1b7-4650-a1ae-3b9952686710",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "And so the equation for the mean square error can be written as the following matrix product:\n\n<span class=\"mathquill\">E(W) = \\frac{1}{m} ((XW)^T - y^T) (XW - y)</span>.\n\nAgain, since we need to minimize it, we can forget about the factor of\n<span class=\"mathquill\">\\frac{1}{m}</span>, so expanding, we get\n\n<span class=\"mathquill\">E(W) = W^TX^TXW - (XW)^Ty - y^T(XW) + y^Ty</span>.\n\nNotice that in the sum above, the second and the third terms are the same, since it's the inner product of two vectors, which means it's the sum of the products of its coordinates. Therefore,\n\n<span class=\"mathquill\">E(W) = W^TX^TXW - 2(XW)^Ty + y^Ty</span>.\n\nNow, to minimize this, we need to take the derivative with respect to all values in the matrix\n<span class=\"mathquill\">W</span>. Using the chain rule, as we used above, we get the following:",
              "instructor_notes": ""
            },
            {
              "id": 440778,
              "key": "4fa1fbdf-ee42-43e3-b282-af759202abca",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f7fc57_e/e.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4fa1fbdf-ee42-43e3-b282-af759202abca",
              "caption": "",
              "alt": "",
              "width": 231,
              "height": 47,
              "instructor_notes": null
            },
            {
              "id": 440777,
              "key": "697258d4-d5fd-46a0-a694-becbcd1208f4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "And in order to set this equal to zero, we need\n\n<span class=\"mathquill\">X^TXW - X^Ty = 0</span>,\nor equivalently,\n\n<span class=\"mathquill\">W = (X^TX)^{-1} X^T y</span>.\n\nThat's it, that's our closed form solution for\n<span class=\"mathquill\">W</span>!\n\nAs we stated in the video, this method will be expensive in real life, since finding the inverse of the matrix\n<span class=\"mathquill\">X^TX</span> is hard, if \n<span class=\"mathquill\">n</span>\nis large. That's why we go through the pain of doing gradient descent many times. But if our data is sparse, namely, if most of the entries of the matrix\n<span class=\"mathquill\">X</span>\nare zero, there are some very interesting algorithms which will find this inverse quickly, and that'll make this method useful in real life.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 262033,
          "key": "53cd8dbe-ad8e-4e33-ae39-06555472cdb7",
          "title": "Linear Regression Warnings",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "53cd8dbe-ad8e-4e33-ae39-06555472cdb7",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 262034,
              "key": "39235421-57ce-4500-b140-b17f82981002",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Linear Regression Warnings\nLinear regression comes with a set of implicit assumptions and is not the best model for every situation. Here are a couple of issues that you should watch out for.\n\n**Linear Regression Works Best When the Data is Linear**  \nLinear regression produces a straight line model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features (we'll come to this next), or use another kind of model.",
              "instructor_notes": ""
            },
            {
              "id": 262035,
              "key": "07c70950-8f22-4435-b8b8-802913fc798f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/January/58868097_quadraticlinearregression/quadraticlinearregression.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/07c70950-8f22-4435-b8b8-802913fc798f",
              "caption": "",
              "alt": null,
              "width": 541,
              "height": 435,
              "instructor_notes": null
            },
            {
              "id": 262036,
              "key": "95dc2c3a-c94f-4bd2-868b-2ba5502af182",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Linear Regression is Sensitive to Outliers**  \nLinear regression tries to find a 'best fit' line among the training data. If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect. \n\nIn this first plot, the model fits the data pretty well.",
              "instructor_notes": ""
            },
            {
              "id": 262037,
              "key": "6a0f99f8-e898-45a8-8bb9-481617cd0abe",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/January/5886817f_lin-reg-no-outliers/lin-reg-no-outliers.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6a0f99f8-e898-45a8-8bb9-481617cd0abe",
              "caption": "",
              "alt": null,
              "width": 541,
              "height": 435,
              "instructor_notes": null
            },
            {
              "id": 262038,
              "key": "e5196063-f07f-4921-be7b-ced07e31d61a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "However, adding a few points that are outliers and don't fit the pattern really changes the way the model predicts. ",
              "instructor_notes": ""
            },
            {
              "id": 262039,
              "key": "495bada0-c139-415f-9df1-555e7f67d432",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/January/588681bb_lin-reg-w-outliers/lin-reg-w-outliers.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/495bada0-c139-415f-9df1-555e7f67d432",
              "caption": "",
              "alt": null,
              "width": 541,
              "height": 435,
              "instructor_notes": null
            },
            {
              "id": 262041,
              "key": "fc421cb6-6178-4e41-bcc0-9c5a983ef610",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In most circumstances, you'll want a model that fits most of the data most of the time, so watch out for outliers!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 439324,
          "key": "f67c8f75-858e-499a-9a8b-52740ce067d1",
          "title": "Polynomial Regression",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f67c8f75-858e-499a-9a8b-52740ce067d1",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461342,
              "key": "2b179b11-ce93-4eb4-9fe2-38a38bd1f10c",
              "title": "Polynomial Regression",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "DBhWG-PagEQ",
                "china_cdn_id": "DBhWG-PagEQ.mp4"
              }
            }
          ]
        },
        {
          "id": 616721,
          "key": "449c4580-d03a-417c-bf4a-4aded96035f3",
          "title": "Quiz: Polynomial Regression",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "449c4580-d03a-417c-bf4a-4aded96035f3",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 616740,
              "key": "ae084df7-4e58-4aa3-830a-9d227045f9ca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Programming Quiz for \"Polynomial Regression\"**\n\n## Polynomial Regression Exercise\n\nGet some practice implementing polynomial regression in this exercise. In data.csv, you can see data generated for one predictor feature ('Var_X') and one outcome feature ('Var_Y'), following a non-linear trend. Use sklearn's [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class to extend the predictor feature column into multiple columns with polynomial features. Play around with different degrees of polynomial and the Test Run button to see what fits best: when you think you have the best-fitting degree, press the Submit button to check your work!\n\n#### Perform the following steps below:\n\n**1. Load in the data**\n* The data is in the file called 'data.csv'. Note that this data has a header line.\n* Make sure that you've split out the data into the predictor feature in `X` and outcome feature in `y`.\n* For `X`, make sure it is in a 2-d array of 20 rows by 1 column. You might need to use NumPy's [`reshape`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.reshape.html) function to accomplish this.\n\n**2. Create polynomial features**\n* Create an instance of sklearn's [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class and assign it to the variable `poly_feat`. Pay attention to how to set the degree of features, since that will be how the exercise is evaluated.\n* Create the polynomial features by using the `PolynomialFeatures` object's [`.fit_transform()`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.fit_transform) method. The \"fit\" side of the method considers how many features are needed in the output, and the \"transform\" side applies those considerations to the data provided to the method as an argument. Assign the new feature matrix to the `X_poly` variable.\n\n**3. Build a polynomial regression model**\n* Create a polynomial regression model by combining sklearn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class with the polynomial features. Assign the fit model to `poly_model`.",
              "instructor_notes": ""
            },
            {
              "id": 616775,
              "key": "0847886c-4ece-4a5b-a236-2a5c72c92234",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "0847886c-4ece-4a5b-a236-2a5c72c92234",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "4795092074889216",
                "initial_code_files": [
                  {
                    "text": "# TODO: Add import statements\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = None\nX = None\ny = None\n\n# Create polynomial features\n# TODO: Create a PolynomialFeatures object, then fit and transform the\n# predictor feature\npoly_feat = None\nX_poly = None\n\n# Make and fit the polynomial regression model\n# TODO: Create a LinearRegression object and fit it to the polynomial predictor\n# features\npoly_model = None\n\n# Once you've completed all of the steps, select Test Run to see your model\n# predictions against the data, or select Submit Answer to check if the degree\n# of the polynomial features is the same as ours!",
                    "name": "poly_reg.py"
                  },
                  {
                    "text": "Var_X,Var_Y\n-0.33532,6.66854\n0.02160,3.86398\n-1.19438,5.16161\n-0.65046,8.43823\n-0.28001,5.57201\n1.93258,-11.13270\n1.22620,-5.31226\n0.74727,-4.63725\n3.32853,3.80650\n2.87457,-6.06084\n-1.48662,7.22328\n0.37629,2.38887\n1.43918,-7.13415\n0.24183,2.00412\n-2.79140,4.29794\n1.08176,-5.86553\n2.81555,-5.20711\n0.54924,-3.52863\n2.36449,-10.16202\n-1.01925,5.31123",
                    "name": "data.csv"
                  },
                  {
                    "text": "# TODO: Add import statements\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = pd.read_csv('data.csv')\nX = train_data['Var_X'].values.reshape(-1, 1)\ny = train_data['Var_Y'].values\n\n# Create polynomial features\n# TODO: Create a PolynomialFeatures object, then fit and transform the\n# predictor feature\npoly_feat = PolynomialFeatures(degree = 4)\nX_poly = poly_feat.fit_transform(X)\n\n# Make and fit the polynomial regression model\n# TODO: Create a LinearRegression object and fit it to the polynomial predictor\n# features\npoly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            }
          ]
        },
        {
          "id": 439325,
          "key": "3f69dcbd-dc3b-4e6d-a1cc-3257db6ce3c0",
          "title": "Regularization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3f69dcbd-dc3b-4e6d-a1cc-3257db6ce3c0",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461566,
              "key": "723e6df8-cda8-4789-9fdf-e7201d60312e",
              "title": "Regularization",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "PyFNIcsNma0",
                "china_cdn_id": "PyFNIcsNma0.mp4"
              }
            }
          ]
        },
        {
          "id": 616922,
          "key": "6b97514c-bfe2-409c-8e96-dcae87141278",
          "title": "Quiz: Regularization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6b97514c-bfe2-409c-8e96-dcae87141278",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 616925,
              "key": "efcb9ce9-ae2d-4a77-9fc2-deac01d4681c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Programming Quiz for \"Regularization\"**\n\n## Regularization Exercise\n\nPerhaps it's not too surprising at this point, but there are classes in sklearn that will help you perform regularization with your linear regression. You'll get practice with implementing that in this exercise. In this assignment's data.csv, you'll find data for a bunch of points including six predictor variables and one outcome variable. Use sklearn's [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) class to fit a linear regression model to the data, while also using L1 regularization to control for model complexity.\n\n#### Perform the following steps:\n\n**1. Load in the data**\n* The data is in the file called 'data.csv'. Note that there's no header row on this file.\n* Split the data so that the six predictor features (first six columns) are stored in `X`, and the outcome feature (last column) is stored in `y`.\n\n**2. Fit data using linear regression with Lasso regularization**\n* Create an instance of sklearn's [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) class and assign it to the variable `lasso_reg`. You don't need to set any parameter values: use the default values for the quiz.\n* Use the `Lasso` object's `.fit()` method to fit the regression model onto the data.\n\n**3. Inspect the coefficients of the regression model**\n* Obtain the coefficients of the fit regression model using the `.coef_` attribute of the `Lasso` object. Store this in the `reg_coef` variable: the coefficients will be printed out, and you will use your observations to answer the question at the bottom of the page.",
              "instructor_notes": ""
            },
            {
              "id": 644545,
              "key": "f2a66399-8e39-46d6-8bfb-ab00ad30caa5",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "f2a66399-8e39-46d6-8bfb-ab00ad30caa5",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "4629330049761280",
                "initial_code_files": [
                  {
                    "text": "# TODO: Add import statements\n\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = None\nX = None\ny = None\n\n# TODO: Create the linear regression model with lasso regularization.\nlasso_reg = None\n\n# TODO: Fit the model.\n\n\n# TODO: Retrieve and print out the coefficients from the regression model.\nreg_coef = None\nprint(reg_coef)",
                    "name": "regularization.py"
                  },
                  {
                    "text": "1.25664,2.04978,-6.23640,4.71926,-4.26931,0.20590,12.31798\n-3.89012,-0.37511,6.14979,4.94585,-3.57844,0.00640,23.67628\n5.09784,0.98120,-0.29939,5.85805,0.28297,-0.20626,-1.53459\n0.39034,-3.06861,-5.63488,6.43941,0.39256,-0.07084,-24.68670\n5.84727,-0.15922,11.41246,7.52165,1.69886,0.29022,17.54122\n-2.86202,-0.84337,-1.08165,0.67115,-2.48911,0.52328,9.39789\n-7.09328,-0.07233,6.76632,13.06072,0.12876,-0.01048,11.73565\n-7.17614,0.62875,-2.89924,-5.21458,-2.70344,-0.22035,4.42482\n8.67430,2.09933,-11.23591,-5.99532,-2.79770,-0.08710,-5.94615\n-6.03324,-4.16724,2.42063,-3.61827,1.96815,0.17723,-13.11848\n8.67485,1.48271,-1.31205,-1.81154,2.67940,0.04803,-9.25647\n4.36248,-2.69788,-4.60562,-0.12849,3.40617,-0.07841,-29.94048\n9.97205,-0.61515,2.63039,2.81044,5.68249,-0.04495,-20.46775\n-1.44556,0.18337,4.61021,-2.54824,0.86388,0.17696,7.12822\n-3.90381,0.53243,2.83416,-5.42397,-0.06367,-0.22810,6.05628\n-12.39824,-1.54269,-2.66748,10.82084,5.92054,0.13415,-32.91328\n5.75911,-0.82222,10.24701,0.33635,0.26025,-0.02588,17.75036\n-7.12657,3.28707,-0.22508,13.42902,2.16708,-0.09153,-2.80277\n7.22736,1.27122,0.99188,-8.87118,-6.86533,0.09410,33.98791\n-10.31393,2.23819,-7.87166,-3.44388,-1.43267,-0.07893,-3.18407\n-8.25971,-0.15799,-1.81740,1.12972,4.24165,-0.01607,-20.57366\n13.37454,-0.91051,4.61334,0.93989,4.81350,-0.07428,-12.66661\n1.49973,-0.50929,-2.66670,-1.28560,-0.18299,-0.00552,-6.56370\n-10.46766,0.73077,3.93791,-1.73489,-3.26768,0.02366,23.19621\n-1.15898,3.14709,-4.73329,13.61355,-3.87487,-0.14112,13.89143\n4.42275,-2.09867,3.06395,-0.45331,-2.07717,0.22815,10.29282\n-3.34113,-0.31138,4.49844,-2.32619,-2.95757,-0.00793,21.21512\n-1.85433,-1.32509,8.06274,12.75080,-0.89005,-0.04312,14.54248\n0.85474,-0.50002,-3.52152,-4.30405,4.13943,-0.02834,-24.77918\n0.33271,-5.28025,-4.95832,22.48546,4.95051,0.17153,-45.01710\n-0.07308,0.51247,-1.38120,7.86552,3.31641,0.06808,-12.63583\n2.99294,2.85192,5.51751,8.53749,4.30806,-0.17462,0.84415\n1.41135,-1.01899,2.27500,5.27479,-4.90004,0.19508,23.54972\n3.84816,-0.66249,-1.35364,16.51379,0.32115,0.41051,-2.28650\n3.30223,0.23152,-2.16852,0.75257,-0.05749,-0.03427,-4.22022\n-6.12524,-2.56204,0.79878,-3.36284,1.00396,0.06219,-9.10749\n-7.47524,1.31401,-3.30847,4.83057,1.00104,-0.19851,-7.69059\n5.84884,-0.53504,-0.19543,10.27451,6.98704,0.22706,-29.21246\n6.44377,0.47687,-0.08731,22.88008,-2.86604,0.03142,10.90274\n6.35366,-2.04444,1.98872,-1.45189,-1.24062,0.23626,4.62178\n6.85563,-0.94543,5.16637,2.85611,4.64812,0.29535,-7.83647\n1.61758,1.31067,-2.16795,8.07492,-0.17166,-0.10273,0.06922\n3.80137,1.02276,-3.15429,6.09774,3.18885,-0.00163,-16.11486\n-6.81855,-0.15776,-10.69117,8.07818,4.14656,0.10691,-38.47710\n-6.43852,4.30120,2.63923,-1.98297,-0.89599,-0.08174,20.77790\n-2.35292,1.26425,-6.80877,3.31220,-6.17515,-0.04764,14.92507\n9.13580,-1.21425,1.17227,-6.33648,-0.85276,-0.13366,-0.17285\n-3.02986,-0.48694,0.24329,-0.38830,-4.70410,-0.18065,15.95300\n3.27244,2.22393,-1.96640,17.53694,1.62378,0.11539,-4.29743\n-4.44346,-1.96429,0.22209,15.29785,-1.98503,0.40131,4.07647\n-2.61294,-0.24905,-4.02974,-23.82024,-5.94171,-0.04932,16.50504\n3.65962,1.69832,0.78025,9.88639,-1.61555,-0.18570,9.99506\n2.22893,-4.62231,-3.33440,0.07179,0.21983,0.14348,-19.94698\n-5.43092,1.39655,-2.79175,0.16622,-2.38112,-0.09009,6.49039\n-5.88117,-3.04210,-0.87931,3.96197,-1.01125,0.08132,-6.01714\n0.51401,-0.30742,6.01407,-6.85848,-3.61343,-0.15710,24.56965\n4.45547,2.34283,0.98094,-4.66298,-3.79507,0.37084,27.19791\n0.05320,0.27458,6.95838,7.50119,-5.50256,0.06913,36.21698\n4.72057,0.17165,4.83822,-1.03917,4.11211,-0.14773,-6.32623\n-11.60674,-1.15594,-10.23150,0.49843,0.32477,-0.14543,-28.54003\n-7.55406,0.45765,10.67537,-15.12397,3.49680,0.20350,11.97581\n-1.73618,-1.56867,3.98355,-5.16723,-1.20911,0.19377,9.55247\n2.01963,-1.12612,1.16531,-2.71553,-5.39782,0.01086,21.83478\n-1.68542,-1.08901,-3.55426,3.14201,0.82668,0.04372,-13.11204\n-3.09104,-0.23295,-5.62436,-3.03831,0.77772,0.02000,-14.74251\n-3.87717,0.74098,-2.88109,-2.88103,3.36945,-0.30445,-18.44363\n-0.42754,-0.42819,5.02998,-3.45859,-4.21739,0.25281,29.20439\n8.31292,2.30543,-1.52645,-8.39725,-2.65715,-0.30785,12.65607\n8.96352,2.15330,7.97777,-2.99501,2.19453,0.11162,13.62118\n-0.90896,-0.03845,11.60698,5.39133,1.58423,-0.23637,13.73746\n2.03663,-0.49245,4.30331,17.83947,-0.96290,0.10803,10.85762\n-1.72766,1.38544,1.88234,-0.58255,-1.55674,0.08176,16.49896\n-2.40833,-0.00177,2.32146,-1.06438,2.92114,-0.05635,-8.16292\n-1.22998,-1.81632,-2.81740,12.29083,-1.40781,-0.15404,-6.76994\n-3.85332,-1.24892,-6.24187,0.95304,-3.66314,0.02746,-0.87206\n-7.18419,-0.91048,-2.41759,2.46251,-5.11125,-0.05417,11.48350\n5.69279,-0.66299,-3.40195,1.77690,3.70297,-0.02102,-23.71307\n5.82082,1.75872,1.50493,-1.14792,-0.66104,0.14593,11.82506\n0.98854,-0.91971,11.94650,1.36820,2.53711,0.30359,13.23011\n1.55873,0.25462,2.37448,16.04402,-0.06938,-0.36479,-0.67043\n-0.66650,-2.27045,6.40325,7.64815,1.58676,-0.11790,-3.12393\n4.58728,-2.90732,-0.05803,2.27259,2.29507,0.13907,-16.76419\n-11.73607,-2.26595,1.63461,6.21257,0.73723,0.03777,-7.00464\n-2.03125,1.83364,1.57590,5.52329,-3.64759,0.06059,23.96407\n4.63339,1.37232,-0.62675,13.46151,3.69937,-0.09897,-13.66325\n-0.93955,-1.39664,-4.69027,-5.30208,-2.70883,0.07360,-0.26176\n3.19531,-1.43186,3.82859,-9.83963,-2.83611,0.09403,14.30309\n-0.66991,-0.33925,-0.26224,-6.71810,0.52439,0.00654,-2.45750\n3.32705,-0.20431,-0.61940,-5.82014,-3.30832,-0.13399,9.94820\n-3.01400,-1.40133,7.13418,-15.85676,3.92442,0.29137,-0.19544\n10.75129,-0.08744,4.35843,-9.89202,-0.71794,0.12349,12.68742\n4.74271,-1.32895,-2.73218,9.15129,0.93902,-0.17934,-15.58698\n3.96678,-1.93074,-1.98368,-12.52082,7.35129,-0.30941,-40.20406\n2.98664,1.85034,2.54075,-2.98750,0.37193,0.16048,9.08819\n-6.73878,-1.08637,-1.55835,-3.93097,-3.02271,0.11860,6.24185\n-4.58240,-1.27825,7.55098,8.83930,-3.80318,0.04386,26.14768\n-10.00364,2.66002,-4.26776,-3.73792,-0.72349,-0.24617,0.76214\n-4.32624,-2.30314,-8.16044,4.46366,-3.33569,-0.01655,-10.05262\n-1.90167,-0.15858,-10.43466,4.89762,-0.64606,-0.14519,-19.63970\n2.43213,2.41613,2.49949,-8.03891,-1.64164,-0.63444,12.76193",
                    "name": "data.csv"
                  },
                  {
                    "text": "# TODO: Add import statements\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Lasso\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = pd.read_csv('data.csv', header = None)\nX = train_data.iloc[:,:-1]\ny = train_data.iloc[:,-1]\n\n# TODO: Create the linear regression model with lasso regularization.\nlasso_reg = Lasso()\n\n# TODO: Fit the model.\nlasso_reg.fit(X, y)\n\n# TODO: Retrieve and print out the coefficients from the regression model.\nreg_coef = lasso_reg.coef_\nprint(reg_coef)",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            },
            {
              "id": 666054,
              "key": "c2a6b24d-ec9a-4e78-ad18-a0c6ff3092fe",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "c2a6b24d-ec9a-4e78-ad18-a0c6ff3092fe",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "For which of the predictor features has the lasso regularization step zeroed the corresponding coefficient?",
                "answers": [
                  {
                    "id": "a1530050954519",
                    "text": "Column X1",
                    "is_correct": true
                  },
                  {
                    "id": "a1530050957525",
                    "text": "Column X2",
                    "is_correct": false
                  },
                  {
                    "id": "a1530050958089",
                    "text": "Column X3",
                    "is_correct": false
                  },
                  {
                    "id": "a1530050958663",
                    "text": "Column X4",
                    "is_correct": false
                  },
                  {
                    "id": "a1530050959237",
                    "text": "Column X5",
                    "is_correct": false
                  },
                  {
                    "id": "a1530050959891",
                    "text": "Column X6",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 640541,
          "key": "4680286f-aa09-4ae6-b3ac-c5aa575ef5ea",
          "title": "Feature Scaling",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4680286f-aa09-4ae6-b3ac-c5aa575ef5ea",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 640684,
              "key": "abc9a876-1bb7-42e2-b438-b56933b9c78a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Feature Scaling\n\nWhat is feature scaling?  Feature scaling is a way of transforming your data into a common range of values.  There are two common scalings:\n\n1. Standardizing\n2. Normalizing\n\n#### Standardizing\n\n**Standardizing** is completed by taking each value of your column, subtracting the mean of the column, and then dividing by the standard deviation of the column.  In Python, let's say you have a column in `df` called `height`.  You could create a standardized height as:\n\n```python\ndf[\"height_standard\"] = (df[\"height\"] - df[\"height\"].mean()) / df[\"height\"].std()\n```\n\nThis will create a new \"standardized\" column where each value is a comparison to the mean of the column, and a new, standardized value can be interpreted as the number of standard deviations the original height was from the mean. This type of feature scaling is by far the most common of all techniques (for the reasons discussed here, but also likely because of precedent).\n\n#### Normalizing\n\nA second type of feature scaling that is very popular is known as **normalizing**.  With normalizing, data are scaled between 0 and 1.  Using the same example as above, we could perform normalizing in Python in the following way:\n\n```python\ndf[\"height_normal\"] = (df[\"height\"] - df[\"height\"].min()) /     \\\n                      (df[\"height\"].max() - df['height'].min())\n```\n\n### When Should I Use Feature Scaling?\n\nIn many machine learning algorithms, the result will change depending on the units of your data.  This is especially true in two specific cases:\n\n1. When your algorithm uses a distance-based metric to predict.  \n2. When you incorporate regularization.\n\n#### Distance Based Metrics\n\nIn future lessons, you will see one common supervised learning technique that is based on the distance points are from one another called [Support Vector Machines (or SVMs)](https://en.wikipedia.org/wiki/Support_vector_machine).  Another technique that involves distance based methods to determine a prediction is [k-nearest neighbors (or k-nn)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).  With either of these techniques, choosing not to scale your data may lead to drastically different (and likely misleading) ending predictions.  \n\nFor this reason, choosing some sort of feature scaling is necessary with these distance based techniques.  \n\n#### Regularization\n\nWhen you start introducing regularization, you will again want to scale the features of your model.  The penalty on particular coefficients in regularized linear regression techniques depends largely on the scale associated with the features.  When one feature is on a small range, say from 0 to 10, and another is on a large range, say from 0 to 1 000 000, applying regularization is going to unfairly punish the feature with the small range.  Features with small ranges need to have larger coefficients compared to features with large ranges in order to have the same effect on the outcome of the data. (Think about how <span class=\"mathquill\">ab = ba</span> for two numbers <span class=\"mathquill\">a</span> and <span class=\"mathquill\">b</span>.) Therefore, if regularization could remove one of those two features with the same net increase in error, it would rather remove the small-ranged feature with the large coefficient, since that would reduce the regularization term the most.\n\nAgain, this means you will want to scale features any time you are applying regularization.\n\n* [A useful Quora post on the importance of feature scaling when using regularization.](https://www.quora.com/Why-do-we-normalize-the-data)    \n\nA point raised in the article above is that feature scaling can speed up convergence of your machine learning algorithms, which is an important consideration when you scale machine learning applications.\n\nUse the quiz below to get some practice with feature scaling.\n",
              "instructor_notes": ""
            },
            {
              "id": 666115,
              "key": "d96c1585-51cb-402f-8a03-16b4f4b8ac2c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Feature Scaling Exercise\n\nPreviously, you saw how regularization will remove features from a model (by setting their coefficients to zero) if the penalty for removing them is small. In this exercise, you'll revisit the same dataset as before and see how scaling the features changes which features are favored in a regularization step. See the \"Quiz: Regularization\" page for more details. The only thing different for this quiz compared to the previous one is the addition of a new step after loading the data, where you will use sklearn's [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardize the data before you fit a linear regression model to the data with L1 (Lasso) regularization.\n\n#### Perform the following steps:\n\n**1. Load in the data**\n* The data is in the file called 'data.csv'. Note that there's no header row on this file.\n* Split the data so that the six predictor features (first six columns) are stored in `X`, and the outcome feature (last column) is stored in `y`.\n\n**2. (NEW) Perform feature scaling on data via standardization**\n* Create an instance of sklearn's [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and assign it to the variable `scaler`.\n* Compute the scaling parameters by using the `.fit_transform()` method on the predictor feature array, which also returns the predictor variables in their standardized values. Store those standardized values in `X_scaled`.\n\n**3. Fit data using linear regression with Lasso regularization**\n* Create an instance of sklearn's [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) class and assign it to the variable `lasso_reg`. You don't need to set any parameter values: use the default values for the quiz.\n* Use the `Lasso` object's `.fit()` method to fit the regression model onto the data. Make sure that you apply the fit to the _standardized_ data from the previous step (`X_scaled`), not the original data.\n\n**4. Inspect the coefficients of the regression model**\n* Obtain the coefficients of the fit regression model using the `.coef_` attribute of the `Lasso` object. Store this in the `reg_coef` variable: the coefficients will be printed out, and you will use your observations to answer the question at the bottom of the page.",
              "instructor_notes": ""
            },
            {
              "id": 667119,
              "key": "d4178130-10f7-43ce-8812-4366a8b829e4",
              "title": "",
              "semantic_type": "QuizAtom",
              "is_public": true,
              "instructor_notes": "",
              "user_state": {
                "node_key": "d4178130-10f7-43ce-8812-4366a8b829e4",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "instruction": null,
              "question": {
                "title": "",
                "semantic_type": "ProgrammingQuestion",
                "evaluation_id": "6212003117531136",
                "initial_code_files": [
                  {
                    "text": "# TODO: Add import statements\n\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = None\nX = None\ny = None\n\n# TODO: Create the standardization scaling object.\nscaler = None\n\n# TODO: Fit the standardization parameters and scale the data.\nX_scaled = None\n\n# TODO: Create the linear regression model with lasso regularization.\nlasso_reg = None\n\n# TODO: Fit the model.\n\n\n# TODO: Retrieve and print out the coefficients from the regression model.\nreg_coef = None\nprint(reg_coef)",
                    "name": "feature_scaling.py"
                  },
                  {
                    "text": "1.25664,2.04978,-6.23640,4.71926,-4.26931,0.20590,12.31798\n-3.89012,-0.37511,6.14979,4.94585,-3.57844,0.00640,23.67628\n5.09784,0.98120,-0.29939,5.85805,0.28297,-0.20626,-1.53459\n0.39034,-3.06861,-5.63488,6.43941,0.39256,-0.07084,-24.68670\n5.84727,-0.15922,11.41246,7.52165,1.69886,0.29022,17.54122\n-2.86202,-0.84337,-1.08165,0.67115,-2.48911,0.52328,9.39789\n-7.09328,-0.07233,6.76632,13.06072,0.12876,-0.01048,11.73565\n-7.17614,0.62875,-2.89924,-5.21458,-2.70344,-0.22035,4.42482\n8.67430,2.09933,-11.23591,-5.99532,-2.79770,-0.08710,-5.94615\n-6.03324,-4.16724,2.42063,-3.61827,1.96815,0.17723,-13.11848\n8.67485,1.48271,-1.31205,-1.81154,2.67940,0.04803,-9.25647\n4.36248,-2.69788,-4.60562,-0.12849,3.40617,-0.07841,-29.94048\n9.97205,-0.61515,2.63039,2.81044,5.68249,-0.04495,-20.46775\n-1.44556,0.18337,4.61021,-2.54824,0.86388,0.17696,7.12822\n-3.90381,0.53243,2.83416,-5.42397,-0.06367,-0.22810,6.05628\n-12.39824,-1.54269,-2.66748,10.82084,5.92054,0.13415,-32.91328\n5.75911,-0.82222,10.24701,0.33635,0.26025,-0.02588,17.75036\n-7.12657,3.28707,-0.22508,13.42902,2.16708,-0.09153,-2.80277\n7.22736,1.27122,0.99188,-8.87118,-6.86533,0.09410,33.98791\n-10.31393,2.23819,-7.87166,-3.44388,-1.43267,-0.07893,-3.18407\n-8.25971,-0.15799,-1.81740,1.12972,4.24165,-0.01607,-20.57366\n13.37454,-0.91051,4.61334,0.93989,4.81350,-0.07428,-12.66661\n1.49973,-0.50929,-2.66670,-1.28560,-0.18299,-0.00552,-6.56370\n-10.46766,0.73077,3.93791,-1.73489,-3.26768,0.02366,23.19621\n-1.15898,3.14709,-4.73329,13.61355,-3.87487,-0.14112,13.89143\n4.42275,-2.09867,3.06395,-0.45331,-2.07717,0.22815,10.29282\n-3.34113,-0.31138,4.49844,-2.32619,-2.95757,-0.00793,21.21512\n-1.85433,-1.32509,8.06274,12.75080,-0.89005,-0.04312,14.54248\n0.85474,-0.50002,-3.52152,-4.30405,4.13943,-0.02834,-24.77918\n0.33271,-5.28025,-4.95832,22.48546,4.95051,0.17153,-45.01710\n-0.07308,0.51247,-1.38120,7.86552,3.31641,0.06808,-12.63583\n2.99294,2.85192,5.51751,8.53749,4.30806,-0.17462,0.84415\n1.41135,-1.01899,2.27500,5.27479,-4.90004,0.19508,23.54972\n3.84816,-0.66249,-1.35364,16.51379,0.32115,0.41051,-2.28650\n3.30223,0.23152,-2.16852,0.75257,-0.05749,-0.03427,-4.22022\n-6.12524,-2.56204,0.79878,-3.36284,1.00396,0.06219,-9.10749\n-7.47524,1.31401,-3.30847,4.83057,1.00104,-0.19851,-7.69059\n5.84884,-0.53504,-0.19543,10.27451,6.98704,0.22706,-29.21246\n6.44377,0.47687,-0.08731,22.88008,-2.86604,0.03142,10.90274\n6.35366,-2.04444,1.98872,-1.45189,-1.24062,0.23626,4.62178\n6.85563,-0.94543,5.16637,2.85611,4.64812,0.29535,-7.83647\n1.61758,1.31067,-2.16795,8.07492,-0.17166,-0.10273,0.06922\n3.80137,1.02276,-3.15429,6.09774,3.18885,-0.00163,-16.11486\n-6.81855,-0.15776,-10.69117,8.07818,4.14656,0.10691,-38.47710\n-6.43852,4.30120,2.63923,-1.98297,-0.89599,-0.08174,20.77790\n-2.35292,1.26425,-6.80877,3.31220,-6.17515,-0.04764,14.92507\n9.13580,-1.21425,1.17227,-6.33648,-0.85276,-0.13366,-0.17285\n-3.02986,-0.48694,0.24329,-0.38830,-4.70410,-0.18065,15.95300\n3.27244,2.22393,-1.96640,17.53694,1.62378,0.11539,-4.29743\n-4.44346,-1.96429,0.22209,15.29785,-1.98503,0.40131,4.07647\n-2.61294,-0.24905,-4.02974,-23.82024,-5.94171,-0.04932,16.50504\n3.65962,1.69832,0.78025,9.88639,-1.61555,-0.18570,9.99506\n2.22893,-4.62231,-3.33440,0.07179,0.21983,0.14348,-19.94698\n-5.43092,1.39655,-2.79175,0.16622,-2.38112,-0.09009,6.49039\n-5.88117,-3.04210,-0.87931,3.96197,-1.01125,0.08132,-6.01714\n0.51401,-0.30742,6.01407,-6.85848,-3.61343,-0.15710,24.56965\n4.45547,2.34283,0.98094,-4.66298,-3.79507,0.37084,27.19791\n0.05320,0.27458,6.95838,7.50119,-5.50256,0.06913,36.21698\n4.72057,0.17165,4.83822,-1.03917,4.11211,-0.14773,-6.32623\n-11.60674,-1.15594,-10.23150,0.49843,0.32477,-0.14543,-28.54003\n-7.55406,0.45765,10.67537,-15.12397,3.49680,0.20350,11.97581\n-1.73618,-1.56867,3.98355,-5.16723,-1.20911,0.19377,9.55247\n2.01963,-1.12612,1.16531,-2.71553,-5.39782,0.01086,21.83478\n-1.68542,-1.08901,-3.55426,3.14201,0.82668,0.04372,-13.11204\n-3.09104,-0.23295,-5.62436,-3.03831,0.77772,0.02000,-14.74251\n-3.87717,0.74098,-2.88109,-2.88103,3.36945,-0.30445,-18.44363\n-0.42754,-0.42819,5.02998,-3.45859,-4.21739,0.25281,29.20439\n8.31292,2.30543,-1.52645,-8.39725,-2.65715,-0.30785,12.65607\n8.96352,2.15330,7.97777,-2.99501,2.19453,0.11162,13.62118\n-0.90896,-0.03845,11.60698,5.39133,1.58423,-0.23637,13.73746\n2.03663,-0.49245,4.30331,17.83947,-0.96290,0.10803,10.85762\n-1.72766,1.38544,1.88234,-0.58255,-1.55674,0.08176,16.49896\n-2.40833,-0.00177,2.32146,-1.06438,2.92114,-0.05635,-8.16292\n-1.22998,-1.81632,-2.81740,12.29083,-1.40781,-0.15404,-6.76994\n-3.85332,-1.24892,-6.24187,0.95304,-3.66314,0.02746,-0.87206\n-7.18419,-0.91048,-2.41759,2.46251,-5.11125,-0.05417,11.48350\n5.69279,-0.66299,-3.40195,1.77690,3.70297,-0.02102,-23.71307\n5.82082,1.75872,1.50493,-1.14792,-0.66104,0.14593,11.82506\n0.98854,-0.91971,11.94650,1.36820,2.53711,0.30359,13.23011\n1.55873,0.25462,2.37448,16.04402,-0.06938,-0.36479,-0.67043\n-0.66650,-2.27045,6.40325,7.64815,1.58676,-0.11790,-3.12393\n4.58728,-2.90732,-0.05803,2.27259,2.29507,0.13907,-16.76419\n-11.73607,-2.26595,1.63461,6.21257,0.73723,0.03777,-7.00464\n-2.03125,1.83364,1.57590,5.52329,-3.64759,0.06059,23.96407\n4.63339,1.37232,-0.62675,13.46151,3.69937,-0.09897,-13.66325\n-0.93955,-1.39664,-4.69027,-5.30208,-2.70883,0.07360,-0.26176\n3.19531,-1.43186,3.82859,-9.83963,-2.83611,0.09403,14.30309\n-0.66991,-0.33925,-0.26224,-6.71810,0.52439,0.00654,-2.45750\n3.32705,-0.20431,-0.61940,-5.82014,-3.30832,-0.13399,9.94820\n-3.01400,-1.40133,7.13418,-15.85676,3.92442,0.29137,-0.19544\n10.75129,-0.08744,4.35843,-9.89202,-0.71794,0.12349,12.68742\n4.74271,-1.32895,-2.73218,9.15129,0.93902,-0.17934,-15.58698\n3.96678,-1.93074,-1.98368,-12.52082,7.35129,-0.30941,-40.20406\n2.98664,1.85034,2.54075,-2.98750,0.37193,0.16048,9.08819\n-6.73878,-1.08637,-1.55835,-3.93097,-3.02271,0.11860,6.24185\n-4.58240,-1.27825,7.55098,8.83930,-3.80318,0.04386,26.14768\n-10.00364,2.66002,-4.26776,-3.73792,-0.72349,-0.24617,0.76214\n-4.32624,-2.30314,-8.16044,4.46366,-3.33569,-0.01655,-10.05262\n-1.90167,-0.15858,-10.43466,4.89762,-0.64606,-0.14519,-19.63970\n2.43213,2.41613,2.49949,-8.03891,-1.64164,-0.63444,12.76193",
                    "name": "data.csv"
                  },
                  {
                    "text": "# TODO: Add import statements\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\n\n# Assign the data to predictor and outcome variables\n# TODO: Load the data\ntrain_data = pd.read_csv('data.csv', header = None)\nX = train_data.iloc[:,:-1]\ny = train_data.iloc[:,-1]\n\n# TODO: Create the standardization scaling object.\nscaler = StandardScaler()\n\n# TODO: Fit the standardization parameters and scale the data.\nX_scaled = scaler.fit_transform(X)\n\n# TODO: Create the linear regression model with lasso regularization.\nlasso_reg = Lasso()\n\n# TODO: Fit the model.\nlasso_reg.fit(X_scaled, y)\n\n# TODO: Retrieve and print out the coefficients from the regression model.\nreg_coef = lasso_reg.coef_\nprint(reg_coef)",
                    "name": "solution.py"
                  }
                ]
              },
              "answer": null
            },
            {
              "id": 667135,
              "key": "c8f82bac-01a8-4a45-93ed-53b8bb0a6101",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "c8f82bac-01a8-4a45-93ed-53b8bb0a6101",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "For which of the predictor features has the lasso regularization step zeroed the corresponding coefficient, on the standardized data?",
                "answers": [
                  {
                    "id": "a1530131865076",
                    "text": "Column X1",
                    "is_correct": true
                  },
                  {
                    "id": "a1530131886536",
                    "text": "Column X2",
                    "is_correct": false
                  },
                  {
                    "id": "a1530131887334",
                    "text": "Column X3",
                    "is_correct": false
                  },
                  {
                    "id": "a1530131887876",
                    "text": "Column X4",
                    "is_correct": true
                  },
                  {
                    "id": "a1530131888433",
                    "text": "Column X5",
                    "is_correct": false
                  },
                  {
                    "id": "a1530131889024",
                    "text": "Column X6",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 634693,
          "key": "df611f4e-8ed0-40f9-894e-818eddd5f885",
          "title": "Outro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "df611f4e-8ed0-40f9-894e-818eddd5f885",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 634700,
              "key": "23860484-d539-4398-9fd0-18fdcb878399",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Recap\n\nIn this lesson, you were introduced to linear models.  Specifically, you saw:\n\n* **Gradient descent** as a method to optimize your linear models.\n* **Multiple Linear Regression** as a technique for when you are comparing more than two variables.\n* **Polynomial Regression** for relationships between variables that aren't linear.\n* **Regularization** as a technique to assure that your models will not only fit to the data available, but also extend to new situations.\n\n### Outro\n\nIn this lesson, you were predicting quantitative values.  Predicting quantitative values is often just considered a `Regression` problem.  In the next lesson, you will be introduced to predicting a category, which is called a `Classification` problem.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}