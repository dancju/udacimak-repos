{
  "data": {
    "lesson": {
      "id": 521352,
      "key": "cecdf335-39ba-4fba-9aed-b24972e81031",
      "title": "Advanced CNN Architectures",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Learn about advances in CNN architectures and see how region-based CNN’s, like Faster R-CNN, have allowed for fast, localized object recognition in images.\n",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/cecdf335-39ba-4fba-9aed-b24972e81031/521352/1544453225844/Advanced+CNN+Architectures+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/cecdf335-39ba-4fba-9aed-b24972e81031/521352/1544453222088/Advanced+CNN+Architectures+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 521353,
          "key": "64c6a826-44e0-4ca5-99bf-8d4eabe820b7",
          "title": "CNN's and Scene Understanding",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "64c6a826-44e0-4ca5-99bf-8d4eabe820b7",
            "completed_at": "2020-04-01T05:28:47.518Z",
            "last_viewed_at": "2020-04-01T05:28:46.500Z",
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626588,
              "key": "b2482041-9956-441d-acad-308770853a7d",
              "title": "01 CNNs And Scene Understanding RENDER Full V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_iRqSOsTBQU",
                "china_cdn_id": "_iRqSOsTBQU.mp4"
              }
            }
          ]
        },
        {
          "id": 620940,
          "key": "77fbec2e-7f7c-413d-ad35-ea0f9df5dfb4",
          "title": "More than Classification",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "77fbec2e-7f7c-413d-ad35-ea0f9df5dfb4",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626587,
              "key": "8d2f64ef-5b5c-4e95-815c-8d297f06b419",
              "title": "02 More Than Classification RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "vBE5KvvAYzg",
                "china_cdn_id": "vBE5KvvAYzg.mp4"
              }
            }
          ]
        },
        {
          "id": 620941,
          "key": "1c5c9438-0037-4e04-9912-0012c920668c",
          "title": "Classification and Localization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1c5c9438-0037-4e04-9912-0012c920668c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626665,
              "key": "f9bf785a-b7e2-4fb5-9f61-780c14bfd24e",
              "title": "03 Classification And Localization RENDER V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "UqNg9d6cKQU",
                "china_cdn_id": "UqNg9d6cKQU.mp4"
              }
            },
            {
              "id": 620958,
              "key": "e065e65a-8018-4fe6-9908-3afa6fba904f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Distracted Driver Detection\n\nAs was mentioned in the video, detecting distracted drivers is a classification *and* localization challenge. As part of this task, some approaches localize cell phones in a image; if a cell phone was detected as close to a driver's face, then it assumes that the driver is on the phone and distracted, but if a phone is located on a car seat or far away from the driver, then the driver is more likely to be focused on the road. [Check out an example of such detection code in Keras](https://github.com/tdeboissiere/VGG16CAM-keras).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 620942,
          "key": "bc937c86-cf27-455a-90bf-34e7b9fa67ec",
          "title": "Bounding Boxes and Regression",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "bc937c86-cf27-455a-90bf-34e7b9fa67ec",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626585,
              "key": "ae5afef9-afa8-4f4a-9425-55812a2dc385",
              "title": "04 Bounding Boxes And Regression V1 RENDER V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "2YM82c7SaCo",
                "china_cdn_id": "2YM82c7SaCo.mp4"
              }
            },
            {
              "id": 620970,
              "key": "02aab189-b70c-4cf8-ac5e-a4b8e663da55",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Beyond Bounding Boxes\n\nTo predict bounding boxes, we train a model to take an image as input and output coordinate values: (x, y, w, h). This kind of model can be extended to _any_ problem that has coordinate values as outputs! One such example is **human pose estimation**.\n",
              "instructor_notes": ""
            },
            {
              "id": 620995,
              "key": "e71293d2-d272-4273-bc4e-f186504bbd35",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aeb6ac2_screen-shot-2018-05-03-at-1.01.46-pm/screen-shot-2018-05-03-at-1.01.46-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e71293d2-d272-4273-bc4e-f186504bbd35",
              "caption": "Huan pose estimation points.",
              "alt": "",
              "width": 500,
              "height": 604,
              "instructor_notes": null
            },
            {
              "id": 620996,
              "key": "7af2e036-bf6f-41cd-8492-ec93a2b2cfdf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the above example, we see that the pose of a human body can be estimated by tracking 14 points along the joints of a body.",
              "instructor_notes": ""
            },
            {
              "id": 620964,
              "key": "e214a50f-0c9c-4b19-82fa-b111a0aab34e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Weighted Loss Functions\n\nYou may be wondering: how can we train a network with two different outputs (a class and a bounding box) and different losses for those outputs?\n\nWe know that, in this case, we use categorical cross entropy to calculate the loss for our predicted and true classes, and we use a regression loss (something like smooth L1 loss) to compare predicted and true bounding boxes. But, we have to train our whole network using one loss, so how can we combine these?\n\nThere are a couple of ways to train on multiple loss functions, and in practice, we often use a weighted sum of classification *and* regression losses (ex. `0.5*cross_entropy_loss + 0.5*L1_loss`); the result is a single error value with which we can do backpropagation. This does introduce a hyperparameter: the loss weights. We want to weight each loss so that these losses are balanced and combined effectively, and in research we see that another regularization term is often introduced to help decide on the weight values that best combine these losses.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 620945,
          "key": "98f18b3d-cdfd-4d3b-9ae1-1a277e40b1dc",
          "title": "Quiz: Loss Values",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "98f18b3d-cdfd-4d3b-9ae1-1a277e40b1dc",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 621661,
              "key": "08656330-4f76-409d-8ecd-45dd7bd92403",
              "title": "",
              "semantic_type": "ValidatedQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "08656330-4f76-409d-8ecd-45dd7bd92403",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Look at the documentation for [MSE Loss](https://pytorch.org/docs/stable/nn.html#mseloss). For a ground truth coordinate `(2, 5)` and a predicted coordinate `(2.5, 3)`, what is the MSE loss between these points? (You may assume default values for averaging.)",
                "matchers": [
                  {
                    "expression": "^\\s*2.125*\\s*"
                  }
                ]
              }
            },
            {
              "id": 621659,
              "key": "3e632541-132b-41bd-bdb4-8252633a374d",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "3e632541-132b-41bd-bdb4-8252633a374d",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Look at the documentation for [Smooth L1 Loss](https://pytorch.org/docs/stable/nn.html#smoothl1loss). For a ground truth coordinate `(2, 5)` and a predicted coordinate `(2.5, 3)`, what is the smooth L1 loss between these points?",
                "answers": [
                  {
                    "id": "a1525393729421",
                    "text": "0.0125",
                    "is_correct": false
                  },
                  {
                    "id": "a1525394126166",
                    "text": "0.55",
                    "is_correct": false
                  },
                  {
                    "id": "a1525394138726",
                    "text": " 0.8125",
                    "is_correct": true
                  },
                  {
                    "id": "a1525394158720",
                    "text": "1.5125",
                    "is_correct": false
                  },
                  {
                    "id": "a1525394169183",
                    "text": "1.825",
                    "is_correct": false
                  },
                  {
                    "id": "a1525394721388",
                    "text": "2.125",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 620946,
          "key": "4a1bf35b-edba-42ef-91c3-cab0732582c6",
          "title": "Region Proposals",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4a1bf35b-edba-42ef-91c3-cab0732582c6",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626584,
              "key": "306befa5-55b0-49fe-a9bf-a2d8d22394ee",
              "title": "05 Region Proposals V1 RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "HLwpr7h3rPY",
                "china_cdn_id": "HLwpr7h3rPY.mp4"
              }
            },
            {
              "id": 620997,
              "key": "c3ac2a32-1665-4e28-8e25-a323f40de5d9",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aeb6b64_screen-shot-2018-05-03-at-1.04.27-pm/screen-shot-2018-05-03-at-1.04.27-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c3ac2a32-1665-4e28-8e25-a323f40de5d9",
              "caption": "Suggested region proposals for an image of a cat and dog.",
              "alt": "",
              "width": 250,
              "height": 798,
              "instructor_notes": null
            },
            {
              "id": 620998,
              "key": "6d51a5df-f77d-4d74-a900-f0794fe3775f",
              "title": "Reflect",
              "semantic_type": "ReflectAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "6d51a5df-f77d-4d74-a900-f0794fe3775f",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "title": null,
                "semantic_type": "TextQuestion",
                "evaluation_id": null,
                "text": "Consider the above image, how do you think you would select the best proposed regions; what criteria do good regions have?"
              },
              "answer": {
                "text": "The regions we want to analyze are those with complete objects in them. We want to get rid of regions that contain image background or only a portion of an object. So, two common approaches are suggested: 1. identify similar regions using feature extraction or a clustering algorithm like k-means, as you've already seen; these methods should identify any areas of interest. 2. Add another layer to our model that performs a binary classification on these regions and labels them: object or not-object; this gives us the ability to discard any non-object regions!",
                "video": null
              }
            }
          ]
        },
        {
          "id": 620948,
          "key": "cefa8cb1-eceb-4c6a-9ed3-c67c81516983",
          "title": "R-CNN",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "cefa8cb1-eceb-4c6a-9ed3-c67c81516983",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626583,
              "key": "16406fef-efc3-4b35-9117-f63013de7b28",
              "title": "06 RCNN V1 RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "EchapZJMTYU",
                "china_cdn_id": "EchapZJMTYU.mp4"
              }
            },
            {
              "id": 627222,
              "key": "1f2e478a-379c-40eb-9812-275e97bb5ec6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## R-CNN Outputs\n\nThe R-CNN is the least sophisticated region-based architecture, but it is the basis for understanding how multiple object recognition algorithms work! It outputs a class score and bounding box coordinates for every input RoI. \n\nAn R-CNN feeds an image into a CNN with regions of interest (RoI’s) already identified. Since these RoI’s are of varying sizes, they often need to be **warped to be a standard size**, since CNN’s typically expect a consistent, square image size as input. After RoI's are warped, the R-CNN architecture, processes these regions one by one and, for each image, produces 1. a class label and 2. a bounding box (that may act as a slight correction to the input region).\n\n1. R-CNN produces bounding box coordinates to reduce localization errors; so a region comes in, but it may not perfectly surround a given object and the output coordinates `(x,y,w,h)` aim to _perfectly_ localize an object in a given region.\n2. R-CNN, unlike other models, does not explicitly produce a confidence score that indicates whether an object is in a region, instead it cleverly produces a set of class scores for which one class is \"background\". This ends up serving a similar purpose, for example, if the class score for a region is `Pbackground = 0.10`, it likely contains an object, but if it's `Pbackground = 0.90`, then the region probably doesn't contain an object.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 620950,
          "key": "5ed0641d-8ca2-49aa-b5e5-ea921548313f",
          "title": "Fast R-CNN",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5ed0641d-8ca2-49aa-b5e5-ea921548313f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626582,
              "key": "08be66be-9ff6-4daf-b433-149de002fb9c",
              "title": "07 Fast RCNN V1 RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "6FOBZ9OgWlY",
                "china_cdn_id": "6FOBZ9OgWlY.mp4"
              }
            },
            {
              "id": 621566,
              "key": "605c1d97-747e-4b55-afa8-a9dd3e51510e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### RoI Pooling\n\nTo warp regions of interest into a consistent size for further analysis, some networks use RoI pooling. RoI pooling is an additional layer in our network that takes in a rectangular region of any size, performs a maxpooling operation on that region in pieces such that the output is a fixed shape. Below is an example of a region with some pixel values being broken up into pieces which pooling will be applied to; a section with the values:\n\n```\n[[0.85, 0.34, 0.76],\n [0.32, 0.74, 0.21]]\n```\n\nWill become a single max value after pooling: `0.85`. After applying this to an image in these pieces, you can see how any rectangular region can be forced into a smaller, square representation.",
              "instructor_notes": ""
            },
            {
              "id": 621567,
              "key": "8fcb3a6a-b296-42ce-ab13-1c6833d559c3",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aeb9cc4_screen-shot-2018-05-03-at-4.34.25-pm/screen-shot-2018-05-03-at-4.34.25-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8fcb3a6a-b296-42ce-ab13-1c6833d559c3",
              "caption": "An example of pooling sections, credit to [this informational resource](https://blog.deepsense.ai/region-of-interest-pooling-explained/) on RoI pooling [by Tomasz Grel].",
              "alt": "",
              "width": 400,
              "height": 1054,
              "instructor_notes": null
            },
            {
              "id": 621569,
              "key": "c7c094f9-209c-4f4c-866c-cca014af51c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You can see the complete process from input image to region to reduced, maxpooled region, below.",
              "instructor_notes": ""
            },
            {
              "id": 621568,
              "key": "b2feeafc-2eca-4dd8-b17b-f5763ea9de6d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aeb9dc6_roi-pooling-gif/roi-pooling-gif.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/b2feeafc-2eca-4dd8-b17b-f5763ea9de6d",
              "caption": "Credit to [this informational resource](https://blog.deepsense.ai/region-of-interest-pooling-explained/) on RoI pooling. ",
              "alt": "",
              "width": 500,
              "height": 600,
              "instructor_notes": null
            },
            {
              "id": 621562,
              "key": "bcdf8f45-0e1c-41a4-938a-0fab74a67166",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Speed\n\nFast R-CNN is about 10 times as fast to train as an R-CNN because it only creates convolutional layers once for a given image and then performs further analysis on the layer. Fast R-CNN also takes a shorter time to test on a new image! It’s test time is dominated by the time it takes to create region proposals.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 620951,
          "key": "b4605891-7ee8-4e10-9681-02476ede15f5",
          "title": "Faster R-CNN",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b4605891-7ee8-4e10-9681-02476ede15f5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626581,
              "key": "86a861ed-93f1-400e-a76a-17b9df46276f",
              "title": "08 Faster RCNN V1 RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ySh_Q3KTTBY",
                "china_cdn_id": "ySh_Q3KTTBY.mp4"
              }
            },
            {
              "id": 626863,
              "key": "1cfdb153-d377-4e24-8b8f-b00153ffc013",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Region Proposal Network\n\nYou may be wondering: how exactly are the RoI's generated in the region proposal portion of the Faster R-CNN architecture?\n\nThe region proposal network (RPN) works in Faster R-CNN in a way that is similar to YOLO object detection, which you'll learn about in the next lesson. The RPN looks at the output of the last convolutional layer, a produced feature map, and takes a sliding window approach to possible-object detection. It slides a small (typically 3x3) window over the feature map, then for _each_ window the RPN:\n1. Uses a set of defined anchor boxes, which are boxes of a defined aspect ratio (wide and short or tall and thin, for example) to generate multiple possible RoI's, each of these is considered a region proposal.\n2. For each proposal, this network produces a probability, `Pc`, that classifies the region as an object (or not) and a set of bounding box coordinates for that object.\n3. Regions with too low a probability of being an object, say `Pc < 0.5`, are discarded.\n\n#### Training the Region Proposal Network\n\nSince, in this case, there are no ground truth regions, how do you train the region proposal network? \n\nThe idea is, for any region, you can check to see if it overlaps with any of the ground truth objects. That is, for a region, if we classify that region as an object or not-object, which class will it fall into? For a region proposal that does cover some portion of an object, we should say that there is a high probability that this region has an object init and that region should be kept; if the likelihood of an object being in a region is too low, that region should be discarded.\n\nI'd recommend [this blog post](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9) if you'd like to learn more about region selection.",
              "instructor_notes": ""
            },
            {
              "id": 621648,
              "key": "5c82dc93-228a-456f-a712-0596b5702209",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Speed Bottleneck\n\nNow, for all of these networks *including* Faster R-CNN, we've aimed to improve the speed of our object detection models by reducing the time it takes to generate and decide on region proposals. You might be wondering: is there a way to get rid of this proposal step entirely? And in the next section we'll see a method that does not rely on region proposals to work!",
              "instructor_notes": ""
            },
            {
              "id": 621657,
              "key": "e0b8d47a-8677-4f05-a376-4454d1587edd",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "e0b8d47a-8677-4f05-a376-4454d1587edd",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Why is speed important?\n\nMulti-object identification consists of locating and classifying different objects in an image. Some models trade off accuracy for speed. What kinds of applications require speed in object recognition? Check all that apply.",
                "answers": [
                  {
                    "id": "a1525392793388",
                    "text": "Accurately classifying images of cancerous vs. non-cancerous tissue.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525392887930",
                    "text": "Pedestrian detection for an autonomous vehicles.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525393172757",
                    "text": "Identifying people's faces in a set of images.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525393224349",
                    "text": "Tracking people's faces so that a camera can focus on them.",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 621683,
              "key": "93f9d550-53b1-43aa-a6c2-c069b9854ce4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Faster R-CNN Implementation\n\nIf you'd like to look at an implementation of this network in code, you can find a peer-reviewed version, at [this Github repo](https://github.com/jwyang/faster-rcnn.pytorch).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 620953,
          "key": "0117c47b-26b7-48eb-a837-7c1130fc1b3a",
          "title": "Detection With and Without Proposals",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0117c47b-26b7-48eb-a837-7c1130fc1b3a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626580,
              "key": "74194b70-a335-4c5b-948c-2afcb1b3714a",
              "title": "09 Detection Without Proposals Summary V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "IMnt1HFu_nc",
                "china_cdn_id": "IMnt1HFu_nc.mp4"
              }
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}