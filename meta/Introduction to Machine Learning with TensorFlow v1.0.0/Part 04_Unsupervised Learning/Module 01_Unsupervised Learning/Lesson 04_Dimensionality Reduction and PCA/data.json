{
  "data": {
    "lesson": {
      "id": 918879,
      "key": "e4062725-cf27-4653-b9bb-e7eb9039b799",
      "title": "Dimensionality Reduction and PCA",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Often we need to reduce a large number of features in our data to a smaller, more relevant set. Principal Component Analysis, or PCA, is a method of feature extraction and dimensionality reduction.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/e4062725-cf27-4653-b9bb-e7eb9039b799/918879/1581974089901/Dimensionality+Reduction+and+PCA+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/e4062725-cf27-4653-b9bb-e7eb9039b799/918879/1581974084684/Dimensionality+Reduction+and+PCA+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 608313,
          "key": "69aa045e-fcfc-4e77-8b19-7ae2d75fb6cf",
          "title": "Video: Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "69aa045e-fcfc-4e77-8b19-7ae2d75fb6cf",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652391,
              "key": "56f620c9-033d-4725-9f97-bfeed7e4143c",
              "title": "Introduction",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "tpFPcxoGxaE",
                "china_cdn_id": "tpFPcxoGxaE.mp4"
              }
            },
            {
              "id": 627787,
              "key": "0c619102-5191-4db1-8232-bb5a7d5ab65a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### What is PCA?\n\nHere you saw that PCA (or principal component analysis) is the first of the techniques you will see aimed at dimensionality reduction. This technique is about taking your full dataset and reducing it to only the parts that hold the most information.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608315,
          "key": "8ded9d67-caa4-4a0d-866c-e21014698f74",
          "title": "Video: Lesson Topics",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8ded9d67-caa4-4a0d-866c-e21014698f74",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652392,
              "key": "a1be344b-06d1-4a26-a605-4178e3f82815",
              "title": "Lesson Topics",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "LBzA08F_r4w",
                "china_cdn_id": "LBzA08F_r4w.mp4"
              }
            },
            {
              "id": 627788,
              "key": "fe494bde-9060-429c-83cb-4bf25696bb84",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### How Well Will You Understand PCA After This Lesson?\n\nThe goal is for everyone to leave this lesson with an understanding of:\n\n1. How PCA is used in the world.\n2. How to perform PCA in python.\n3. A conceptual understanding of how the algorithm works.\n4. How to interpret the results of PCA.\n\nIf you want to dive deeper into the mathematics, there will be additional links provided, but it will not be a main focus of this lesson.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608316,
          "key": "1c7d0881-59b4-4f37-b793-109553bd2d45",
          "title": "Text: Lesson Topics",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1c7d0881-59b4-4f37-b793-109553bd2d45",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627629,
              "key": "9868b1b7-1d7a-476a-99a1-84f879ecebe5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### PCA Lesson Topics\n\nThere is a lot to cover with Principal Component Analysis (or PCA).  However, you will gain a solid understanding of PCA by the end of this lesson, by applying this technique in a couple of scenarios using scikit-learn, and practicing interpreting the results.  \n\nWe will also cover conceptually how the algorithm works, and I will provide links to explore what is happening mathematically in case you want to dive in deeper!  Here is an outline of what you can expect in this lesson.\n\n### 1. **Dimensionality Reduction through Feature Selection and Feature Extraction** \nWith large datasets we often suffer with what is known as the \"curse of dimensionality,\" and need to reduce the number of features to effectively develop a model.  Feature Selection and Feature Extraction are two general approaches for reducing dimensionality. \n\n### 2. **Feature Extraction using PCA**\nPrincipal Component Analysis is a common method for extracting new \"latent features\" from our dataset, based on existing features. \n\n### 3. **Fitting PCA**\n In this part of the lesson, you will use PCA in scikit-learn to reduce the dimensionality of images of handwritten digits.  \n\n### 4. **Interpreting Results**\nOnce you are able to use PCA on a dataset, it is essential that you know how to interpret the results you get back.  There are two main parts to interpreting your results - the principal components themselves and the variability of the original data captured by those components.  You will get familiar with both.\n\n### 5. **Mini-project** \nFinally, you will put your skills to work on a new dataset.\n### 6. **Quick Recap** \nWe will do a quick recap, and you will be ready to use PCA for your own applications, as well as the project!\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608317,
          "key": "46efe647-a802-44ff-bc05-bfde3cdd010b",
          "title": "Video: Latent Features",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "46efe647-a802-44ff-bc05-bfde3cdd010b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652393,
              "key": "2b9449e9-e637-4f1d-898c-b6dc9500d138",
              "title": "Latent Features",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "kYLcVgpEwGs",
                "china_cdn_id": "kYLcVgpEwGs.mp4"
              }
            },
            {
              "id": 627789,
              "key": "ec1b138d-d7fa-4b73-b27d-a5d13f8f66d0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Latent Features\n\nLatent features are features that aren't explicitly in your dataset.  \n\nIn this example, you saw that the following features are all related to the latent feature **home size**:\n\n1. lot size\n2. number of rooms\n3. floor plan size\n4. size of garage\n5. number of bedrooms\n6. number of bathrooms\n\nSimilarly, the following features could be reduced to a single latent feature of **home neighborhood**:\n\n1. local crime rate\n2. number of schools in five miles\n3. property tax rate\n4. local median income\n5. average air quality index\n6. distance to highway\n\nSo even if our original dataset has the 12 features listed, we might be able to reduce this to only 2 latent features relating to the home size and home neighborhood.  ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608318,
          "key": "33a962d0-f194-4bf2-9705-8580af85f7f9",
          "title": "Latent Features",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "33a962d0-f194-4bf2-9705-8580af85f7f9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627792,
              "key": "9cd68615-1c52-4f4f-b9e7-cde3c27c674c",
              "title": "",
              "semantic_type": "MatchingQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "9cd68615-1c52-4f4f-b9e7-cde3c27c674c",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "complex_prompt": {
                  "text": "At the end of the last video, you saw some common questions asked on company surveys.  See if you can match each question to its corresponding latent feature."
                },
                "concepts_label": "Question",
                "answers_label": "Latent Feature",
                "concepts": [
                  {
                    "text": "I feel that my manager is supportive of my growth within the company.",
                    "correct_answer": {
                      "id": "a1526358279254",
                      "text": "Supportive work environment"
                    }
                  },
                  {
                    "text": "I feel confident that I can contribute to the company goals.",
                    "correct_answer": {
                      "id": "a1526361186714",
                      "text": "Confidence to make an impact"
                    }
                  },
                  {
                    "text": "I feel that my work contributes positively to the world.",
                    "correct_answer": {
                      "id": "a1526361420758",
                      "text": "Confidence to make an impact"
                    }
                  },
                  {
                    "text": "I feel that my opinions are heard when making decisions about company decisions.",
                    "correct_answer": {
                      "id": "a1526361454557",
                      "text": "Supportive work environment"
                    }
                  },
                  {
                    "text": "My peers are supportive of my contributions.",
                    "correct_answer": {
                      "id": "a1526361613340",
                      "text": "Supportive work environment"
                    }
                  }
                ],
                "answers": [
                  {
                    "id": "a1526361420758",
                    "text": "Confidence to make an impact"
                  },
                  {
                    "id": "a1526361454557",
                    "text": "Supportive work environment"
                  },
                  {
                    "id": "a1526362575869",
                    "text": "Confidence to make an impact"
                  },
                  {
                    "id": "a1526361613340",
                    "text": "Supportive work environment"
                  },
                  {
                    "id": "a1526358279254",
                    "text": "Supportive work environment"
                  },
                  {
                    "id": "a1526361186714",
                    "text": "Confidence to make an impact"
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 608319,
          "key": "7e8947f4-f122-42bf-b058-96e21b0044e5",
          "title": "Video: How to Reduce Features?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7e8947f4-f122-42bf-b058-96e21b0044e5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652394,
              "key": "6a793c2a-0148-45b3-a9c2-56e4f9d1f815",
              "title": "How to Reduce Features",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ydhrelgjriI",
                "china_cdn_id": "ydhrelgjriI.mp4"
              }
            },
            {
              "id": 797535,
              "key": "fdb9b787-b8c0-49f6-a6e7-bc0b15c30711",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Reducing the Number of Features - Dimensionality Reduction\n\n\nOur real estate example is great to help develop an understanding of feature reduction and latent features. But we have a smallish number of features in this example, so it's not clear why it's so necessary to reduce the number of features. And in this case it wouldn't actually be required - we could handle all six original features to create a model.\n\nBut the [\"curse of dimensionality\"](https://en.wikipedia.org/wiki/Curse_of_dimensionality) becomes more clear when we're grappling with large real-world datasets that might involve hundreds or thousands of features, and to effectively develop a model really requires us to reduce our number of dimensions.\n\n## Two Approaches : Feature Selection and Feature Extraction\n### Feature Selection\nFeature Selection involves finding a **subset** of the original features of your data that you determine are most relevant and useful. In the example image below, taken from the video, notice that \"floor plan size\" and \"local crime rate\" are features that we have selected as a subset of the original data.\n",
              "instructor_notes": ""
            },
            {
              "id": 797537,
              "key": "7e76a960-dbbf-47b0-935f-4451da7b98fe",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/December/5c1bed13_screen-shot-2018-12-20-at-11.27.05-am/screen-shot-2018-12-20-at-11.27.05-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7e76a960-dbbf-47b0-935f-4451da7b98fe",
              "caption": "Example of Feature Selection",
              "alt": "",
              "width": 900,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 797536,
              "key": "ae91553a-354a-4bd0-813f-39bf1b442b94",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "##### Methods of Feature Selection:\n- **Filter methods** - Filtering approaches use a ranking or sorting algorithm to filter out those features that have less usefulness. Filter methods are based on discerning some inherent correlations among the feature data in unsupervised learning, or on correlations with the output variable in supervised settings. Filter methods are usually applied as a preprocessing step. Common tools for determining correlations in filter methods include: **Pearson's Correlation**, **Linear Discriminant Analysis (LDA)**, and **Analysis of Variance (ANOVA)**. \n- **Wrapper methods** - Wrapper approaches generally select features by directly testing their impact on the performance of a model. The idea is to \"wrap\" this procedure around your algorithm, repeatedly calling the algorithm using different subsets of features, and measuring the performance of each model. Cross-validation is used across these multiple tests. The features that produce the best models are selected. Clearly this is a computationally expensive approach for finding the best performing subset of features, since they have to make a number of calls to the learning algorithm. Common examples of wrapper methods are: **Forward Search**, **Backward Search**, and **Recursive Feature Elimination**. \n\nScikit-learn has a [feature selection module](https://scikit-learn.org/stable/modules/feature_selection.html) that offers a variety of methods to improve model accuracy scores or to boost their performance on very high-dimensional datasets.\n\n### Feature Extraction\nFeature Extraction involves extracting, or constructing, new features called **latent features**. In the example image below, taken from the video, \"Size Feature\" and \"Neighborhood Quality Feature\" are new latent features, extracted from the original input data. ",
              "instructor_notes": ""
            },
            {
              "id": 797538,
              "key": "0f3a35ad-9285-4e56-925c-0c63268acf30",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/December/5c1bee1f_screen-shot-2018-12-20-at-11.28.50-am/screen-shot-2018-12-20-at-11.28.50-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0f3a35ad-9285-4e56-925c-0c63268acf30",
              "caption": "Example of Feature Extraction",
              "alt": "",
              "width": 900,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 627795,
              "key": "aa34b03b-4f3c-4dc3-afe9-b06da8307d82",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "#### Methods of Feature Extraction\n\nConstructing latent features is exactly the goal of **Principal Component Analysis (PCA)**, which we'll explore throughout the rest of this lesson.\n\nOther methods for accomplishing Feature Extraction include **Independent Component Analysis (ICA)** and **Random Projection**, which we will study in the following lesson.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 797559,
              "key": "9eb83900-4505-4584-8bc5-75760a8b630b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Further Exploration\n\nIf you're interested in deeper study of these topics, here are a couple of helpful blog posts and a research paper:\n- https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/\n- https://elitedatascience.com/dimensionality-reduction-algorithms\n- http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608365,
          "key": "ea7037d6-5871-4d1b-b5bc-90d9226f21f6",
          "title": "Video: Dimensionality Reduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ea7037d6-5871-4d1b-b5bc-90d9226f21f6",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652395,
              "key": "bead025d-317b-4d81-a8d4-444c1e3c9702",
              "title": "Dimensionality Reduction",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "mANti9veGtc",
                "china_cdn_id": "mANti9veGtc.mp4"
              }
            },
            {
              "id": 627928,
              "key": "267c1d65-c97b-4456-b5e2-2b0aec0538e3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Principal Components\n\nA few takeaways from this video:\n\n1. An advantage of Feature Extraction over Feature Selection is that the latent features can be constructed to incorporate data from multiple features, and thus retain more information present in the various original inputs, than just losing that information by dropping many original inputs.\n\n2. **Principal components** are linear combinations of the original features in a dataset that aim to retain the most information in the original data.\n\n3. You can think of a **principal component** in the same way that you think about a **latent feature**.  \n\nThe general approach to this problem of high-dimensional datasets is to search for a **projection** of the data onto a smaller number of  features which preserves the information as much as possible.\n\nWe'll take a closer look in the rest of this lesson.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 613555,
              "key": "d4f2bfb8-494c-4a4b-9857-25c84a77d5f4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "  ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 608367,
          "key": "2c834fb2-23d6-4e1d-b30c-03141f80b561",
          "title": "Video: PCA Properties",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2c834fb2-23d6-4e1d-b30c-03141f80b561",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652396,
              "key": "5568eaf6-c30a-46ce-aad8-a0ccf594c3af",
              "title": "PCA Properties",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "1oaaq-0wdB0",
                "china_cdn_id": "1oaaq-0wdB0.mp4"
              }
            },
            {
              "id": 627940,
              "key": "bf47ab10-e115-47a2-8e51-e8008b83df5a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Principal Component Properties\n\nThere are two main properties of **principal components**:\n\n1. **They retain the most amount of information in the dataset.**  In this video, you saw that  retaining the most information in the dataset meant finding a line that reduced the distances of the points to the component across all the points (same as in regression!).  \n\n2. **The created components are orthogonal to one another.**  So far we have been mostly focused on what the first component of a dataset would look like.  However, when there are many components, the additional components will all be orthogonal to one another.  Depending on how the components are used, there are benefits to having orthogonal components.  In regression, we often would like independent features, so using the components in regression now guarantees this.\n\n[This is a great post answering a number of common questions on PCA.](https://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 627807,
          "key": "fdc44d96-23a4-45d7-99cf-eb214d10513f",
          "title": "Quiz: How Does PCA Work?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fdc44d96-23a4-45d7-99cf-eb214d10513f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627941,
              "key": "614b991f-dec3-4712-bf76-b6d08caaa9f2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Check Your Understanding\n\nBefore getting into the code, let's check some of the main ideas of PCA.",
              "instructor_notes": ""
            },
            {
              "id": 627949,
              "key": "80e8b8d4-204b-4d0d-934a-ca7e731ec55f",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "80e8b8d4-204b-4d0d-934a-ca7e731ec55f",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following best describes the goal of PCA?",
                "answers": [
                  {
                    "id": "a1526408605373",
                    "text": "To reduce the dimensionality of an existing dataset to a smaller number of features.",
                    "is_correct": true
                  },
                  {
                    "id": "a1526408639141",
                    "text": "To create a new dataset.",
                    "is_correct": false
                  },
                  {
                    "id": "a1526408781482",
                    "text": "To maximize the variability in a dataset.",
                    "is_correct": false
                  },
                  {
                    "id": "a1526408794648",
                    "text": "None of the above are related to the goal of PCA.",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 627960,
              "key": "c939f8bc-21ab-47a6-bdcf-21fc345d7e56",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "c939f8bc-21ab-47a6-bdcf-21fc345d7e56",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "You can also think of a **principal component** as a... (select all that apply)",
                "answers": [
                  {
                    "id": "a1526409290264",
                    "text": "linear combination of the original features in a dataset.",
                    "is_correct": true
                  },
                  {
                    "id": "a1526409319358",
                    "text": "latent variable.",
                    "is_correct": true
                  },
                  {
                    "id": "a1526409662426",
                    "text": "a new feature that can be used in a future analysis.",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 627970,
              "key": "7cd7aed1-d0d4-4240-a6a5-5081b79154b1",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "7cd7aed1-d0d4-4240-a6a5-5081b79154b1",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "The amount of information lost when performing PCA can be thought of as",
                "answers": [
                  {
                    "id": "a1526409987364",
                    "text": "the variability in the original data.",
                    "is_correct": false
                  },
                  {
                    "id": "a1526410043889",
                    "text": "the difference between the value we predict and the value that is actually calculated.",
                    "is_correct": false
                  },
                  {
                    "id": "a1526412017823",
                    "text": "the distance from the original points to the created component.",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 608368,
          "key": "83ac579a-dfee-43b4-a6c3-6d0a33380d05",
          "title": "Screencast: PCA",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "83ac579a-dfee-43b4-a6c3-6d0a33380d05",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627981,
              "key": "d6fe142e-2f6f-4d57-9547-a5438a187541",
              "title": "09  PCA V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "0RLDZWeq5JE",
                "china_cdn_id": "0RLDZWeq5JE.mp4"
              }
            },
            {
              "id": 613153,
              "key": "c6c0464f-6d2f-4d8c-a8a4-b1df6ac35cf1",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "3854ccca-7824-4f9b-8fa6-ddb1e532074a",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/PCA_SC.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 608371,
          "key": "ec47206e-4b7b-4437-94b3-71197c805de9",
          "title": "Notebook: PCA - Your Turn",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ec47206e-4b7b-4437-94b3-71197c805de9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 613180,
              "key": "c5bfebc2-60c6-41a6-b661-645cdb4b8ce2",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "2baccbcb-0b91-47a7-903d-a378dea66b4e",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/PCA_1.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 608374,
          "key": "319476e4-4000-45c5-84d8-1474485b4df7",
          "title": "Screencast: PCA Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "319476e4-4000-45c5-84d8-1474485b4df7",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627982,
              "key": "4c39faea-2d54-4daf-a867-c262fba8f695",
              "title": "11  PCA 1 Solution V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "u0rJRmubQ44",
                "china_cdn_id": "u0rJRmubQ44.mp4"
              }
            },
            {
              "id": 614315,
              "key": "9f133dd1-3d89-4459-9684-79e06dd44e76",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "7e6f6cd2-2557-42a9-bbed-b96fb6aec066",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/PCA_1_Solution.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 608376,
          "key": "8fb00610-ab64-4da3-b142-4f991012d45a",
          "title": "Screencast: Interpret PCA Results",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8fb00610-ab64-4da3-b142-4f991012d45a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627983,
              "key": "18cca2d4-ed9a-4141-89d6-38689b76f9fe",
              "title": "12  Interpret PCA  Results V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ZX6EACfsZbc",
                "china_cdn_id": "ZX6EACfsZbc.mp4"
              }
            },
            {
              "id": 614318,
              "key": "cf55bf90-dd57-4146-a8e3-452c09bf8fcb",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "930b570b-7ae5-4851-83bf-4b50f2119d06",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Interpret_PCA_Results_SC.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 608377,
          "key": "bd7693da-3172-4ae7-ba23-0c0511a4d25b",
          "title": "Notebook: Interpretation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "bd7693da-3172-4ae7-ba23-0c0511a4d25b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 614321,
              "key": "214de2b7-0be9-410a-a404-d674ee9842d8",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "ca27a72a-163d-4137-a8ca-4b4ad96b0110",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Interpret_PCA_Results.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 608378,
          "key": "5e1fbdd3-b9ff-4f48-aa4a-9727bc868e8a",
          "title": "Screencast: Interpretation Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5e1fbdd3-b9ff-4f48-aa4a-9727bc868e8a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627984,
              "key": "f5faa90e-62ba-4b06-a243-15569a43a997",
              "title": "14  Interpretation Solution V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "wU2duZa0ds0",
                "china_cdn_id": "wU2duZa0ds0.mp4"
              }
            },
            {
              "id": 614322,
              "key": "742c05b2-bf72-4dc7-b42f-3365294ea189",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "ed6bd228-5fa6-4f95-b97f-f830584965b3",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Interpret_PCA_Results_Solution.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 613543,
          "key": "c086e658-4e1d-4257-95d3-ea73e2d634db",
          "title": "Text: What Are EigenValues & EigenVectors?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c086e658-4e1d-4257-95d3-ea73e2d634db",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 615078,
              "key": "a29ef98e-1c8d-4386-8af6-1ee7eef41008",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### What Are Eigenvalues and Eigenvectors?\n\nThe mathematics of PCA isn't really necessary for PCA to be useful.  However, it can be useful to fully understand the mathematics of a technique to understand how it might be extended to new cases.  For this reason, the page has a few additional references which go more into the mathematics of PCA.\n\nA simple introduction of what PCA is aimed to accomplish is provided [here in a simple example](https://www.youtube.com/watch?v=HH8pouRwphA).\n\nA nice visual, and mathematical, illustration of PCA is provided in [this video by 3 blue 1 brown](https://www.youtube.com/watch?v=PFDu9oVAE-g).\n\nIf you dive into the literature surrounding PCA, you will without a doubt run into the language of eigenvalues and eigenvectors.  These are just the math-y words for things you have already encountered in this lesson.  \n\nAn eigenvalue is the same as the amount of variability captured by a principal component, and an eigenvector is the principal component itself.  To see more on these ideas, take a look at the following three links below:\n\n[A great introduction into the mathematics of principal components analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf).\n\n[An example of using PCA in python by one of my favorite data scientists.](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n\n[An example of PCA from the scikit learn documentation.](http://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py)\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 613544,
          "key": "2d00547b-cc33-40aa-8152-d2fbbff43f0b",
          "title": "Video: When to Use PCA?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2d00547b-cc33-40aa-8152-d2fbbff43f0b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652397,
              "key": "c35ef30d-3b33-4e3b-a1cf-b0c8b9104c63",
              "title": "When to Use PCA",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "arSP83-CM6w",
                "china_cdn_id": "arSP83-CM6w.mp4"
              }
            },
            {
              "id": 628048,
              "key": "8c2141bc-50ff-41cd-bd56-3caf38f98af3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Where is PCA Used?\n\nIn general, PCA is used to **reduce the dimensionality of your data**.  Here are links to some specific use cases beyond what you covered in this lesson:\n\n1. PCA for [microarray data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2669932/).\n2. PCA for [anomaly detection](https://arxiv.org/pdf/1801.01571.pdf).\n3. PCA for [time series data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.853.2380&rep=rep1&type=pdf).\n\nIf you ever feel overwhelmed by the amount of data you have, you can look to PCA to reduce the size of your dataset, while still retaining the maximum amount of information (though this does often come at the cost of reducing your data interpretability).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 613545,
          "key": "ce2e83db-cba2-4c2e-a726-5ab15971ed2f",
          "title": "Video: Recap",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ce2e83db-cba2-4c2e-a726-5ab15971ed2f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652398,
              "key": "e99fc4f0-578f-418b-a514-f192764d83da",
              "title": "17 PCA Recap V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Egz3-noHCmg",
                "china_cdn_id": "Egz3-noHCmg.mp4"
              }
            }
          ]
        },
        {
          "id": 613546,
          "key": "89f79b90-38bc-4bcd-bf6b-7d4f5cdd3572",
          "title": "Notebook: Mini-Project",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "89f79b90-38bc-4bcd-bf6b-7d4f5cdd3572",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 615017,
              "key": "aae2a968-20e1-4273-88b1-c977a1169fe5",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "d582ad9e-8577-49f9-bc50-7e158e5b667b",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/PCA_Mini_Project.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 613547,
          "key": "278e49ea-164c-4b9e-ab81-5da569a6119f",
          "title": "Mini-Project Solution",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "278e49ea-164c-4b9e-ab81-5da569a6119f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 614935,
              "key": "98ff0bbc-4b6e-4adb-9574-df8e725e0619",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view3854ccca",
              "pool_id": "jupyter",
              "view_id": "0a57ec29-5647-4ab3-99b4-eb86a7f6de90",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/PCA_Mini_Project_Solution.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 613548,
          "key": "96359c0d-dc77-462f-8f10-33fb06115a33",
          "title": "Video: Outro",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "96359c0d-dc77-462f-8f10-33fb06115a33",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 652399,
              "key": "6eeb3341-631f-4c58-aa0d-1bfb1a370929",
              "title": "Outro",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "CuIqzL8HjI8",
                "china_cdn_id": "CuIqzL8HjI8.mp4"
              }
            }
          ]
        },
        {
          "id": 613549,
          "key": "59171fd4-3476-4785-ab4b-4183acbfc6ff",
          "title": "Text: Recap",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "59171fd4-3476-4785-ab4b-4183acbfc6ff",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 627786,
              "key": "a2fa8c3d-0bcc-4279-ab85-94e480514b66",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Dimensionality Reduction and PCA - Lesson Topics\n\nPhew!  That was a ton of information - here is a quick recap!\n\n### 1. **Two Methods for Dimensionality Reduction**\nYou learned that **Feature Selection** and **Feature Extraction** are two general approaches for reducing the number of features in your data. Feature Selection processes result in a subset of the most significant original features in the data, while Feature Extraction methods like PCA construct new latent features that well represent the original data.\n\n### 2. **Dimensionality Reduction and Principal Components** \n\nYou learned that Principal Component Analysis (PCA) is a technique that is used to reduce the dimensionality of your dataset. The reduced features are called **principal components**, or **latent features**.  These **principal components** are simply a linear combination of the original features in your dataset.\n\nYou learned that these components have two major properties:\n\n1. They aim to capture the most amount of variability in the original dataset.\n2. They are orthogonal to (independent of) one another.\n\n### 3. **Fitting PCA**\n\nOnce you got the gist of what PCA was doing, we used it on handwritten digits within scikit-learn.\n\nWe did this all within a function called `do_pca`, which returned the PCA model, as well as the reduced feature matrix.  You simply passed in the number of features you wanted back, as well as the original dataset.\n\n### 4. **Interpreting Results**\n\nYou then saw there are two major parts to interpreting the PCA results:\n\n1. The **variance explained** by each component. You were able to visualize this with scree plots to understand how many components you might keep based on how much information was being retained.\n2. The **principal components** themselves, which gave us an idea of which original features were most related to why a component was able to explain certain aspects about the original datasets.\n\n### 5. **Mini-project** \nFinally, you applied PCA to a dataset on vehicle information. You gained valuable experience using scikit-learn, as well as interpreting the results of PCA.\n\nWith mastery of these skills, you are now ready to use PCA for any task in which you feel it may be useful. If you have a large amount of data, and are feeling afflicted by the curse of dimensionality, you want to reduce your data to a smaller number of latent features, and you know just the way to do it!\n\n\n### 6. **Do you think you understand PCA well enough yet to explain it in a way that would make sense to your grandmother?**\n\n Here is an interesting StackExchange post that does just that, and with animated graphics!\nhttps://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}