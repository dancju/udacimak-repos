{
  "data": {
    "lesson": {
      "id": 502040,
      "key": "7fa776a7-b884-401c-b954-6bb2a55243a6",
      "title": "RNN's",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Explore how memory can be incorporated into a deep learning model using recurrent neural networks (RNNs). Learn how RNNs can learn from and generate ordered sequences of data.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": null,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/7fa776a7-b884-401c-b954-6bb2a55243a6/502040/1544453005730/RNN%27s+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/7fa776a7-b884-401c-b954-6bb2a55243a6/502040/1544452999654/RNN%27s+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 502136,
          "key": "1a47863c-444c-4320-abf4-ab2600935427",
          "title": "RNN's in Computer Vision",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1a47863c-444c-4320-abf4-ab2600935427",
            "completed_at": "2020-04-01T05:29:00.331Z",
            "last_viewed_at": "2020-04-01T05:28:59.119Z",
            "unstructured": null
          },
          "resources": {
            "files": [
              {
                "name": "Video classification methods",
                "uri": "https://video.udacity-data.com/topher/2018/May/5af0e03b_video-classification/video-classification.pdf"
              }
            ],
            "google_plus_link": null,
            "career_resource_center_link": null,
            "coaching_appointments_link": null,
            "office_hours_link": null,
            "aws_provisioning_link": null
          },
          "atoms": [
            {
              "id": 626114,
              "key": "9cc37639-5831-484f-a7a8-f958f3e92d73",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Recurrent Neural Networks\n\nSo far, we've been looking at convolutional neural networks and models that allows us to analyze the spatial information in a given input image. CNN's excel in tasks that rely on finding spatial and visible patterns in training data.\n\nIn this and the next couple lessons, we'll be reviewing RNN's or recurrent neural networks. These networks give us a way to incorporate **memory** into our neural networks, and will be critical in analyzing sequential data. RNN's are most often associated with text processing and text generation because of the way sentences are structured as a sequence of words, but they are also useful in a number of computer vision applications, as well!\n\n### RNN's in Computer Vision\n\nAt the end of this lesson, you will be tasked with creating an automatic image captioning model that takes in an image as input and outputs a *sequence* of words, describing that image. Image captions are used to create accessible content and in a number of other cases where one may want to read about the contents of an image. This model will include a CNN component for finding spatial patterns in the input image *and* and RNN component that will be responsible for generative descriptive text!\n\nRNN's are also sometimes used to analyze sequences of images; this can be useful in captioning video, as well as video classification, gesture recognition, and object tracking; all of these tasks see as input a *sequence* of image frames.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 626118,
              "key": "67df841b-20bf-4026-8d50-b8edbf2a0124",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Sketch RNN\n\nOne of my favorite use cases for RNN's in computer vision tasks is in generating drawings. [Sketch RNN (demo here)](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html) is a program that learns to complete a drawing, once you give it something (a line or circle, etc.) to start!",
              "instructor_notes": ""
            },
            {
              "id": 626119,
              "key": "6aea38c1-6b9b-4fc0-8de8-8708655b5dc4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5af0dfc7_screen-shot-2018-05-07-at-4.20.50-pm/screen-shot-2018-05-07-at-4.20.50-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6aea38c1-6b9b-4fc0-8de8-8708655b5dc4",
              "caption": "Sketch RNN example output. Left, Mona Lisa. Right, pineapple.",
              "alt": "",
              "width": 400,
              "height": 782,
              "instructor_notes": null
            },
            {
              "id": 626122,
              "key": "5c579757-6a53-47fd-b8a5-121d1bf80953",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "It's interesting to think of drawing as a sequential act, but it is! This network takes a starting line or squiggle and then, having trained on a number of types of sketches, does it's best to complete the drawing based on your input squiggle.",
              "instructor_notes": ""
            },
            {
              "id": 626125,
              "key": "2c2dd2d6-ab3a-43d2-b0e5-dea5ddc9e8a1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Next, you'll learn all about how RNN's are structured and how they can be trained! This section is taught by Ortal, who has a PhD in Computer Engineering and has been a professor and researcher in the fields of applied cryptography and embedded systems.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 384343,
          "key": "749d84c4-681c-4b47-b98e-b32598e5f072",
          "title": "RNN Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "749d84c4-681c-4b47-b98e-b32598e5f072",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 439854,
              "key": "a148da3d-d07d-4ff0-820c-4eef52a27ada",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# RNN Introduction ",
              "instructor_notes": ""
            },
            {
              "id": 430448,
              "key": "29274b0c-0857-40c5-9d16-95001567a107",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Hi! I am Ortal, your instructor for this lesson!\n\nIn this lesson we will learn about **Recurrent Neural Networks (RNNs)**.\n\nThe neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous inputs when generating the current output.  In other words, our systems did not have any **memory** elements. RNNs address this very basic and important issue by using **memory** (i.e. past inputs to the network) when producing the current output.  \n\n",
              "instructor_notes": ""
            },
            {
              "id": 474164,
              "key": "32544bb6-19b0-467e-87d6-6d92d2c205c8",
              "title": "01 RNN Intro V6 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "AIQEqg6F38A",
                "china_cdn_id": "AIQEqg6F38A.mp4"
              }
            }
          ]
        },
        {
          "id": 384344,
          "key": "7b7c1680-19e1-46c5-a48a-143ad051254c",
          "title": "RNN History",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7b7c1680-19e1-46c5-a48a-143ad051254c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 439855,
              "key": "2bd124e8-6da6-4794-a59c-c05b8b4e2240",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A bit of history",
              "instructor_notes": ""
            },
            {
              "id": 430468,
              "key": "f97efd4d-7097-47cc-9961-090b10529c87",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "How did the theory behind RNN evolve? Where were we a few years ago and where are we now? ",
              "instructor_notes": ""
            },
            {
              "id": 474166,
              "key": "03b7f469-9a95-4935-b5dd-9703cd22390e",
              "title": "02 RNN History V4 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "HbxAnYUfRnc",
                "china_cdn_id": "HbxAnYUfRnc.mp4"
              }
            },
            {
              "id": 434386,
              "key": "d59d889d-24a5-4963-8784-770dace7c8cd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As mentioned in this video, RNNs have a key flaw, as capturing relationships that span more than 8 or 10 steps back is practically impossible. This flaw stems from the \"**vanishing gradient**\" problem in which the contribution of information decays geometrically over time. \n\nWhat does this mean?\n\n As you may recall, while training our network we use **backpropagation**. In the backpropagation process we adjust our weight matrices with the use of a **gradient**. In the process, gradients are calculated by continuous multiplications of derivatives.  The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\". \n\n**LSTM** is one option to overcome the Vanishing Gradient problem in RNNs. \n\nPlease use these resources if you would like to read more about the [Vanishing Gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem or understand further the concept of a [Geometric Series](https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions) and how its values may exponentially decrease. \n",
              "instructor_notes": ""
            },
            {
              "id": 431208,
              "key": "c7505c36-5bff-4093-8ae7-b19e32e677f5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you are still curious, for more information on the important milestones mentioned here, please take a peek at the following links:\n\n- [TDNN](https://en.wikipedia.org/wiki/Time_delay_neural_network)\n\n- Here is the original [Elman Network](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).\n\n- In this [LSTM](http://www.bioinf.jku.at/publications/older/2604.pdf) link you will find the original paper written by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and   [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/). Don't get into all the details just yet. We will cover all of this later!\n\nAs mentioned in the video, Long Short-Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies.  In this lesson we will focus on RNNs and continue with LSTMs. We will not be focusing on GRUs. \nMore information about GRUs can be found in the following [blog](https://deeplearning4j.org/lstm.html). Focus on the overview titled: **GRUs**.\n\n\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 412962,
          "key": "6d454d80-ea18-40cf-bc21-459ece28a5c5",
          "title": "RNN Applications",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6d454d80-ea18-40cf-bc21-459ece28a5c5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 439863,
              "key": "0b99a13a-c715-487e-823e-a6e596585517",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Applications",
              "instructor_notes": ""
            },
            {
              "id": 430470,
              "key": "d1005731-cd6d-4bc8-9f9e-763113a493c3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The world's leading tech companies are all using RNNs, particularly LSTMs, in their applications. Let's take a look at a few.",
              "instructor_notes": ""
            },
            {
              "id": 474191,
              "key": "d63905f4-ab82-44eb-83c9-1d33579d4f24",
              "title": "03 RNN Applications V3 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "6JbTNARuKII",
                "china_cdn_id": "6JbTNARuKII.mp4"
              }
            },
            {
              "id": 430476,
              "key": "c1787ac5-5862-4035-9b56-6ec52100f5bc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "There are so many interesting applications, let's look at a few more!\n\n  -  Are you into gaming and bots? Check out the [DotA  2 bot by Open AI](https://blog.openai.com/dota-2/)\n\n- How about [automatically adding sounds to silent movies?](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)\n\n\n- Here is a cool tool for [automatic handwriting generation](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3)\n\n\n- Amazon's  voice to text using high quality speech recognition,  [Amazon Lex](https://aws.amazon.com/lex/faqs/).\n\n\n- Facebook uses RNN and LSTM technologies for [building language models](https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/)\n\n\n- Netflix also uses RNN models - [ here is an interesting read](https://arxiv.org/pdf/1511.06939.pdf)",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420905,
          "key": "ebec6246-752c-49ef-bf74-423b8c3683f3",
          "title": "Feedforward Neural Network-Reminder",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ebec6246-752c-49ef-bf74-423b8c3683f3",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 440354,
              "key": "9e9b9934-382d-45f9-9d91-86fc72a66ead",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Feedforward Neural Network - A Reminder",
              "instructor_notes": ""
            },
            {
              "id": 430803,
              "key": "9b265939-bc3e-457d-a07a-61e6c8c287c3",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59de877f_screen-shot-2017-10-11-at-2.04.14-pm/screen-shot-2017-10-11-at-2.04.14-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9b265939-bc3e-457d-a07a-61e6c8c287c3",
              "caption": "",
              "alt": "",
              "width": 500,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 430802,
              "key": "6f4dd8bb-8dac-40f0-8d0b-0c8433cafecc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The mathematical calculations needed for training RNN systems are fascinating. To deeply understand the process, we first need to feel confident with the vanilla FFNN system. We need to thoroughly understand the feedforward process, as well as the backpropagation process used in the training phases of such systems. \nThe next few videos will cover these topics, which you are already familiar with. We will address the feedforward process as well as backpropagation, using specific examples. These examples will serve as extra content to help further understand RNNs later in this lesson.\n\nThe following couple of videos will give you a brief overview of the **Feedforward Neural Network (FFNN)**.",
              "instructor_notes": ""
            },
            {
              "id": 474173,
              "key": "dd058d94-1deb-4717-81ec-077c0f41cd12",
              "title": "04 RNN FFNN Reminder A V7 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_vrp2lZjXf0",
                "china_cdn_id": "_vrp2lZjXf0.mp4"
              }
            },
            {
              "id": 431468,
              "key": "cb3bfbb7-82ab-47c3-93e0-5096310237d7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "OK, you can take a small break now. We will continue with FFNN when you come back! ",
              "instructor_notes": ""
            },
            {
              "id": 474174,
              "key": "0a320478-f1cd-4ee2-908e-89111c3a8b50",
              "title": "05 RNN FFNN Reminder B V6 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "FfPjaGcZODc",
                "china_cdn_id": "FfPjaGcZODc.mp4"
              }
            },
            {
              "id": 440851,
              "key": "48530ea0-6aef-436d-9652-a3c1d354d358",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As mentioned before, when working with neural networks we have 2 primary phases: \n\n**Training** \n\nand\n\n **Evaluation**.\n\nDuring the **training** phase, we take the data set (also called the *training set*), which includes many pairs of inputs and their corresponding targets (outputs). Our goal is to find a set of weights that would best map the inputs to the desired outputs.\nIn the **evaluation** phase, we use the network that was created in the training phase, apply our new inputs and expect to obtain the desired outputs.\n\nThe training phase will include two steps:\n\n **Feedforward**\n\n and \n\n**Backpropagation**\n\nWe will repeat these steps as many times as we need until we decide that our system has reached the best set of weights, giving us the best possible outputs.\n\nThe next two videos will focus on the feedforward process.",
              "instructor_notes": ""
            },
            {
              "id": 474106,
              "key": "1d64935a-8b42-4a1d-88a5-3961a37d668a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You will notice that in these videos I use subscripts as well as superscript as a numeric notation for the weight matrix.\n\nFor example:\n-  <span class=\"mathquill\"> W_k</span> is weight matrix <span class=\"mathquill\"> k</span> \n- <span class=\"mathquill\">\\ W_{ij}^k</span>  is the <span class=\"mathquill\"> ij</span>  element of weight matrix  <span class=\"mathquill\"> k</span> ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 384346,
          "key": "29a326a7-32b9-46bb-93aa-10a09acef7c0",
          "title": "The Feedforward Process",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "29a326a7-32b9-46bb-93aa-10a09acef7c0",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 440359,
              "key": "696f1a11-afe8-4880-89aa-2014d483f3c7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Feedforward",
              "instructor_notes": ""
            },
            {
              "id": 431471,
              "key": "92c6c860-d585-4cb3-98db-340928aead10",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nIn this section we will look closely at the math behind the feedforward process. With the use of basic Linear Algebra tools, these calculations are pretty simple! \n\nIf you are not feeling confident with linear combinations and matrix multiplications, you  can use the following links as a refresher:\n- [Linear Combination](http://linear.ups.edu/html/section-LC.html)\n- [Matrix Multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)\n\n\n\n\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 440361,
              "key": "45930311-6ef3-47f6-b412-6babf88305cf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Assuming that we have a single hidden layer, we will need two steps in our calculations. The first will be calculating the value of the hidden states and the latter will be calculating the value of the outputs.\n",
              "instructor_notes": ""
            },
            {
              "id": 440365,
              "key": "e96fb5bd-1bb5-4f7c-8f15-e4d40c7e0e68",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f39785_screen-shot-2017-10-27-at-1.29.13-pm/screen-shot-2017-10-27-at-1.29.13-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e96fb5bd-1bb5-4f7c-8f15-e4d40c7e0e68",
              "caption": "",
              "alt": "",
              "width": 600,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 440364,
              "key": "91379207-b2a6-43a8-8155-cc5ab80b89b1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice that both the hidden layer and the output layer are displayed as vectors, as they are both represented by more than a single neuron. ",
              "instructor_notes": ""
            },
            {
              "id": 436097,
              "key": "b0db56e7-0171-4f8c-aa3b-2de8225c4381",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n\nOur first video will help you understand the first step- **Calculating the value of the hidden states**.",
              "instructor_notes": ""
            },
            {
              "id": 409935,
              "key": "3b360960-2df4-429d-9e4f-39d539010ea0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " ",
              "instructor_notes": ""
            },
            {
              "id": 474175,
              "key": "589a2926-e1d5-45ea-b7fb-7666f3bf8584",
              "title": "06 FeedForward A V7 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "4rCfnWbx8-0",
                "china_cdn_id": "4rCfnWbx8-0.mp4"
              }
            },
            {
              "id": 440387,
              "key": "6b23b3be-b184-405c-98f1-19575c182482",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As you saw in the video above, vector h' of the hidden layer will be calculated by multiplying the input vector with the weight matrix  <span class=\"mathquill\">W^{1}</span> the following way:\n\n<span class=\"mathquill\">\\bar{h'} = (\\bar{x}  W^1 )</span>\n\nUsing vector by matrix multiplication, we can look at this computation the following way:",
              "instructor_notes": ""
            },
            {
              "id": 468939,
              "key": "d64f9f34-2e3a-4b68-bb3e-1fd2b88c7245",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a26f9f2_screen-shot-2017-12-05-at-11.55.58-am/screen-shot-2017-12-05-at-11.55.58-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d64f9f34-2e3a-4b68-bb3e-1fd2b88c7245",
              "caption": "_Equation 1_",
              "alt": "",
              "width": 400,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 440473,
              "key": "162d9ea0-0743-4b6e-b68a-ea1c85c334b6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nAfter finding <span class=\"mathquill\">h'</span> , we need an activation function (<span class=\"mathquill\">\n \\Phi</span>) to finalize the computation of the hidden layer's values. This activation function can be a Hyperbolic Tangent, a Sigmoid or a ReLU function. We can use the following two equations to express the final hidden vector <span class=\"mathquill\">\n\\bar{h}</span>:\n\n<span class=\"mathquill\">\\bar{h} = \\Phi(\\bar{x}  W^1 )</span> \n\nor\n \n <span class=\"mathquill\"> \\bar{h} = \\Phi(h')</span>\n\n\nSince <span class=\"mathquill\">W_{ij}</span>\nrepresents the weight component in the weight matrix, connecting neuron **i** from the input to neuron **j** in the hidden layer, we can also write these calculations in the following way:\n(notice that in this example we have *n* inputs and only 3 hidden neurons)\n",
              "instructor_notes": ""
            },
            {
              "id": 440476,
              "key": "73a26228-11ed-4130-89cb-a38ea7f669c1",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f3ddb6_screen-shot-2017-10-27-at-6.29.49-pm/screen-shot-2017-10-27-at-6.29.49-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/73a26228-11ed-4130-89cb-a38ea7f669c1",
              "caption": "Equation 2",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 440844,
              "key": "e4f0e21f-0d2b-4d8a-a373-9075d769a111",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "More information on the activation functions and how to use them can be found [here](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)",
              "instructor_notes": ""
            },
            {
              "id": 431472,
              "key": "929695e4-4225-45fd-ada5-d65f2dc51cdd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": " This next video will help you understand the second step- **Calculating the values of the Outputs**. \n\n",
              "instructor_notes": ""
            },
            {
              "id": 474220,
              "key": "7befda45-9446-41b7-87fd-1bb483849c1a",
              "title": "07 FeedForward B V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "kTYbTVh1d0k",
                "china_cdn_id": "kTYbTVh1d0k.mp4"
              }
            },
            {
              "id": 440608,
              "key": "d6f62586-0513-45b0-9b8f-3e01dd9815b3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As you've seen in the video above, the process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication, which can be followed by an activation function. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.\n\n  ",
              "instructor_notes": ""
            },
            {
              "id": 440610,
              "key": "cf9137ab-969c-4419-99cb-c6eebdd36665",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f767b8_screen-shot-2017-10-30-at-10.54.50-am/screen-shot-2017-10-30-at-10.54.50-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cf9137ab-969c-4419-99cb-c6eebdd36665",
              "caption": "",
              "alt": "",
              "width": 150,
              "height": 250,
              "instructor_notes": null
            },
            {
              "id": 440611,
              "key": "95aceac7-be87-4610-a060-a3f54fd46c21",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer.\n\nIn our example, the input vector is <span class=\"mathquill\">\\bar{h}</span> and the matrix is  <span class=\"mathquill\">W^2</span>, therefore  <span class=\"mathquill\">\\bar{y}=\\bar{h}W^2</span>. In some applications it can be beneficial to use a softmax function (if we want all output values to be between zero and 1, and their sum to be 1).",
              "instructor_notes": ""
            },
            {
              "id": 440638,
              "key": "cf84836c-6269-45a4-9e15-5588c2910ada",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59f775fc_screen-shot-2017-10-30-at-11.56.27-am/screen-shot-2017-10-30-at-11.56.27-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cf84836c-6269-45a4-9e15-5588c2910ada",
              "caption": "Equation 3",
              "alt": "",
              "width": 280,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 434391,
              "key": "7b0bfee5-dde6-49b4-98c2-0c4cf5318ba1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The two error functions that are most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (usually used in classification problems).\n\nIn the above calculations we used a variation of the MSE.",
              "instructor_notes": ""
            },
            {
              "id": 440852,
              "key": "e554c261-007d-47be-9f95-93430383ea00",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The next few videos will focus on the backpropagation process, or what we also call stochastic gradient decent with the use of the chain rule.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 413097,
          "key": "780e4ed9-5dfb-434f-bba7-8a8a8e8756d4",
          "title": "Feedforward Quiz",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "780e4ed9-5dfb-434f-bba7-8a8a8e8756d4",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 467628,
              "key": "82a1ca05-7cc5-422e-aa9f-2197de9e5b04",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following picture is of a feedforward network with\n- A single input x\n- Two hidden layers \n- A singe output\n\nThe first hidden layer has M neurons.\n\nThe second hidden layer has N neurons.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 467629,
              "key": "be9e525b-b712-4149-bc17-7e9f017cc22f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a24f0cc_screen-shot-2017-12-03-at-10.43.49-pm/screen-shot-2017-12-03-at-10.43.49-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/be9e525b-b712-4149-bc17-7e9f017cc22f",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 467630,
              "key": "2a1d6a3f-c8a3-4706-9017-1609b1c249df",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "2a1d6a3f-c8a3-4706-9017-1609b1c249df",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the total number of multiplication operations needed for a single feedforward pass?",
                "answers": [
                  {
                    "id": "a1512370508520",
                    "text": "MN",
                    "is_correct": false
                  },
                  {
                    "id": "a1512370601107",
                    "text": "M+N",
                    "is_correct": false
                  },
                  {
                    "id": "a1512370611393",
                    "text": "M+N+2MN",
                    "is_correct": false
                  },
                  {
                    "id": "a1512370637214",
                    "text": "M+N+NM",
                    "is_correct": true
                  },
                  {
                    "id": "a1512370649056",
                    "text": "There isn't enough information to solve this question ",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 467631,
              "key": "cfe64ebc-2411-4d66-920c-3eea29a242d9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n# Solution \n\nTo calculate the number of multiplications needed for a single feedforward pass, we can break down the network to three steps:\n- Step 1: From the single input to the first hidden layer\n- Step 2: From the first hidden layer to the second hidden layer\n- Step 2: From the second hidden layer to the single output \n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 467632,
              "key": "1fcd9ee2-b66f-426d-ab7f-e6aced373136",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Step 1**\n\nThe single input is multiplied by a vector with M values. Each value in the vector will represent a weight connecting the input to the first hidden layer. Therefore, we will have **M** multiplication operations.",
              "instructor_notes": ""
            },
            {
              "id": 467633,
              "key": "97ba6c45-1ac2-41bb-b5cd-727a1d719da0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Step 2**\n\nEach value in the first hidden layer (M in total) will be multiplied by a vector with N values. Each value in the vector will represent a weight connecting the neurons in the first hidden layer to the neurons in the second hidden layer.  Therefore, we will have here M times N calculations, or simply **MN** multiplication operations.",
              "instructor_notes": ""
            },
            {
              "id": 467634,
              "key": "e34d0377-4144-49f8-a5b6-f97862f73f12",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Step 3**\n\nEach value in the second hidden layer (N in total) will be multiplied once, by the weight element connecting it to the single output. Therefore, we will have **N** multiplication operations.",
              "instructor_notes": ""
            },
            {
              "id": 467635,
              "key": "4b976f01-c0e9-4155-bdba-b7d7da3b9d3e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In total, we will add the number of operations we calculated in each step:  **M+MN+N** .",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 412993,
          "key": "6fbfd066-6907-4f72-98b8-c5743818fa06",
          "title": "Backpropagation- Theory",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6fbfd066-6907-4f72-98b8-c5743818fa06",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 439947,
              "key": "e04a195c-f30f-428d-8ede-8fadb9d4b37b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Backpropagation Theory",
              "instructor_notes": ""
            },
            {
              "id": 440846,
              "key": "d1d1ac5f-c872-4d6b-ac9f-38c29d9c0a2a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Since partial derivatives are the key mathematical concept used in backpropagation, it's important that you feel confident in your ability to calculate them. Once you know how to calculate basic derivatives, calculating partial derivatives is easy to understand.  \nFor more information on partial derivatives use the following [link](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)\n\nFor calculation purposes in future quizzes of the lesson, you can use the following link as a reference for [common derivatives](http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf).\n",
              "instructor_notes": ""
            },
            {
              "id": 440856,
              "key": "610c8927-7740-4e4d-9445-54dea71de749",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nIn the **backpropagation** process we minimize the network error slightly with each iteration, by adjusting the weights. The following video will help you understand the mathematical process we use for computing these adjustments. ",
              "instructor_notes": ""
            },
            {
              "id": 474178,
              "key": "7bbdcdfb-6783-4b53-81ec-9a48c7bff28b",
              "title": "08 Backpropagation Theory V6 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Xlgd8I3TWUg",
                "china_cdn_id": "Xlgd8I3TWUg.mp4"
              }
            },
            {
              "id": 440878,
              "key": "cad0ba59-a830-44d2-9408-4d3351cb09c3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If we look at an arbitrary layer k, we can define the amount by which we change the weights from neuron i to neuron j stemming from layer k as: <span class=\"mathquill\">\\Delta W^k</span><span class=\"mathquill\">_{ij}</span>.\n\nThe superscript (*k*) indicates that the weight connects layer *k* to layer *k+1*. \n\nTherefore, the weight update rule for that neuron can be expressed as:\n\n <span class=\"mathquill\">W_{new}  = W_{previous} +\\Delta W^k</span><span class=\"mathquill\">_{ij}</span>    \n\n_Equation 4_\n\n   \n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458789,
              "key": "082f7223-0096-44a8-8fd0-87eaee93b176",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The updated value  <span class=\"mathquill\">\\Delta W_{ij}^k</span> is calculated through the use of the gradient calculation, in the following way:\n\n<span class=\"mathquill\">\\Delta W_{ij}^k=\\alpha (-\\frac{\\partial E}{\\partial W})</span>, where  <span class=\"mathquill\">\\alpha</span> is a small positive number called the** Learning Rate**.\n\n_Equation 5_\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458790,
              "key": "2a49aaa1-9add-46ef-bb30-6d5fce3449bd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "From these derivation we can easily see that the weight updates are calculated the by the following equation:  \n\n  <span class=\"mathquill\">W_{new}= W_{previous} +\\alpha (-\\frac{\\partial E}{\\partial W} )</span>\n\n_Equation 6_\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458791,
              "key": "4da48bb3-ed89-4d2f-9589-9b9588da5cfd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Since many weights determine the network’s output, we can use a vector of the partial derivatives (defined by the Greek letter Nabla  <span class=\"mathquill\">\\nabla</span>) of the network error - each with respect to a different weight. \n\n <span class=\"mathquill\">W_{new}= W_{previous}+\\alpha \\nabla_W(-E)</span>\n\n_Equation 7_",
              "instructor_notes": ""
            },
            {
              "id": 439948,
              "key": "38597fb7-a3ac-40d8-97ab-06b58df458b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here you can find other good resources for understanding and tuning the Learning Rate:\n\n- [resource 1](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/) \n- [resource 2](http://cs231n.github.io/neural-networks-3/#loss)",
              "instructor_notes": ""
            },
            {
              "id": 474143,
              "key": "f0fa6948-99ad-4336-8c5a-37cbb8b863ac",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following video is given as a refresher to **overfitting** . You have already seen this concept in the _Training Neural Networks_ lesson. Feel free to skip it and jump right into the next video.",
              "instructor_notes": ""
            },
            {
              "id": 474179,
              "key": "62220293-8496-48bb-928b-343f78aa68f4",
              "title": "13 Overfitting Intro V4 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "rmBLnVbFfFY",
                "china_cdn_id": "rmBLnVbFfFY.mp4"
              }
            }
          ]
        },
        {
          "id": 413001,
          "key": "00d69708-34de-45a2-a93d-4213f0cea51e",
          "title": "Backpropagation - Example (part a)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "00d69708-34de-45a2-a93d-4213f0cea51e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 442256,
              "key": "ae5a440d-eb6a-472a-aca5-145531a9e0a5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Backpropagation- Example (part a)",
              "instructor_notes": ""
            },
            {
              "id": 439951,
              "key": "aee80864-0336-4670-96e8-345f7a024c90",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We will now continue with an example focusing on the backpropagation process, and consider a network having two inputs  <span class=\"mathquill\">[x_1, x_2]</span>, three neurons in a single hidden layer <span class=\"mathquill\">[h_1, h_2, h_3]</span> and a single output <span class=\"mathquill\">y</span>.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 463934,
              "key": "0ead1cab-6c6f-4c2b-be78-aa9462b52f78",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a0f8f4c_screen-shot-2017-11-17-at-5.38.55-pm/screen-shot-2017-11-17-at-5.38.55-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0ead1cab-6c6f-4c2b-be78-aa9462b52f78",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 440978,
              "key": "0b7fd1ad-c89a-4084-81bb-6ba34e2acc50",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The weight matrices to update are  <span class=\"mathquill\">W^1</span> from the input to the hidden layer,  and  <span class=\"mathquill\">W^2 </span> from the hidden layer to the output. Notice that in our case <span class=\"mathquill\">W^2 </span> is a vector, not a matrix, as we only have one output.",
              "instructor_notes": ""
            },
            {
              "id": 474180,
              "key": "2143aad0-14e4-48a6-9be4-593d9bf66a91",
              "title": "10 Backpropagation Example A V3 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "3k72z_WaeXg",
                "china_cdn_id": "3k72z_WaeXg.mp4"
              }
            },
            {
              "id": 440979,
              "key": "7645b826-bc9a-4caa-89b7-f26b687a8908",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The chain of thought in the weight updating process is as follows:\n\nTo update the weights, we need the network error. To find the network error, we need the network output, and to find the network output we need the value of the hidden layer, vector <span class=\"mathquill\">\\bar {h}</span>.\n \n<span class=\"mathquill\">\\bar{h}=[h_1, h_2, h_3]</span>\n\n_Equation 8_",
              "instructor_notes": ""
            },
            {
              "id": 440981,
              "key": "15915e40-3996-43a9-9ac3-e52a8a318353",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Each element of vector  <span class=\"mathquill\">\\bar {h}</span> is calculated by a simple linear combination of the input vector with its corresponding weight matrix <span class=\"mathquill\">W^1</span>, followed by an activation function. \n\n",
              "instructor_notes": ""
            },
            {
              "id": 468955,
              "key": "cb372632-54b4-40f0-bd7d-fa684d460721",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a26fcf8_screen-shot-2017-12-05-at-12.09.13-pm/screen-shot-2017-12-05-at-12.09.13-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cb372632-54b4-40f0-bd7d-fa684d460721",
              "caption": "_Equation 9_",
              "alt": "",
              "width": 150,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 442033,
              "key": "ea162c5a-f426-4544-a2a2-22c8b86aa992",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We now need to find the network's output, <span class=\"mathquill\">y</span>.  <span class=\"mathquill\">y</span> is calculated in a similar way by using a linear combination of the vector <span class=\"mathquill\">\\bar{h}</span> with its corresponding elements of the weight vector <span class=\"mathquill\">W^2</span>.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 442051,
              "key": "e618b007-cbca-4a9e-ab2f-2cd7b3ed963b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/59fa1600_screen-shot-2017-11-01-at-11.43.26-am/screen-shot-2017-11-01-at-11.43.26-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e618b007-cbca-4a9e-ab2f-2cd7b3ed963b",
              "caption": "_Equation 10_",
              "alt": "",
              "width": 130,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 436168,
              "key": "43591641-f65e-4467-9abb-b87e91dd0cfb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "After computing the output, we can finally find the network error. \n\nAs a reminder, the two Error functions most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (often used in classification problems).\n\nIn this example, we use a variation of the MSE:     \n\n <span class=\"mathquill\">E=\\frac{(d-y)^2}{2} </span>,\n\nwhere <span class=\"mathquill\">d </span> is the desired output and <span class=\"mathquill\">y </span> is the calculated one. Notice that **y** and **d** are not vectors in this case, as we have a single output.\n\nThe error is their squared difference, <span class=\"mathquill\">E=(d-y)^2</span>, and is also called the network's **Loss Function**.  We are dividing the error term by 2 to simplify notation, as will become clear soon.\n",
              "instructor_notes": ""
            },
            {
              "id": 442061,
              "key": "244d0c16-2144-452c-b0f7-84f989421da0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The aim of the backpropagation process is to minimize the error, which in our case is the Loss Function. To do that we need to calculate its partial derivative with respect to all of the weights.\n\nSince we just found  the output y, we can now minimize the error by finding the updated values <span class=\"mathquill\">\\Delta W_{ij}^k</span>.\nThe superscript k indicates that we need to update each and every layer k.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 442071,
              "key": "24a340a8-cb2a-4d89-9193-613cf78d629e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As we noted before, the weight update value <span class=\"mathquill\">\\Delta W_{ij}^k</span> is calculated with the use of the gradient the following way:\n\n<span class=\"mathquill\">\\Delta W_{ij}^k=\\alpha (-\\frac{\\partial E}{\\partial W})</span>\n\nTherefore:\n\n<span class=\"mathquill\">\\Delta W_{ij}^k=\\alpha (-\\frac{\\partial E}{\\partial W})=-\\frac{\\alpha}{2} \\frac{\\partial (d-y)^2}{\\partial W_{ij}}=-2 \\frac{\\alpha}{2}(d-y) \\large  \\frac{\\partial (d-y)}{\\partial W_{ij}}</span>\n\nwhich can be simplified as:\n\n<span class=\"mathquill\">\\Delta W_{ij}^k=\\alpha(d-y) \\frac{\\partial y}{\\partial W_{ij}}</span>\n\n_Equation 11_\n\n\n(Notice that <span class=\"mathquill\">d</span> is a  constant value, so it’s partial derivative is simply a zero)\n\n",
              "instructor_notes": ""
            },
            {
              "id": 442175,
              "key": "54a99300-6558-4357-8594-5e9f940d4d0c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "This partial derivative of the output with respect to each weight, defines the gradient and is often denoted by the Greek letter <span class=\"mathquill\">\\delta</span>.\n\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 468971,
              "key": "b21bcfae-9426-497b-9c23-7c6b39232ed6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a26feea_screen-shot-2017-12-05-at-12.16.55-pm/screen-shot-2017-12-05-at-12.16.55-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/b21bcfae-9426-497b-9c23-7c6b39232ed6",
              "caption": "_Equation 12_",
              "alt": "",
              "width": 120,
              "height": 50,
              "instructor_notes": null
            },
            {
              "id": 468974,
              "key": "e93e177a-9cb6-4a1d-9b54-1a34d37d8da0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We will find all the elements of the gradient using the chain rule.\n",
              "instructor_notes": ""
            },
            {
              "id": 439952,
              "key": "deb4daff-3ec4-4cd6-aea2-58c30cba0cff",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you are feeling confident with the **chain rule** and understand how to apply it, skip the next video and continue with our example. Otherwise, give Luis a few minutes of your time as he takes you through the process!\n",
              "instructor_notes": ""
            },
            {
              "id": 442410,
              "key": "1246a9e0-be28-41a7-8264-727f99f51b64",
              "title": "Regra da cadeia",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "YAhIBOnbt54",
                "china_cdn_id": "YAhIBOnbt54.mp4"
              }
            }
          ]
        },
        {
          "id": 413022,
          "key": "514cf09c-9651-4877-8b2c-d651bca3cd16",
          "title": "Backpropagation- Example (part b)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "514cf09c-9651-4877-8b2c-d651bca3cd16",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 442408,
              "key": "06e49c2e-163a-43b0-ac11-c01fe89108c5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Backpropagation- Example (part b)",
              "instructor_notes": ""
            },
            {
              "id": 436167,
              "key": "dab6caba-6197-45ef-b541-d89a218e8006",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Now that we understand the chain rule, we can continue with our **backpropagation** example, where we will calculate the gradient",
              "instructor_notes": ""
            },
            {
              "id": 474181,
              "key": "3eabc7b6-ca17-4091-a6e0-1af52eb72a09",
              "title": "12 Backpropagation Example B V6 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "yiSwuMP2UIA",
                "china_cdn_id": "yiSwuMP2UIA.mp4"
              }
            },
            {
              "id": 442176,
              "key": "677006ea-4c3b-4c57-8b05-9085661ab467",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In our example we only have one hidden layer, so our backpropagation process will consist of two steps: \n\nStep 1: Calculating the gradient with respect to the weight vector <span class=\"mathquill\">W^2</span>  (from the output to the hidden layer). <span class=\"mathquill\"> </span>    \nStep 2: Calculating the gradient with respect to the weight matrix <span class=\"mathquill\">W^1</span> (from the hidden layer to the input).\n\n\n**Step 1**\n(Note that the weight vector referenced here will be <span class=\"mathquill\">W^2</span>. All indices referring to <span class=\"mathquill\">W^2</span> have been omitted from the calculations to keep the notation simple).",
              "instructor_notes": ""
            },
            {
              "id": 442184,
              "key": "6252ca2c-597d-49d6-8e43-256192e418e8",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/59fa335a_screen-shot-2017-11-01-at-1.48.59-pm/screen-shot-2017-11-01-at-1.48.59-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6252ca2c-597d-49d6-8e43-256192e418e8",
              "caption": "_Equation 13_",
              "alt": "",
              "width": 250,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442182,
              "key": "de1ef8da-0e2d-4537-9661-76634b702bb8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As you may recall: \n\n<span class=\"mathquill\">\\large\\Delta W_{ij}=\\alpha(d-y) \\frac{\\partial y}{\\partial W_{ij}}</span>  \n\nIn this specific step, since the output is of only a single value, we can rewrite the equation the following way (in which we have a weights *vector*):\n\n<span class=\"mathquill\">\\large\\Delta W_i=\\alpha(d-y) \\frac{\\partial y}{\\partial W_i}</span>\n\nSince we already calculated the gradient, we now know that the incremental value we need for step one is:\n\n<span class=\"mathquill\">\\Delta W_i=\\alpha(d-y) h_i</span>\n\n_Equation 14_\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 442200,
              "key": "9124ff2b-c40e-4c30-a259-8357e7c8a8d2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n\n\nHaving calculated the incremental value, we can update vector  <span class=\"mathquill\">W^2</span> the following way:      \n\n",
              "instructor_notes": ""
            },
            {
              "id": 505023,
              "key": "e8aad62f-7fc4-4052-8560-deec11e28f29",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a5e7fa6_screen-shot-2018-01-16-at-2.40.57-pm/screen-shot-2018-01-16-at-2.40.57-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e8aad62f-7fc4-4052-8560-deec11e28f29",
              "caption": "_Equation 15_",
              "alt": "",
              "width": 300,
              "height": 150,
              "instructor_notes": null
            },
            {
              "id": 442202,
              "key": "55ff213a-adcf-42bf-99ad-a632ca6e3c79",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Step 2**\n(In this step, we will need to use both weight matrices. Therefore we will not be omitting the weight  indices.) \n\nIn our second step we will update the weights of matrix <span class=\"mathquill\">W^1</span> by calculating the partial derivative of <span class=\"mathquill\">y</span> with respect to the weight matrix <span class=\"mathquill\">W^1</span>.\n",
              "instructor_notes": ""
            },
            {
              "id": 442208,
              "key": "816d6c14-c489-47ce-9906-8035e09f3e7f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The chain rule will be used the following way:\n\nobtain the partial derivative of <span class=\"mathquill\">y</span> with respect to <span class=\"mathquill\">\\bar{h}</span>, and multiply it by the partial derivative of  <span class=\"mathquill\">\\bar{h}</span> with respect to the corresponding elements in  <span class=\"mathquill\">W^1</span>. Instead of referring to vector <span class=\"mathquill\">\\bar{h}</span>, we can observe each element and present the equation the following way:\n\n",
              "instructor_notes": ""
            },
            {
              "id": 551777,
              "key": "75ca6a44-91ba-49b5-8b81-88af24e9b892",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/February/5a8dfa93_screen-shot-2018-02-21-at-3.02.16-pm/screen-shot-2018-02-21-at-3.02.16-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/75ca6a44-91ba-49b5-8b81-88af24e9b892",
              "caption": "_Equation 16_",
              "alt": "",
              "width": 300,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442213,
              "key": "ca2c2774-6527-4d2f-9124-06a26bb9e0e8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this example we have only 3 neurons the the single hidden layer, therefore this will be a linear combination of three elements:",
              "instructor_notes": ""
            },
            {
              "id": 551778,
              "key": "9c290e17-a4d1-4d5f-b183-7cc69c6c298a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/February/5a8dfb31_screen-shot-2018-02-21-at-3.05.00-pm/screen-shot-2018-02-21-at-3.05.00-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9c290e17-a4d1-4d5f-b183-7cc69c6c298a",
              "caption": "_Equation 17_",
              "alt": "",
              "width": 290,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442216,
              "key": "68a79db6-e9d2-42d7-b981-c1e6cf448412",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We will calculate each derivative separately. <span class=\"mathquill\">\\frac{\\partial y}{\\partial h_j}</span> will be calculated first, followed by <span class=\"mathquill\">\\frac{\\partial h_j}{\\partial W^1_{ij}}</span>.\n",
              "instructor_notes": ""
            },
            {
              "id": 442218,
              "key": "14af31fc-2974-4110-8c93-722183dfc631",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/59fa4d0f_screen-shot-2017-11-01-at-3.38.43-pm/screen-shot-2017-11-01-at-3.38.43-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/14af31fc-2974-4110-8c93-722183dfc631",
              "caption": "_Equation 18_",
              "alt": "",
              "width": 230,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442220,
              "key": "afd41a5f-9cba-43ad-b635-0a511071669e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice that most of the derivatives were zero, leaving us with the simple solution of <span class=\"mathquill\"> \\frac{\\partial y}{\\partial h_{j}}=W^2_j</span>\n",
              "instructor_notes": ""
            },
            {
              "id": 442222,
              "key": "a829b670-9013-4c79-94e5-a67ad8d31db3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "To calculate <span class=\"mathquill\">\\frac{\\partial h_j}{\\partial W^1_{{ij}}}</span> we need to remember first that",
              "instructor_notes": ""
            },
            {
              "id": 467575,
              "key": "637855b9-5426-4a14-a3b8-b89b255f8971",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a23a0c1_screen-shot-2017-12-02-at-10.58.26-pm/screen-shot-2017-12-02-at-10.58.26-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/637855b9-5426-4a14-a3b8-b89b255f8971",
              "caption": "_Equation 19_",
              "alt": "",
              "width": 170,
              "height": 80,
              "instructor_notes": null
            },
            {
              "id": 442225,
              "key": "7e99ca8f-a089-4fab-bd5c-98c95bd06245",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Therefore:\n",
              "instructor_notes": ""
            },
            {
              "id": 467577,
              "key": "d99f9dda-d64c-4a8a-812d-4026b2f7cf13",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a23a28a_screen-shot-2017-12-02-at-11.06.19-pm/screen-shot-2017-12-02-at-11.06.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d99f9dda-d64c-4a8a-812d-4026b2f7cf13",
              "caption": "_Equation 20_",
              "alt": "",
              "width": 280,
              "height": 80,
              "instructor_notes": null
            },
            {
              "id": 442229,
              "key": "f199ecd3-ee08-4390-9d12-939c68683b38",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Since the function <span class=\"mathquill\">\\ h_j</span> is an activation function (<span class=\"mathquill\">\\Phi</span>) of a linear combination, its partial derivative will be calculated the following way:",
              "instructor_notes": ""
            },
            {
              "id": 467576,
              "key": "8d4c792f-7e95-4c2d-aec7-4df3bfcd6383",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a23a1f8_screen-shot-2017-12-02-at-11.03.45-pm/screen-shot-2017-12-02-at-11.03.45-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8d4c792f-7e95-4c2d-aec7-4df3bfcd6383",
              "caption": "_Equation 21_",
              "alt": "",
              "width": 600,
              "height": 80,
              "instructor_notes": null
            },
            {
              "id": 442234,
              "key": "41fc35f1-88a7-4e01-98a9-864956e5fae3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Given that there are various activation functions, we will leave the partial derivative of  <span class=\"mathquill\">\\Phi</span> using a general notation. Each neuron j will have its own value for  <span class=\"mathquill\">\\Phi</span> and  <span class=\"mathquill\">\\Phi'</span>, according to the activation function we choose to use.\n",
              "instructor_notes": ""
            },
            {
              "id": 467572,
              "key": "060d698a-3038-48e3-9982-438cd11d16b2",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a2399d7_screen-shot-2017-12-02-at-10.29.14-pm/screen-shot-2017-12-02-at-10.29.14-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/060d698a-3038-48e3-9982-438cd11d16b2",
              "caption": "_Equation 22_",
              "alt": "",
              "width": 200,
              "height": 70,
              "instructor_notes": null
            },
            {
              "id": 442240,
              "key": "e5f25457-0d2c-4ef7-92d4-15533115456c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The second calculation of equation 21 can be calculated the following way:\n\n(Notice how simple the result is, as most of the components of this partial derivative are zero).",
              "instructor_notes": ""
            },
            {
              "id": 442241,
              "key": "92ffe90d-11e2-45c5-b4c6-ac847dd57175",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/59fa5d33_screen-shot-2017-11-01-at-4.47.47-pm/screen-shot-2017-11-01-at-4.47.47-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/92ffe90d-11e2-45c5-b4c6-ac847dd57175",
              "caption": "_Equation 23_",
              "alt": "",
              "width": 200,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442243,
              "key": "a198838b-71cb-4280-afd8-94579d851094",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "After understanding how to treat each multiplication of equation 21 separately, we can now summarize it the following way:\n",
              "instructor_notes": ""
            },
            {
              "id": 442244,
              "key": "69156a30-3d43-425b-b8af-29ad81dcb29c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/59fa6366_screen-shot-2017-11-01-at-5.14.13-pm/screen-shot-2017-11-01-at-5.14.13-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/69156a30-3d43-425b-b8af-29ad81dcb29c",
              "caption": "_Equation 24_",
              "alt": "",
              "width": 150,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 442245,
              "key": "59ab59cb-18ce-49db-899c-2f14f46c68c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We are ready to finalize **step 2**, in which we update the weights of matrix <span class=\"mathquill\">W^1</span> by calculating the gradient shown in equation 17.  From the above calculations, we can conclude that:\n",
              "instructor_notes": ""
            },
            {
              "id": 551779,
              "key": "70d33479-2a64-4948-b58c-b963cbb7cb1c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/February/5a8dfc5f_screen-shot-2018-02-21-at-3.10.10-pm/screen-shot-2018-02-21-at-3.10.10-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/70d33479-2a64-4948-b58c-b963cbb7cb1c",
              "caption": "_Equation 25_",
              "alt": "",
              "width": 350,
              "height": 50,
              "instructor_notes": null
            },
            {
              "id": 442247,
              "key": "09e816c2-7044-4035-bce8-42cf64a0a841",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Since <span class=\"mathquill\">\\Delta W^1_{ij}=\\alpha(d-y) \\large\\frac{\\partial y}{\\partial W^1_{ij}}</span>  , when finalizing step 2, we have:\n\n",
              "instructor_notes": ""
            },
            {
              "id": 467574,
              "key": "35220654-18f6-410e-babf-241af5fc4514",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a239dfd_screen-shot-2017-12-02-at-10.46.12-pm/screen-shot-2017-12-02-at-10.46.12-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/35220654-18f6-410e-babf-241af5fc4514",
              "caption": "_Equation 26_",
              "alt": "",
              "width": 270,
              "height": 40,
              "instructor_notes": null
            },
            {
              "id": 442249,
              "key": "9d2d7e22-298e-4d85-93fe-3d9e4e26a841",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Having calculated the incremental value, we can update vector  <span class=\"mathquill\">W^1</span> the following way:      \n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 506142,
              "key": "236217fb-b6dd-41f1-b501-3fcfa5f6b9c9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "<span class=\"mathquill\">W^1_{new}=W^1_{previous}+\\Delta W^1_{ij}</span>",
              "instructor_notes": ""
            },
            {
              "id": 506145,
              "key": "49a20819-9bc2-433a-83b5-a32e8179e627",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "<span class=\"mathquill\">W^1_{new}=W^1_{previous}+\\alpha(d-y)W^2_j\\Phi'_jx_i</span>\n\n_Equation 27_",
              "instructor_notes": ""
            },
            {
              "id": 442257,
              "key": "d54ec2ab-890e-4c8f-b430-d94108cddeae",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "After updating the weight matrices we begin once again with the Feedforward pass, starting the process of updating the weights all over again.\n ",
              "instructor_notes": ""
            },
            {
              "id": 434394,
              "key": "3c8d0db2-efa1-4a9c-a4e2-5c70416d1fb9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\n\nThis video touches on the subject of Mini Batch Training. We will further explain things in our **Hyperparameters** lesson coming up.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 467975,
          "key": "d9e54163-c08b-471d-b83a-9bfed326699c",
          "title": "Backpropagation Quiz",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d9e54163-c08b-471d-b83a-9bfed326699c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 467981,
              "key": "4008ab9d-d330-4ce5-8883-eacf678d79b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following picture is of a feedforward network with\n- A single input x\n- Two hidden layers with two neurons in each layer\n- A single output\n\n",
              "instructor_notes": ""
            },
            {
              "id": 468367,
              "key": "1202cee1-bf94-4dc3-8db9-61aff418b27a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b0b2_screen-shot-2017-12-04-at-12.31.11-pm/screen-shot-2017-12-04-at-12.31.11-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/1202cee1-bf94-4dc3-8db9-61aff418b27a",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468364,
              "key": "6f8b012a-15cc-4f18-9d65-4dfe4673c113",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "6f8b012a-15cc-4f18-9d65-4dfe4673c113",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the update rule of weight matrix W1? \n\n(In other words, what is the partial derivative of y with respect to W1?)\n\nHint: Use the chain rule",
                "answers": [
                  {
                    "id": "a1512419288568",
                    "text": "Equation A",
                    "is_correct": true
                  },
                  {
                    "id": "a1512419323386",
                    "text": "Equation B",
                    "is_correct": false
                  },
                  {
                    "id": "a1512419325106",
                    "text": "Equation C",
                    "is_correct": false
                  },
                  {
                    "id": "a1512419361462",
                    "text": "Equation D",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 495722,
              "key": "05824b6e-5969-4bd1-906d-55975aa0d335",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a4c0c9a_screen-shot-2018-01-02-at-2.49.43-pm/screen-shot-2018-01-02-at-2.49.43-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/05824b6e-5969-4bd1-906d-55975aa0d335",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 495720,
              "key": "82e92b88-c87d-44cc-82fa-bfc90dd24255",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a4c0c55_screen-shot-2018-01-02-at-2.44.44-pm/screen-shot-2018-01-02-at-2.44.44-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/82e92b88-c87d-44cc-82fa-bfc90dd24255",
              "caption": "",
              "alt": "",
              "width": 60,
              "height": 92,
              "instructor_notes": null
            },
            {
              "id": 468383,
              "key": "a6a61c27-c5e4-42a7-bc9b-35ff05db41f8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Solution \n\nThere are two separate paths which <span class=\"mathquill\">W_1</span> contributes to the output in: \n- Path A \n- Path B \n\n(both displayed in the pictures below)",
              "instructor_notes": ""
            },
            {
              "id": 468379,
              "key": "2e865a3f-115b-4256-9730-30d973986b28",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b504_screen-shot-2017-12-04-at-12.49.13-pm/screen-shot-2017-12-04-at-12.49.13-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/2e865a3f-115b-4256-9730-30d973986b28",
              "caption": "_Path A_",
              "alt": "",
              "width": 400,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468380,
              "key": "d07fc7e9-9c5d-4ff2-bc9d-9d70d7ba5f25",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b514_screen-shot-2017-12-04-at-12.49.52-pm/screen-shot-2017-12-04-at-12.49.52-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d07fc7e9-9c5d-4ff2-bc9d-9d70d7ba5f25",
              "caption": "_Path B_",
              "alt": "",
              "width": 400,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468385,
              "key": "e0951681-8bc1-496c-ab9e-06fb234cf0ef",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The mathematical derivations considering path A  (while applying the chain rule) are:",
              "instructor_notes": ""
            },
            {
              "id": 468387,
              "key": "fad353ed-cf8d-45c1-9a12-bfd454b525b4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b6b8_screen-shot-2017-12-04-at-12.40.54-pm/screen-shot-2017-12-04-at-12.40.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fad353ed-cf8d-45c1-9a12-bfd454b525b4",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468386,
              "key": "68f0ab78-da3e-4d03-bb47-1bef9abcbf75",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The mathematical derivations considering path B  (while applying the chain rule) are:",
              "instructor_notes": ""
            },
            {
              "id": 468388,
              "key": "8552943e-013a-45de-b431-97e4d0bbe362",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b6d9_screen-shot-2017-12-04-at-12.42.55-pm/screen-shot-2017-12-04-at-12.42.55-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8552943e-013a-45de-b431-97e4d0bbe362",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468390,
              "key": "381bf1da-77fd-4959-9f21-7b76b33a4fbe",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "To finalize our calculations we need to consider all of the paths contributing to the calculation of y. In this case we have the two paths mentioned. Therefore, the final calculation will be the addition of the derivatives calculated in each path.",
              "instructor_notes": ""
            },
            {
              "id": 468391,
              "key": "7354f97e-5f2d-4d56-9b9f-83e8a1e77658",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25b85c_screen-shot-2017-12-04-at-12.42.42-pm/screen-shot-2017-12-04-at-12.42.42-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7354f97e-5f2d-4d56-9b9f-83e8a1e77658",
              "caption": "",
              "alt": "",
              "width": 450,
              "height": 100,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 384349,
          "key": "e5c94729-1e8a-4770-8e2e-5ad1c2af7f9b",
          "title": " RNN (part a)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e5c94729-1e8a-4770-8e2e-5ad1c2af7f9b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 457385,
              "key": "4d58a56c-a167-44b9-8566-e42165b5f2ba",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We are finally ready to talk about Recurrent Neural Networks (or RNNs), where we will be opening the doors to new content! \n\n",
              "instructor_notes": ""
            },
            {
              "id": 474182,
              "key": "735b4ad3-6cf3-4078-9a23-c1af8d506273",
              "title": "14 RNN A V4 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ofbnDxGSUcg",
                "china_cdn_id": "ofbnDxGSUcg.mp4"
              }
            },
            {
              "id": 457397,
              "key": "9938f697-66bf-4e0e-b7ff-e0472ad5ee54",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "RNNs are based on the same principles as those behind FFNNs, which is why we spent so much time reminding ourselves of the feedforward and backpropagation steps that are used in the training phase.\n\nThere are two main differences between FFNNs and RNNs. The Recurrent Neural Network uses:\n-  **sequences** as inputs in the training phase, and\n- **memory** elements\n\nMemory is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training step.  \n",
              "instructor_notes": ""
            },
            {
              "id": 457406,
              "key": "757f6ad7-b30d-4271-848b-4090657ba443",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The basic three layer neural network with feedback that serve as memory inputs is called the **Elman Network** and is depicted in the following picture:",
              "instructor_notes": ""
            },
            {
              "id": 457408,
              "key": "d95d1615-276c-4a87-a2f4-c374694e93ed",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00d738_screen-shot-2017-11-06-at-1.40.14-pm/screen-shot-2017-11-06-at-1.40.14-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d95d1615-276c-4a87-a2f4-c374694e93ed",
              "caption": "_Elman Network_, source: Wikipedia",
              "alt": "source: Wikipedia",
              "width": 280,
              "height": 180,
              "instructor_notes": null
            },
            {
              "id": 457402,
              "key": "517e8bf1-acde-4959-955f-fb8767af3299",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As mentioned in the _History_ concept, \nhere is the original [Elman Network](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract) publication from 1990. This link is provided here as it's a significant milestone in the world on RNNs. To simplify things a bit, you can take a look at the following [additional info](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks).\n",
              "instructor_notes": ""
            },
            {
              "id": 457414,
              "key": "ed2a95e9-6835-489b-bcb3-2478e720580b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's continue now to the next video with more information about RNNs.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 415876,
          "key": "37e5c026-5ec4-44ef-b3c8-ceb1c1532b8f",
          "title": "RNN (part b)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "37e5c026-5ec4-44ef-b3c8-ceb1c1532b8f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 474183,
              "key": "71c92c3e-2412-4e64-a161-faff9f1ac4a4",
              "title": "16 RNN B V4 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "wsif3p5t7CI",
                "china_cdn_id": "wsif3p5t7CI.mp4"
              }
            },
            {
              "id": 457416,
              "key": "fb0206ab-5e9d-4baa-8a0e-7e07164a3c06",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nAs we've see, in FFNN the output at any time *t*, is a function of the current input and the weights. This can be easily expressed using the following equation:\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 457419,
              "key": "7eeb4f04-216c-4e74-b808-96a3b80d7300",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00dcdd_screen-shot-2017-11-06-at-2.05.19-pm/screen-shot-2017-11-06-at-2.05.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7eeb4f04-216c-4e74-b808-96a3b80d7300",
              "caption": "_Equation 28_",
              "alt": "",
              "width": 120,
              "height": 61,
              "instructor_notes": null
            },
            {
              "id": 457418,
              "key": "b3865304-99eb-4aa3-a583-3e13b2bbf85d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:\n",
              "instructor_notes": ""
            },
            {
              "id": 457420,
              "key": "ed21f17a-8fc8-4763-9f88-3244fc6d290a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00dd01_screen-shot-2017-11-06-at-2.04.24-pm/screen-shot-2017-11-06-at-2.04.24-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ed21f17a-8fc8-4763-9f88-3244fc6d290a",
              "caption": "_Equation 29_",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 457421,
              "key": "3e408a14-65c7-40c7-bd8a-2ad0d8c6542e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "This is the RNN **folded model**:",
              "instructor_notes": ""
            },
            {
              "id": 457422,
              "key": "544bf074-6d96-4fd2-9a7c-fed326b98387",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00ddac_screen-shot-2017-11-06-at-2.09.07-pm/screen-shot-2017-11-06-at-2.09.07-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/544bf074-6d96-4fd2-9a7c-fed326b98387",
              "caption": "_The RNN folded model_",
              "alt": "",
              "width": 200,
              "height": 350,
              "instructor_notes": null
            },
            {
              "id": 457423,
              "key": "51b8641f-0fa2-43c9-b3f4-4d4e9c4d3a00",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this picture, <span class=\"mathquill\">\\bar{x}</span> represents the input vector, <span class=\"mathquill\">\\bar{y}</span> represents the output vector and <span class=\"mathquill\">\\bar{s}</span> denotes the state vector.\n\n\n\n<span class=\"mathquill\">W_x</span>  is the weight matrix connecting the inputs to the state layer.\n\n<span class=\"mathquill\">W_y</span> is the weight matrix connecting the state layer to the output layer.\n\n<span class=\"mathquill\">W_s</span> represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
              "instructor_notes": ""
            },
            {
              "id": 457426,
              "key": "5b64af6e-8b49-42b8-93b3-47340cee8016",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The model can also be \"unfolded in time\". The **unfolded model** is usually what we use when working with RNNs.",
              "instructor_notes": ""
            },
            {
              "id": 457427,
              "key": "d9266823-2076-4bc2-b087-77edb26cb7af",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00e48f_screen-shot-2017-11-06-at-2.38.51-pm/screen-shot-2017-11-06-at-2.38.51-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d9266823-2076-4bc2-b087-77edb26cb7af",
              "caption": "_The RNN unfolded model_",
              "alt": "",
              "width": 550,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 457428,
              "key": "d0e84bc7-af1f-4e2d-9205-538e7c4d6b91",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In both the folded and unfolded models shown above the following notation is used:\n\n<span class=\"mathquill\">\\bar{x}</span> represents the input vector, <span class=\"mathquill\">\\bar{y}</span> represents the output vector and <span class=\"mathquill\">\\bar{s}</span> represents the state vector.\n\n<span class=\"mathquill\">W_x</span>  is the weight matrix connecting the inputs to the state layer.\n\n<span class=\"mathquill\">W_y</span> is the weight matrix connecting the state layer to the output layer.\n\n<span class=\"mathquill\">W_s</span> represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.",
              "instructor_notes": ""
            },
            {
              "id": 457431,
              "key": "90e61f23-50ad-4811-8d15-a2fffce33ce0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function <span class=\"mathquill\">\\Phi</span> in the following way:\n\n<span class=\"mathquill\">\\bar{h}=\\Phi(\\bar{x}W)</span>.\n\n_Equation 30_\n\nIn RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and **also** on the previous state:\n\n\n ",
              "instructor_notes": ""
            },
            {
              "id": 457432,
              "key": "2e351a5b-3f08-47eb-9b70-239b43758f50",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00e614_screen-shot-2017-11-06-at-2.45.22-pm/screen-shot-2017-11-06-at-2.45.22-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/2e351a5b-3f08-47eb-9b70-239b43758f50",
              "caption": "_Equation 31_",
              "alt": "",
              "width": 200,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 457434,
              "key": "f8ce5266-0fe7-4538-bf36-8121797e5a4f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The output vector is calculated exactly the same as in FFNNs. It can be a linear combination of the inputs to each output node with the corresponding weight matrix <span class=\"mathquill\">W_y</span>,  or a softmax function of the same linear combination. \n\n<span class=\"mathquill\">\\bar{y}_t=\\bar{s}_t W_y</span> \n\nor \n\n<span class=\"mathquill\">\\bar{y}_t=\\sigma(\\bar{s}_t W_y)</span>\n\n_Equation 32_",
              "instructor_notes": ""
            },
            {
              "id": 475979,
              "key": "7ff5446e-1b53-4ff9-a713-5ead0fb059c9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The next video will focus on the **unfolded model** as we try to further understand it.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 416161,
          "key": "25ec7155-a508-4bfe-bbe4-6fd21e9ff4fa",
          "title": "RNN-  Unfolded Model",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "25ec7155-a508-4bfe-bbe4-6fd21e9ff4fa",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 457435,
              "key": "fc48d5e2-3386-4b15-abea-cd1c3b3d1d99",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The next video will focus on the **unfolded model** as we try to further understand it.",
              "instructor_notes": ""
            },
            {
              "id": 474184,
              "key": "8ba22fb1-f892-4b55-9b98-9522eda9fe66",
              "title": "17 RNN Unfolded V3 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "xLIA_PTWXog",
                "china_cdn_id": "xLIA_PTWXog.mp4"
              }
            }
          ]
        },
        {
          "id": 413102,
          "key": "36e442ba-e553-45ee-870a-ccf9c5d3ba3a",
          "title": "Unfolded Model Quiz",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "36e442ba-e553-45ee-870a-ccf9c5d3ba3a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 465945,
              "key": "58649467-45dd-4693-869b-33c9634c51f1",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1f3f12_screen-shot-2017-11-29-at-3.08.28-pm/screen-shot-2017-11-29-at-3.08.28-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/58649467-45dd-4693-869b-33c9634c51f1",
              "caption": "A folded model of a RNN",
              "alt": "",
              "width": 150,
              "height": 370,
              "instructor_notes": null
            },
            {
              "id": 465990,
              "key": "701324e0-97b0-40fb-8320-b8ae820e8ec8",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "701324e0-97b0-40fb-8320-b8ae820e8ec8",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Look at the above picture of a folded model of a RNN. Which of the pictures below represents the unfolded model of the same network? ",
                "answers": [
                  {
                    "id": "a1512000036963",
                    "text": "Picture A",
                    "is_correct": false
                  },
                  {
                    "id": "a1512000043273",
                    "text": "Picture B",
                    "is_correct": false
                  },
                  {
                    "id": "a1512000044415",
                    "text": "Both A and B are correct",
                    "is_correct": true
                  },
                  {
                    "id": "a1512000063906",
                    "text": "I don't have enough information to answer this question",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 465984,
              "key": "adfed6b8-ae88-4c05-9a3b-e479c1e33a55",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1f481f_screen-shot-2017-11-29-at-3.51.44-pm/screen-shot-2017-11-29-at-3.51.44-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/adfed6b8-ae88-4c05-9a3b-e479c1e33a55",
              "caption": "Picture A",
              "alt": "",
              "width": 250,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 465981,
              "key": "3c5d3f1d-2f17-4c6c-9e13-b1c74f9c6623",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1f47ab_screen-shot-2017-11-29-at-3.49.20-pm/screen-shot-2017-11-29-at-3.49.20-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3c5d3f1d-2f17-4c6c-9e13-b1c74f9c6623",
              "caption": "Picture B",
              "alt": "",
              "width": 350,
              "height": 350,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 418565,
          "key": "bd7af2d0-39a8-436e-a5a4-356f04ac8556",
          "title": "RNN- Example",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "bd7af2d0-39a8-436e-a5a4-356f04ac8556",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 433599,
              "key": "ddabd5cd-2c0d-4595-80ff-54505974d927",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this example we will illustrate how RNNs can be helpful in detecting sequences. When detecting a sequence, the system has to remember what the previous inputs were, so it makes sense to use a recurrent network.\n\nIf you are unfamiliar with the term sequence detection, the idea is to see if a specific pattern of inputs has entered the system. In our example the  pattern will be the word U,D,A,C,I,T,Y.",
              "instructor_notes": ""
            },
            {
              "id": 474185,
              "key": "9ffa6495-70b9-4074-b48f-8bbff6de557d",
              "title": "18 RNN Example V5 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "MDLk3fhpTx0",
                "china_cdn_id": "MDLk3fhpTx0.mp4"
              }
            }
          ]
        },
        {
          "id": 403178,
          "key": "c872c9ec-18a9-4e3c-840c-b9fc85c93641",
          "title": "Backpropagation Through Time (part a)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c872c9ec-18a9-4e3c-840c-b9fc85c93641",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 457440,
              "key": "194582e2-da02-4930-b75a-a27dc0668031",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We are now ready to understand how to train the RNN. \n\n\nWhen we train RNNs we also use backpropagation, but with a conceptual change. The process is similar to that in the FFNN, with the exception that we need to consider previous time steps, as the system has memory. This process is called **Backpropagation Through Time (BPTT)** and will be the topic of the next three videos. \n\n \n- As always, don't forget to take notes.",
              "instructor_notes": ""
            },
            {
              "id": 432886,
              "key": "263e3fe9-18f6-4f94-8200-9ef03857f6f3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the following videos we will use the Loss Function for our error. The Loss Function is the square of the difference between the desired and the calculated outputs. There are variations to the Loss Function, for example, factoring it with a scalar. In the backpropagation example we used a factoring scalar of 1/2 for calculation convenience.\n\n\nAs described previously, the two most commonly used are the [Mean Squared Error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) (usually used in regression problems) and the [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf) (usually used in classification problems).\n\nHere, we are using a variation of the MSE.",
              "instructor_notes": ""
            },
            {
              "id": 474186,
              "key": "1ac28df4-ae54-4dcd-ae8d-14ff45cc7528",
              "title": "19 RNN BPTT A V6 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "eE2L3-2wKac",
                "china_cdn_id": "eE2L3-2wKac.mp4"
              }
            },
            {
              "id": 457441,
              "key": "7e1a0118-9ba9-4135-8fbf-79b7e12f565e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Before diving into Backpropagation Through Time we need a few reminders.\n\nThe state vector  <span class=\"mathquill\">\\bar{s}_t</span> is calculated the following way:\n  ",
              "instructor_notes": ""
            },
            {
              "id": 457442,
              "key": "4d8418a7-d063-4ff7-b9f4-338d4398f316",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00f741_screen-shot-2017-11-06-at-2.45.22-pm/screen-shot-2017-11-06-at-2.45.22-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4d8418a7-d063-4ff7-b9f4-338d4398f316",
              "caption": "_Equation 33_",
              "alt": "",
              "width": 200,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 457445,
              "key": "733fb39b-6baa-4bed-a114-1de926b9e2cc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The output vector <span class=\"mathquill\">\\bar{y}_t</span> can be product of the state vector <span class=\"mathquill\">\\bar{s}_t</span> and the corresponding weight elements of matrix <span class=\"mathquill\">W_y</span>. As mentioned before, if the desired outputs are between 0 and 1, we can also use a softmax function. The following set of equations depicts these calculations:\n\n",
              "instructor_notes": ""
            },
            {
              "id": 457448,
              "key": "693626fc-06c0-4e84-8b97-1bba5f85c7bc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a00faa4_screen-shot-2017-11-06-at-4.12.59-pm/screen-shot-2017-11-06-at-4.12.59-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/693626fc-06c0-4e84-8b97-1bba5f85c7bc",
              "caption": "_Equation 34_",
              "alt": "",
              "width": 120,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 457452,
              "key": "917e106a-48ef-478d-81d6-94981437b3a5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As mentioned before, for the error calculations we will use the Loss Function, where \n\n<span class=\"mathquill\">E_t</span> represents the output error at time t\n\n<span class=\"mathquill\">d_t</span> represents the desired output at time t\n\n<span class=\"mathquill\">y_t</span> represents the calculated output at time t",
              "instructor_notes": ""
            },
            {
              "id": 464453,
              "key": "65b5c959-884f-45e5-9e0a-f69726d8288c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14beb2_screen-shot-2017-11-21-at-4.02.19-pm/screen-shot-2017-11-21-at-4.02.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/65b5c959-884f-45e5-9e0a-f69726d8288c",
              "caption": "_Equation 35_",
              "alt": "",
              "width": 150,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 457455,
              "key": "3e4bd888-34d4-4774-b5f0-b2ef08420358",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In **BPTT** we train the network at timestep t as well as take into account all of the previous timesteps. \n\nThe easiest way to explain the idea is to simply jump into an example.\n\nIn this example we will focus on the **BPTT** process for time step t=3. You will see that in order to adjust all three weight matrices, <span class=\"mathquill\">W_x, W_s </span> and <span class=\"mathquill\">W_y</span>, we need to consider timestep 3 as well as timestep 2 and timestep 1.\n ",
              "instructor_notes": ""
            },
            {
              "id": 457471,
              "key": "dda6fc4b-35d9-4cfa-8e32-d5d28c442cf1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As we are focusing on timestep t=3, the Loss function will be: <span class=\"mathquill\">E_3=(\\bar{d}_3-\\bar{y}_3)^2</span> ",
              "instructor_notes": ""
            },
            {
              "id": 465225,
              "key": "f0d34111-88ce-4ff7-83c4-19b897efdb36",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1c8721_screen-shot-2017-11-27-at-1.43.36-pm/screen-shot-2017-11-27-at-1.43.36-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f0d34111-88ce-4ff7-83c4-19b897efdb36",
              "caption": "_The Folded Model at Timestep 3_",
              "alt": "",
              "width": 130,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 457463,
              "key": "59934737-fea4-4cf5-b32b-11dd2e927483",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "To update each weight matrix, we need to find the partial derivatives of the Loss Function at time 3,  as a function of all of the weight matrices. We will modify each matrix using gradient descent while considering the previous timesteps.\n",
              "instructor_notes": ""
            },
            {
              "id": 465226,
              "key": "fe719988-2bea-46e4-9829-1b3f94f83ede",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1c87d9_screen-shot-2017-11-27-at-1.46.43-pm/screen-shot-2017-11-27-at-1.46.43-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/fe719988-2bea-46e4-9829-1b3f94f83ede",
              "caption": "_Gradient Considerations in the Folded Model_",
              "alt": "",
              "width": 130,
              "height": 300,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 419664,
          "key": "ab9dd7dd-26fd-4ff6-8127-2665c9d4177d",
          "title": "Backpropagation Through Time (part b)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ab9dd7dd-26fd-4ff6-8127-2665c9d4177d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 433529,
              "key": "c50b2904-22d6-4b66-b289-81fe4868944a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We will now unfold the model. You will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time  process. These multiplications stem from the chain rule and are easily visualized using this model.\n\n  \n",
              "instructor_notes": ""
            },
            {
              "id": 433530,
              "key": "c1caca95-a75f-4533-935a-badcf808e7a4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this video we will understand how to use Backpropagation Through Time (BPTT) when adjusting two weight matrices:\n\n- <span class=\"mathquill\">W_y</span> - the weight matrix connecting the state the output \n-  <span class=\"mathquill\">W_s</span> - the weight matrix connecting one state to the next state\n\n",
              "instructor_notes": ""
            },
            {
              "id": 474187,
              "key": "79a3dc44-285a-448a-b50d-3f5c82361472",
              "title": "20 RNN BPTT B V5 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "bUU9BEQw0IA",
                "china_cdn_id": "bUU9BEQw0IA.mp4"
              }
            },
            {
              "id": 457485,
              "key": "a3b94828-af91-4e6f-95d3-a910b2a4e246",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The unfolded model can be very helpful in visualizing the BPTT process.",
              "instructor_notes": ""
            },
            {
              "id": 465228,
              "key": "d8162ad5-64d5-49f7-a681-ee93ca8473e4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1c8a95_screen-shot-2017-11-27-at-1.58.01-pm/screen-shot-2017-11-27-at-1.58.01-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d8162ad5-64d5-49f7-a681-ee93ca8473e4",
              "caption": "_The Unfolded Model at timestep 3_",
              "alt": "",
              "width": 300,
              "height": 250,
              "instructor_notes": null
            },
            {
              "id": 457487,
              "key": "ef3b669e-5f2f-4cf9-8dd9-5a3c5e438631",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Gradient calculations needed to adjust  <span class=\"mathquill\">W_y</span>**\n\nThe partial derivative of the Loss Function with respect to <span class=\"mathquill\">W_y</span> is found by a simple one step chain rule:\n(Note that in this case we do not need to use BPTT. Visualization of the calculations path can be found in the video).",
              "instructor_notes": ""
            },
            {
              "id": 464439,
              "key": "783c4f9f-f10e-4c60-96bf-59d6d6e15a3e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14b8fb_screen-shot-2017-11-21-at-3.38.11-pm/screen-shot-2017-11-21-at-3.38.11-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/783c4f9f-f10e-4c60-96bf-59d6d6e15a3e",
              "caption": "_Equation 36_",
              "alt": "",
              "width": 140,
              "height": 80,
              "instructor_notes": null
            },
            {
              "id": 458802,
              "key": "17764a25-f73a-4336-94ed-9a16f23b7c72",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Generally speaking, we can consider multiple timesteps back, and not only 3 as in this example. For an arbitrary timestep N, the gradient calculation needed for adjusting  <span class=\"mathquill\">W_y</span>, is:\n",
              "instructor_notes": ""
            },
            {
              "id": 464463,
              "key": "014490c0-6678-4a1b-8863-170af82bb208",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c338_screen-shot-2017-11-21-at-4.21.41-pm/screen-shot-2017-11-21-at-4.21.41-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/014490c0-6678-4a1b-8863-170af82bb208",
              "caption": "_Equation 37_",
              "alt": "",
              "width": 150,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 457489,
              "key": "869fc831-7334-4846-9d65-82fc38cf449d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Gradient calculations needed to adjust  <span class=\"mathquill\">W_s</span>**\n\nWe still need to adjust  <span class=\"mathquill\">W_s</span> the weight matrix connecting one state to the next and <span class=\"mathquill\">W_x</span> the weight matrix connecting the input to the state. We will arbitrarily start with <span class=\"mathquill\">W_s</span>.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458603,
              "key": "58161f45-6e03-4e12-b321-57fff2f559c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "To understand the **BPTT** process, we can simplify the unfolded model. We will focus on the contributions of <span class=\"mathquill\">W_s</span>  to the output, the following way:",
              "instructor_notes": ""
            },
            {
              "id": 465229,
              "key": "207433ad-0852-4aee-89bf-6d1ce7de0e71",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1c8b04_screen-shot-2017-11-27-at-2.00.15-pm/screen-shot-2017-11-27-at-2.00.15-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/207433ad-0852-4aee-89bf-6d1ce7de0e71",
              "caption": "_Simplified Unfolded model for Adjusting Ws_",
              "alt": "",
              "width": 350,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 458604,
              "key": "52754928-3b93-4901-ab83-c905fb88e0df",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "When calculating the partial derivative of the Loss Function with respect to <span class=\"mathquill\">W_s</span>, we need to consider all of the states contributing to the output. In the case of this example it will be states <span class=\"mathquill\">\\bar{s_3}</span> which depends on its predecessor <span class=\"mathquill\">\\bar{s_2}</span> which depends on its predecessor <span class=\"mathquill\">\\bar{s_1}</span>, the first state.\n\nIn **BPTT** we will take into account every gradient stemming from each state, **accumulating** all of these contributions.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458605,
              "key": "84ec652e-b731-485c-97ff-551bfdff6860",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_3}</span> is the following :\n(Notice the use of the chain rule here. If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464442,
              "key": "c7cc377a-7ff2-4ffb-b509-6f7d8ffad719",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14ba05_screen-shot-2017-11-21-at-3.42.29-pm/screen-shot-2017-11-21-at-3.42.29-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c7cc377a-7ff2-4ffb-b509-6f7d8ffad719",
              "caption": "_Equation 38_",
              "alt": "",
              "width": 180,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 458608,
              "key": "5a6d8e52-c30f-400a-bad9-3bbeca71c732",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_2}</span> is the following :\n(Notice how the equation, derived by the chain rule, considers the contribution of <span class=\"mathquill\">\\bar{s_2}</span> to <span class=\"mathquill\">\\bar{s_3}</span> . If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464444,
              "key": "3adb4fd6-7c51-4b07-890b-2bccf6b5d74e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14ba6b_screen-shot-2017-11-21-at-3.44.15-pm/screen-shot-2017-11-21-at-3.44.15-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3adb4fd6-7c51-4b07-890b-2bccf6b5d74e",
              "caption": "_Equation 39_",
              "alt": "",
              "width": 180,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 458610,
              "key": "7728166f-2fd9-4923-8d28-b4536fddf3fb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_1}</span> is the following :\n(Notice how the equation, derived by the chain rule, considers the contribution of <span class=\"mathquill\">\\bar{s_1}</span> to <span class=\"mathquill\">\\bar{s_2}</span> and <span class=\"mathquill\">\\bar{s_3}</span> . If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464445,
              "key": "165072e2-e77a-4edc-9eec-271d45894f4d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14bac7_screen-shot-2017-11-21-at-3.45.50-pm/screen-shot-2017-11-21-at-3.45.50-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/165072e2-e77a-4edc-9eec-271d45894f4d",
              "caption": "_Equation 40_",
              "alt": "",
              "width": 210,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458613,
              "key": "7a9cfba9-926d-45ae-8b67-267c7759a1c1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "After considering the contributions from all three states: <span class=\"mathquill\">\\bar{s_3}</span> ,<span class=\"mathquill\">\\bar{s_2}</span> and <span class=\"mathquill\">\\bar{s_1}</span>, we will **accumulate** them to find the final gradient calculation. \n\nThe following equation is the gradient contributing to the adjustment of <span class=\"mathquill\">W_s</span> using **Backpropagation Through Time**:",
              "instructor_notes": ""
            },
            {
              "id": 464446,
              "key": "0aa58b43-769a-4355-b5e7-066f875b31ae",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14bb9a_screen-shot-2017-11-21-at-3.49.24-pm/screen-shot-2017-11-21-at-3.49.24-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0aa58b43-769a-4355-b5e7-066f875b31ae",
              "caption": "_Equation 41_",
              "alt": "",
              "width": 200,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 458635,
              "key": "744a69e5-de3c-420a-8b1d-205b5bdf62ea",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this example we had 3 time steps to consider, therefore we accumulated three partial derivative calculations. Generally speaking, we can consider multiple timesteps back. If you look closely at the three components of equation 41, you will notice a pattern. You will find that as we propagate a step back, we have an additional partial derivatives to consider in the chain rule. Mathematically this can be easily written in the following general equation for adjusting <span class=\"mathquill\">W_s</span> using **BPTT**:\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 464462,
              "key": "538d76e4-4598-46c2-81e8-6a17efd775fe",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c286_screen-shot-2017-11-21-at-4.17.35-pm/screen-shot-2017-11-21-at-4.17.35-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/538d76e4-4598-46c2-81e8-6a17efd775fe",
              "caption": "_Equation 42_",
              "alt": "",
              "width": 180,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 458637,
              "key": "e00a028d-1b7a-4529-ab89-9586d186d0ea",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice that Equation 6 considers a general setting of N steps back. As mentioned in this lesson, capturing relationships that span more than 8 to 10 steps back is practically impossible due to the vanishing gradient problem. We will talk about a solution to this problem in our LSTM section coming up soon.",
              "instructor_notes": ""
            },
            {
              "id": 433533,
              "key": "ddaa8f64-a8bf-4471-b981-3a926912bc24",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We still need to adjust  <span class=\"mathquill\">W_x</span>, the weight matrix connecting the input to the state. \n\nLet's take a small break. You can use this time to go over the **BPTT** process we've seen so far. Try to get yourself comfortable with the math. ",
              "instructor_notes": ""
            },
            {
              "id": 433538,
              "key": "4b9ed63c-6089-49bd-ad67-2a174ce0fd9d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Once you are feeling confident with the content of the video you just viewed, try to derive the calculations for adjusting the last matrix, <span class=\"mathquill\">W_x</span> by yourself. This is by no means a must, but if you feel that you are up for the challenge, go for it!  It will be interesting to compare your notes with ours.  \n\nIf you chose to take on the challenge, focus on simplifying the unfolded model, leaving only what you need for the calculations. Sketch the backpropagation \"path\", and step by step think of how the chain rule helps with the derivations here. Don't forget to** accumulate!**. \n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 419874,
          "key": "e8aae31b-d77c-416c-82d7-82774154950e",
          "title": "Backpropagation Through Time (part c)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e8aae31b-d77c-416c-82d7-82774154950e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 433542,
              "key": "9c9a75e4-90c0-401d-a730-c0cc57351dca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Last step! Adjusting  <span class=\"mathquill\">W_x</span>, the weight matrix connecting the input to the state. \n\nIf you took on the previous challenge of deriving the math by yourself first, sit back, fasten your seat belts and compare our notes to yours! Don't worry if you made mistakes, we all do. Your mistakes will help you learn what to avoid next time.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 474188,
              "key": "3d7548cb-5199-4a9a-8415-58b47bbbeedf",
              "title": "21 RNN BPTT C V7 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "uBy_eIJDD1M",
                "china_cdn_id": "uBy_eIJDD1M.mp4"
              }
            },
            {
              "id": 458599,
              "key": "921693bf-a0ab-40be-ac71-7d7ebbff7199",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Gradient calculations needed to adjust  <span class=\"mathquill\">W_x</span>**\n\nTo further understand the **BPTT** process, we will simplify the unfolded model again. This time the focus will be on the contributions of <span class=\"mathquill\">W_x</span>  to the output, the following way:",
              "instructor_notes": ""
            },
            {
              "id": 458621,
              "key": "815d60c4-4292-4917-a37f-f924c0221e3e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a0396bd_screen-shot-2017-11-08-at-3.43.34-pm/screen-shot-2017-11-08-at-3.43.34-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/815d60c4-4292-4917-a37f-f924c0221e3e",
              "caption": "_Simplified Unfolded model for Adjusting Wx_",
              "alt": "",
              "width": 300,
              "height": 250,
              "instructor_notes": null
            },
            {
              "id": 458622,
              "key": "c2b1a4b4-c112-425d-a6e2-742431af0cdf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "When calculating the partial derivative of the Loss Function with respect to to <span class=\"mathquill\">W_x</span> we need to consider, again, all of the states contributing to the output. As we saw before, in the case of this example it will be states <span class=\"mathquill\">\\bar{s_3}</span> which depend on its predecessor <span class=\"mathquill\">\\bar{s_2}</span> which depends on its predecessor <span class=\"mathquill\">\\bar{s_1}</span>, the first state.\n\nAs we mentioned previously, in **BPTT** we will take into account each gradient stemming from each state, **accumulating** all of the contributions.",
              "instructor_notes": ""
            },
            {
              "id": 458624,
              "key": "a6ca4fde-011f-4248-b2ba-2989438a02fc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_3}</span> is the following :\n(Notice the use of the chain rule here. If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464455,
              "key": "5ff6f662-e6c1-452e-aa57-1ea197c33c76",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14bfd1_screen-shot-2017-11-21-at-4.07.21-pm/screen-shot-2017-11-21-at-4.07.21-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5ff6f662-e6c1-452e-aa57-1ea197c33c76",
              "caption": "_Equation 43_",
              "alt": "",
              "width": 180,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458627,
              "key": "d5d6359d-543a-4f65-aade-6662ba07b628",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_2}</span> is the following :\n(Notice how the equation, derived by the chain rule, considers the contribution of <span class=\"mathquill\">\\bar{s_2}</span> to <span class=\"mathquill\">\\bar{s_3}</span> . If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464456,
              "key": "6745e673-112b-42b3-8043-9161b2a18c44",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c034_screen-shot-2017-11-21-at-4.08.59-pm/screen-shot-2017-11-21-at-4.08.59-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6745e673-112b-42b3-8043-9161b2a18c44",
              "caption": "_Equation 44_",
              "alt": "",
              "width": 200,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458629,
              "key": "7da37694-0e4b-483e-93f1-48ceefd20beb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "- At timestep t=3, the contribution to the gradient stemming from <span class=\"mathquill\">\\bar{s_1}</span> is the following :\n(Notice how the equation, derived by the chain rule, considers the contribution of <span class=\"mathquill\">\\bar{s_1}</span> to <span class=\"mathquill\">\\bar{s_2}</span> and <span class=\"mathquill\">\\bar{s_3}</span> . If you need, go back to the video to visualize the calculation path).\n",
              "instructor_notes": ""
            },
            {
              "id": 464458,
              "key": "a2ce0aa2-d812-44d5-93b9-9a90ee0ceb35",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c0a8_screen-shot-2017-11-21-at-4.10.56-pm/screen-shot-2017-11-21-at-4.10.56-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a2ce0aa2-d812-44d5-93b9-9a90ee0ceb35",
              "caption": "_Equation 45_",
              "alt": "",
              "width": 210,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458631,
              "key": "14f574d2-3c5a-4e34-b75e-6e7927178baf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nAfter considering the contributions from all three states: <span class=\"mathquill\">\\bar{s_3}</span> ,<span class=\"mathquill\">\\bar{s_2}</span> and <span class=\"mathquill\">\\bar{s_1}</span>, we will **accumulate** them to find the final gradient calculation. \n\nThe following equation is the gradient contributing to the adjustment of <span class=\"mathquill\">W_x</span> using **Backpropagation Through Time**:",
              "instructor_notes": ""
            },
            {
              "id": 464460,
              "key": "db84ca1b-0a63-4c9b-ba6e-f3a31feab1f7",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c18c_screen-shot-2017-11-21-at-4.14.45-pm/screen-shot-2017-11-21-at-4.14.45-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/db84ca1b-0a63-4c9b-ba6e-f3a31feab1f7",
              "caption": "_Equation 46_\n",
              "alt": "",
              "width": 200,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 458639,
              "key": "9f23b114-6949-4cda-8dda-6859b2d43f49",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As mentioned before, in this example we had 3 time steps to consider, therefore we accumulated three partial derivative calculations. Generally speaking, we can consider multiple timesteps back. If you look closely at equations 1, 2 and 3, you will notice a pattern again. You will find that as we propagate a step back, we have an additional partial derivatives to consider in the chain rule. Mathematically this can be easily written in the following general equation for adjusting <span class=\"mathquill\">W_x</span> using **BPTT**:\n",
              "instructor_notes": ""
            },
            {
              "id": 464461,
              "key": "14ae01e3-4f39-4aa2-a2c2-702632ec03b6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c244_screen-shot-2017-11-21-at-4.17.19-pm/screen-shot-2017-11-21-at-4.17.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/14ae01e3-4f39-4aa2-a2c2-702632ec03b6",
              "caption": "_Equation 47_",
              "alt": "",
              "width": 210,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458633,
              "key": "aaef1abd-138f-45e2-b35f-563be207b37d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice the similarities between the calculations of <span class=\"mathquill\">\\frac{\\partial{E_3} }{\\partial W_s}</span>   and  <span class=\"mathquill\">\\frac{\\partial{E_3} }{\\partial W_x}</span>. Hopefully after understanding the calculation process of  <span class=\"mathquill\">\\frac{\\partial{E_3} }{\\partial W_s}</span>, understanding that of  <span class=\"mathquill\">\\frac{\\partial{E_3} }{\\partial W_x}</span> was straight forward.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 413098,
          "key": "9528e9b5-7b59-49be-bbbc-fd3f3e697e55",
          "title": "BPTT Quiz 1",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9528e9b5-7b59-49be-bbbc-fd3f3e697e55",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 466007,
              "key": "4e2074ac-9b09-490b-830a-40f3bc78f3a5",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1f4e94_screen-shot-2017-11-29-at-3.08.28-pm/screen-shot-2017-11-29-at-3.08.28-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4e2074ac-9b09-490b-830a-40f3bc78f3a5",
              "caption": "_A folded RNN model_",
              "alt": "",
              "width": 150,
              "height": 370,
              "instructor_notes": null
            },
            {
              "id": 466012,
              "key": "be1c2e50-ac7d-4a33-a84f-ee1782d12a72",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "be1c2e50-ac7d-4a33-a84f-ee1782d12a72",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Consider the above folded RNN Model.  Both states **S** and **Z** have multiple neurons in each layer.\nThe mathematical derivation of state **Z** at time t is:",
                "answers": [
                  {
                    "id": "a1512001229376",
                    "text": "Equation A",
                    "is_correct": false
                  },
                  {
                    "id": "a1512001620390",
                    "text": "Equation B",
                    "is_correct": false
                  },
                  {
                    "id": "a1512001621601",
                    "text": "Equation C",
                    "is_correct": false
                  },
                  {
                    "id": "a1512001622375",
                    "text": "Equation D",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 466031,
              "key": "80933965-510b-4023-9ea9-dfa5606cef8a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1f6010_screen-shot-2017-11-29-at-5.33.53-pm/screen-shot-2017-11-29-at-5.33.53-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/80933965-510b-4023-9ea9-dfa5606cef8a",
              "caption": "",
              "alt": "",
              "width": 350,
              "height": 280,
              "instructor_notes": null
            },
            {
              "id": 467636,
              "key": "bb6b4e25-ca5b-4998-8057-fab696198945",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Solution \n\n <span class=\"mathquill\">\\bar{z}</span> and <span class=\"mathquill\">\\bar{s}</span> are vectors, as we indicate that they have multiple neurons in each layer. Using this logic we can understand that equations A and C are incorrect. Since <span class=\"mathquill\">w_2</span> connects the hidden state <span class=\"mathquill\">\\bar{z}</span> to itself, we know that we need to consider the previous timestep here. Therefore only equation D is the correct one.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 467637,
          "key": "21bae9d9-29eb-49b9-9d86-3c1fccdf31ce",
          "title": "BPTT Quiz 2",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "21bae9d9-29eb-49b9-9d86-3c1fccdf31ce",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 467639,
              "key": "ec7b9630-ceee-403c-9fb6-19e36fc8e653",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a24fab9_screen-shot-2017-12-03-at-11.34.41-pm/screen-shot-2017-12-03-at-11.34.41-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ec7b9630-ceee-403c-9fb6-19e36fc8e653",
              "caption": "_A folded RNN model_",
              "alt": "",
              "width": 150,
              "height": 370,
              "instructor_notes": null
            },
            {
              "id": 467638,
              "key": "f6f9ae3e-b0a5-4f08-a4cd-421b18cbe14f",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "f6f9ae3e-b0a5-4f08-a4cd-421b18cbe14f",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol **E**. What is the update rule of weight matrix V1 at time t, over a single timestep ? \n",
                "answers": [
                  {
                    "id": "a1512372686853",
                    "text": "Equation A",
                    "is_correct": false
                  },
                  {
                    "id": "a1512372691106",
                    "text": "Equation B",
                    "is_correct": true
                  },
                  {
                    "id": "a1512372692271",
                    "text": "Equation C",
                    "is_correct": false
                  },
                  {
                    "id": "a1512372693985",
                    "text": "Equation D",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 467640,
              "key": "bc4b58e0-8722-467f-983d-79944095be4f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a24fb21_screen-shot-2017-12-03-at-11.36.39-pm/screen-shot-2017-12-03-at-11.36.39-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/bc4b58e0-8722-467f-983d-79944095be4f",
              "caption": "",
              "alt": "",
              "width": 350,
              "height": 280,
              "instructor_notes": null
            },
            {
              "id": 467641,
              "key": "5b8aadf6-cce1-4c12-986b-1300be1fe7e3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Solution \n\nEquation B Is the only equation with the correct derivation of the chain rule with the proper use of the learning rate.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 467642,
          "key": "0f22f887-59e5-4bac-a2e9-d85c4f4b1c69",
          "title": "BPTT Quiz 3",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0f22f887-59e5-4bac-a2e9-d85c4f4b1c69",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 467643,
              "key": "8db02bcc-2a5c-4f60-baf2-9d504932479b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a24fe85_screen-shot-2017-12-03-at-11.34.41-pm/screen-shot-2017-12-03-at-11.34.41-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8db02bcc-2a5c-4f60-baf2-9d504932479b",
              "caption": "_A folded RNN model_",
              "alt": "",
              "width": 150,
              "height": 370,
              "instructor_notes": null
            },
            {
              "id": 467644,
              "key": "7378703a-8fd9-4d46-ad7f-e58a23b5f2ab",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "7378703a-8fd9-4d46-ad7f-e58a23b5f2ab",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Lets look at the same folded model again (displayed above). Assume that the error is noted by the symbol **E**. What is the update rule of weight matrix U at time t+1 (over 2 timesteps) ? \nHint: Use the unfolded model for a better visualization.",
                "answers": [
                  {
                    "id": "a1512374013770",
                    "text": "Equation A",
                    "is_correct": false
                  },
                  {
                    "id": "a1512374168777",
                    "text": "Equation B",
                    "is_correct": false
                  },
                  {
                    "id": "a1512374170835",
                    "text": "Equation C",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 467970,
              "key": "75c699df-12f6-481a-bdce-1ada5a120cfc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25ac0d_screen-shot-2017-12-04-at-12.10.02-pm/screen-shot-2017-12-04-at-12.10.02-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/75c699df-12f6-481a-bdce-1ada5a120cfc",
              "caption": "",
              "alt": "",
              "width": 550,
              "height": 550,
              "instructor_notes": null
            },
            {
              "id": 467645,
              "key": "f1e10f0d-b14a-4f67-aab4-0cb2a8932d4f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Solution \n\nTo understand how to update weight matrix U, we will need to unfold the model in time. We will unfold the model over two time steps, as we need to look only time t and time t+1. The following three pictures will help you understand the **three** paths we need to consider. Notice that we have two hidden layers that serve as memory elements, so this case will be different than the one we saw in the video, but the idea is the same. We will use **BPTT** while applying the chain rule.",
              "instructor_notes": ""
            },
            {
              "id": 467884,
              "key": "c4740ec5-5b19-43bd-86f2-29b80be8183a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a259f26_screen-shot-2017-12-04-at-11.16.19-am/screen-shot-2017-12-04-at-11.16.19-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c4740ec5-5b19-43bd-86f2-29b80be8183a",
              "caption": "_The first path to consider_",
              "alt": "",
              "width": 280,
              "height": 350,
              "instructor_notes": null
            },
            {
              "id": 467966,
              "key": "41f2e955-1238-4751-b73e-e038dbb97820",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following is the equation we derive using the first path:",
              "instructor_notes": ""
            },
            {
              "id": 467958,
              "key": "0987adfc-1dc3-4cd3-9f73-fbb501d399b6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a6b2_screen-shot-2017-12-04-at-11.37.27-am/screen-shot-2017-12-04-at-11.37.27-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0987adfc-1dc3-4cd3-9f73-fbb501d399b6",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 476091,
              "key": "4b992c55-b68e-40f6-bff1-3ce1d559f924",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "______________________________________________________________________________________________________",
              "instructor_notes": ""
            },
            {
              "id": 467900,
              "key": "2573a2b1-ab92-439b-9787-2f19573b1831",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a02b_screen-shot-2017-12-04-at-11.14.30-am/screen-shot-2017-12-04-at-11.14.30-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/2573a2b1-ab92-439b-9787-2f19573b1831",
              "caption": "_The second path to consider_",
              "alt": "",
              "width": 280,
              "height": 350,
              "instructor_notes": null
            },
            {
              "id": 467967,
              "key": "4e483600-12a1-40a9-8d3a-950faffede44",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following is the equation we derive using the second path:",
              "instructor_notes": ""
            },
            {
              "id": 467959,
              "key": "884243b1-ce36-4c28-87f4-790704b73fb5",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a6c3_screen-shot-2017-12-04-at-11.42.56-am/screen-shot-2017-12-04-at-11.42.56-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/884243b1-ce36-4c28-87f4-790704b73fb5",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 476092,
              "key": "7a6b2439-2772-4a2b-b451-55f9cb424968",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "______________________________________________________________________________________________________",
              "instructor_notes": ""
            },
            {
              "id": 467905,
              "key": "08dcfb7e-c1bd-4602-9e2f-97c24c3cf010",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a091_screen-shot-2017-12-04-at-11.12.31-am/screen-shot-2017-12-04-at-11.12.31-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/08dcfb7e-c1bd-4602-9e2f-97c24c3cf010",
              "caption": "_The third path to consider_",
              "alt": "",
              "width": 280,
              "height": 350,
              "instructor_notes": null
            },
            {
              "id": 467968,
              "key": "b5e6a4b5-60b8-49e6-af8a-d8a5dfd59cd2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The following is the equation we derive using the third path:",
              "instructor_notes": ""
            },
            {
              "id": 467962,
              "key": "5309c0e1-04eb-45f2-bb27-116a55d72853",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a730_screen-shot-2017-12-04-at-11.50.40-am/screen-shot-2017-12-04-at-11.50.40-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5309c0e1-04eb-45f2-bb27-116a55d72853",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 476093,
              "key": "8845f57b-88b6-4df0-b9c9-b0b195b9b791",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "______________________________________________________________________________________________________",
              "instructor_notes": ""
            },
            {
              "id": 467969,
              "key": "e0c2111c-0950-48fa-8212-1c320793f19a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Finally, after considering all three paths, we can derive the correct equation for the purposes of updating  weight matrix U, using BPTT:",
              "instructor_notes": ""
            },
            {
              "id": 467963,
              "key": "c2a705ff-c37f-407c-8fed-9315a0b904ec",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25a757_screen-shot-2017-12-04-at-11.48.22-am/screen-shot-2017-12-04-at-11.48.22-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c2a705ff-c37f-407c-8fed-9315a0b904ec",
              "caption": "_The final answer for BPTT Quiz 3_",
              "alt": "",
              "width": 400,
              "height": 300,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 466789,
          "key": "593835fd-09a7-44a3-a1fc-56dadb02b2ca",
          "title": "Some more math",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "593835fd-09a7-44a3-a1fc-56dadb02b2ca",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 466790,
              "key": "4cf3dd64-748d-4cbf-8c69-ab93a4779254",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "This section is given as bonus material and is not mandatory. If you are curious how we derived the final accumulative equation for BPTT, this section will help you out.\n\nIn the previous videos, we talked about **Backpropagation Through Time**. We used a lot of partial derivatives, accumulating the contributions to the change in the error from each state.  Remember?\nWhen we needed a general scheme for the BPTT, I simply displayed the equation without giving you further explanations.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 466791,
              "key": "b02e4006-0d2d-4686-9317-62f2af185be1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As a reminder, the following two equations were derived when adjusting the weights of matrix <span class=\"mathquill\">W_s</span> and matrix <span class=\"mathquill\">W_x</span>:",
              "instructor_notes": ""
            },
            {
              "id": 466799,
              "key": "c06e3c30-7bc2-4b94-8be9-7225ce8f93a4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a20a572_screen-shot-2017-11-30-at-4.40.57-pm/screen-shot-2017-11-30-at-4.40.57-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c06e3c30-7bc2-4b94-8be9-7225ce8f93a4",
              "caption": "_Equation 48: BPTT calculations for the purpose of adjusting Ws_",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 466800,
              "key": "94faa676-8aaa-4de1-ba5f-e1446c89d707",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a20a581_screen-shot-2017-11-30-at-4.41.08-pm/screen-shot-2017-11-30-at-4.41.08-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/94faa676-8aaa-4de1-ba5f-e1446c89d707",
              "caption": "_Equation 49: BPTT calculations for the purpose of adjusting Wx_",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468438,
              "key": "0387b475-3433-4dbd-8f65-c4a4515fcbed",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "To generalize the case, we will avoid proving equation 48 or 49, and will focus on a general framework.\nLet's look at the following sketch, presenting a portion of a network:",
              "instructor_notes": ""
            },
            {
              "id": 468450,
              "key": "72d5ee61-070f-4ebc-9684-643a1d0f114a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25c6a6_screen-shot-2017-12-04-at-2.04.54-pm/screen-shot-2017-12-04-at-2.04.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/72d5ee61-070f-4ebc-9684-643a1d0f114a",
              "caption": "",
              "alt": "",
              "width": 450,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468447,
              "key": "42e603a9-23ff-468d-96c6-c87896069e1f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the picture above, we have four states, starting with <span class=\"mathquill\">s_t</span>.\nWe will initially consider the three weight matrices <span class=\"mathquill\">W_1</span>,<span class=\"mathquill\">W_2</span> and <span class=\"mathquill\">W_3</span> as three different matrices.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 468525,
              "key": "1e1de823-239b-4bb1-9393-4a1222b9b3e0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Using the chain rule we can derive the following three equations:",
              "instructor_notes": ""
            },
            {
              "id": 495699,
              "key": "d621729a-81e6-40b7-9315-328e2ddaf304",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/January/5a4c07b0_screen-shot-2018-01-02-at-2.27.51-pm/screen-shot-2018-01-02-at-2.27.51-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d621729a-81e6-40b7-9315-328e2ddaf304",
              "caption": "_Equation 50  (Equation set)_",
              "alt": "",
              "width": 320,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468542,
              "key": "4937cc48-ff08-41f9-b796-fb204a5ca01e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In **Backpropagation Through Time** we accumulate the contributions, therefore:",
              "instructor_notes": ""
            },
            {
              "id": 468544,
              "key": "c481714f-a880-41ed-9583-b22775de1de5",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a25e048_screen-shot-2017-12-04-at-3.54.17-pm/screen-shot-2017-12-04-at-3.54.17-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c481714f-a880-41ed-9583-b22775de1de5",
              "caption": "_Equation 51_",
              "alt": "",
              "width": 300,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468730,
              "key": "052bfa3c-48cf-4c6f-ae08-5ca06f1fd2a1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Since this network is displayed as _unfolded in time_, we understand that the weight matrices connecting each of the states are identical. Therefore:\n\n<center><span class=\"mathquill\">W_1</span>=<span class=\"mathquill\">W_2</span>=<span class=\"mathquill\">W_3</span>\n\nLets simply call it weight matrix <span class=\"mathquill\">W</span>. Therefore:\n\n<span class=\"mathquill\">W_1</span>=<span class=\"mathquill\">W_2</span>=<span class=\"mathquill\">W_3</span>=<span class=\"mathquill\">W</span> \n\n_Equation 52_\n\n",
              "instructor_notes": ""
            },
            {
              "id": 468732,
              "key": "f84765a9-9c67-4852-a532-fd8bd962acbe",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "From _equation 52_, _equation 51_ and the _set of equations 50_ we derive that:",
              "instructor_notes": ""
            },
            {
              "id": 468740,
              "key": "9e29bb2e-af26-474a-9ab6-4a426f9e3b81",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a26499a_screen-shot-2017-12-04-at-11.23.49-pm/screen-shot-2017-12-04-at-11.23.49-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9e29bb2e-af26-474a-9ab6-4a426f9e3b81",
              "caption": "_Equation 53_",
              "alt": "",
              "width": 350,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 468733,
              "key": "9ab3da4f-8679-408c-a3b1-0b2bce07e25f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "_Equation 53_ summarizes the mathematical procedure of BPTT and can be simply written as:\n",
              "instructor_notes": ""
            },
            {
              "id": 468745,
              "key": "269f4db2-6610-4162-84ea-0f86c45b7160",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a264f63_screen-shot-2017-12-04-at-11.48.08-pm/screen-shot-2017-12-04-at-11.48.08-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/269f4db2-6610-4162-84ea-0f86c45b7160",
              "caption": "_Equation 54_",
              "alt": "",
              "width": 260,
              "height": 150,
              "instructor_notes": null
            },
            {
              "id": 468743,
              "key": "a58511b9-7c4a-4362-bef3-612819c6eca4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Notice that for <span class=\"mathquill\">i=t+1</span>, we derive the following:",
              "instructor_notes": ""
            },
            {
              "id": 468746,
              "key": "52588eee-01d9-4a3a-89f8-f3c45114e3dd",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a265049_screen-shot-2017-12-04-at-11.51.54-pm/screen-shot-2017-12-04-at-11.51.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/52588eee-01d9-4a3a-89f8-f3c45114e3dd",
              "caption": "_Equation 55_",
              "alt": "",
              "width": 260,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468747,
              "key": "578d096e-64a8-47a5-9406-bb6379f455e1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "With the use of the chain rule we can derive the following equation (displayed in _set of equations 50_).",
              "instructor_notes": ""
            },
            {
              "id": 468748,
              "key": "bddd3653-c425-49e7-a299-5d97a557bbc0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a2650dc_screen-shot-2017-12-04-at-11.54.48-pm/screen-shot-2017-12-04-at-11.54.48-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/bddd3653-c425-49e7-a299-5d97a557bbc0",
              "caption": "_Equation 56_",
              "alt": "",
              "width": 280,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 468749,
              "key": "69e2d845-0084-4f0b-a4e3-e73a01d57294",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "A general derivation of the BPTT calculation can be displayed the following way:",
              "instructor_notes": ""
            },
            {
              "id": 468750,
              "key": "e470c3d1-3ec4-4961-9f7c-913fc019872a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a265320_screen-shot-2017-12-05-at-12.04.21-am/screen-shot-2017-12-05-at-12.04.21-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e470c3d1-3ec4-4961-9f7c-913fc019872a",
              "caption": "_Equation 57_",
              "alt": "",
              "width": 280,
              "height": 100,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 420972,
          "key": "d4a9bba6-9784-441e-b021-5510a14b46d4",
          "title": "RNN Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d4a9bba6-9784-441e-b021-5510a14b46d4",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 458785,
              "key": "e5f5aca8-9160-4fb4-9d30-d2bdc6c50902",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Let's summarize what we have seen so far:\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 543014,
              "key": "9516f782-623b-4ac3-a61f-3857c4c973db",
              "title": "RNN Summary",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "nXP0oGGRrO8",
                "china_cdn_id": "nXP0oGGRrO8.mp4"
              }
            },
            {
              "id": 458786,
              "key": "100f7757-2a6d-4514-9c62-31ecd5681db9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As you have seen, in RNNs the current state depends on the input as well as the previous states, with the use of an activation function.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458788,
              "key": "94980147-1511-4eab-a80c-310363fa073a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a04ea8c_screen-shot-2017-11-09-at-3.53.12-pm/screen-shot-2017-11-09-at-3.53.12-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/94980147-1511-4eab-a80c-310363fa073a",
              "caption": "_Equation 56_",
              "alt": "",
              "width": 200,
              "height": 120,
              "instructor_notes": null
            },
            {
              "id": 458787,
              "key": "714781bc-e052-4ead-b487-9ae8f80e90c9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The current output is a simple linear combination of the current state elements with the corresponding weight matrix. \n\n<span class=\"mathquill\">\\bar{y}_t=\\bar{s}_t W_y</span> (without the use of an activation function)\n\nor \n\n<span class=\"mathquill\">\\bar{y}_t=\\sigma(\\bar{s}_t W_y)</span> (with the use of an activation function)\n\n\n_Equation 57_",
              "instructor_notes": ""
            },
            {
              "id": 458794,
              "key": "8fd973d6-ea0a-4622-8bb3-ac7ea89c211d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We can represent the recurrent network with the use of a folded model or an unfolded model:",
              "instructor_notes": ""
            },
            {
              "id": 465357,
              "key": "e8a48952-497a-4321-9043-709f2fe91ff1",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1c955f_screen-shot-2017-11-27-at-2.44.11-pm/screen-shot-2017-11-27-at-2.44.11-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e8a48952-497a-4321-9043-709f2fe91ff1",
              "caption": "_The RNN  Folded Model_",
              "alt": "",
              "width": 180,
              "height": 280,
              "instructor_notes": null
            },
            {
              "id": 465393,
              "key": "8e73ec00-784d-40ab-87aa-459567e3207f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1ca463_screen-shot-2017-11-27-at-3.48.31-pm/screen-shot-2017-11-27-at-3.48.31-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8e73ec00-784d-40ab-87aa-459567e3207f",
              "caption": "_The RNN Unfolded Model_",
              "alt": "",
              "width": 310,
              "height": 160,
              "instructor_notes": null
            },
            {
              "id": 458804,
              "key": "80234a7b-4b82-4bb6-8eb1-f6ad7aa08373",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the case of a single hidden (state) layer, we will have three weight matrices to consider. Here we use the following notations:\n\n<span class=\"mathquill\">W_x</span> -  represents the weight matrix connecting the inputs to the state layer.\n\n<span class=\"mathquill\">W_y</span> - represents the weight matrix connecting the state to the output.\n\n<span class=\"mathquill\">W_s</span> - represents the weight matrix connecting the state from the previous timestep to the state in the following timestep.\n",
              "instructor_notes": ""
            },
            {
              "id": 458806,
              "key": "8a647e58-914c-48bf-b823-1902c4a060e7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The gradient calculations for the purpose of adjusting the weight matrices are the following:",
              "instructor_notes": ""
            },
            {
              "id": 464464,
              "key": "3b14ac83-0d97-43be-a956-78d65f48e59c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c386_screen-shot-2017-11-21-at-4.21.41-pm/screen-shot-2017-11-21-at-4.21.41-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3b14ac83-0d97-43be-a956-78d65f48e59c",
              "caption": "_Equation 58_",
              "alt": "",
              "width": 150,
              "height": 90,
              "instructor_notes": null
            },
            {
              "id": 464466,
              "key": "e1038952-b11c-4755-9dd8-d6fe8159700c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c411_screen-shot-2017-11-21-at-4.17.35-pm/screen-shot-2017-11-21-at-4.17.35-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e1038952-b11c-4755-9dd8-d6fe8159700c",
              "caption": "_Equation 59_",
              "alt": "",
              "width": 180,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 464467,
              "key": "e2c5c302-7112-4fdf-a8fd-aaa9f9842a8e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a14c442_screen-shot-2017-11-21-at-4.17.19-pm/screen-shot-2017-11-21-at-4.17.19-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e2c5c302-7112-4fdf-a8fd-aaa9f9842a8e",
              "caption": "_Equation 60_",
              "alt": "",
              "width": 180,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458811,
              "key": "a4f4bce9-e9dd-48b3-b4d1-838014589e2e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In equations _51_ and _52_ we used **Backpropagation Through Time (BPTT)** where we accumulate all of the contributions from previous timesteps.\n",
              "instructor_notes": ""
            },
            {
              "id": 458813,
              "key": "1b3bfdd8-18d7-46c0-92f9-e2ba535bfcd7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "When training RNNs using BPTT, we can choose to use mini-batches, where we update the weights in batches periodically (as opposed to once every inputs sample). We calculate the gradient for each step but do not update the weights right away. Instead, we update the weights once every fixed number of steps. This helps reduce the complexity of the training process and helps remove noise from the weight updates.\n\nThe following is the equation used for **Mini-Batch Training Using Gradient Descent**:\n(where <span class=\"mathquill\">\\delta_{ij}</span> represents the gradient calculated once every inputs sample and M represents the number of gradients we accumulate in the process).",
              "instructor_notes": ""
            },
            {
              "id": 458819,
              "key": "1a764540-1dfc-4519-937b-d5c273a32b2e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a05088c_screen-shot-2017-11-09-at-6.01.16-pm/screen-shot-2017-11-09-at-6.01.16-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/1a764540-1dfc-4519-937b-d5c273a32b2e",
              "caption": "_Equation 61_",
              "alt": "",
              "width": 180,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 458817,
              "key": "e4a75978-fc77-452a-a5ad-9a2bcc349e07",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If we backpropagate more than ~10 timesteps, the gradient will become too small. This phenomena is known as  the **vanishing gradient problem**  where the contribution of information decays geometrically over time. Therefore temporal dependencies that span many time steps will effectively be discarded by the network. **Long Short-Term Memory (LSTM)** cells were designed to specifically solve this problem.\n\n\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 458818,
              "key": "b31aa8ba-6e15-46bb-aa16-3315db87972b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In RNNs we can also have the opposite problem, called the **exploding gradient** problem, in which the value of the gradient grows uncontrollably. A simple solution for the exploding gradient problem is **Gradient Clipping**.",
              "instructor_notes": ""
            },
            {
              "id": 420994,
              "key": "53528bbb-4d73-4bed-a607-8c83235cce3d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "More information about Gradient Clipping can be found [here](https://arxiv.org/abs/1211.5063).\n\nYou can concentrate on Algorithm 1 which describes the gradient clipping idea in simplicity.\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 420985,
          "key": "6cd02f1c-246b-4487-8b29-8ee3ee4b1061",
          "title": "From RNN to LSTM",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6cd02f1c-246b-4487-8b29-8ee3ee4b1061",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 461496,
              "key": "9ea47a15-42b0-436c-a8f0-8efeb342de28",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Before we take a close look at the **Long Short-Term Memory (LSTM)** cell, let's take a look at the following video:",
              "instructor_notes": ""
            },
            {
              "id": 474190,
              "key": "071a57e7-828b-44e4-adb4-d1a19d7850a4",
              "title": "23 From RNNs To LSTMs V4 Final",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "MsqybcWmzGY",
                "china_cdn_id": "MsqybcWmzGY.mp4"
              }
            },
            {
              "id": 461494,
              "key": "d8eeb1b1-770a-4f70-a68f-7c96687a3c63",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Long Short-Term Memory Cells**, [(LSTM)](http://www.bioinf.jku.at/publications/older/2604.pdf) give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies. They were proposed in 1997 by [Sepp Hochreiter](https://en.wikipedia.org/wiki/Sepp_Hochreiter) and  [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/)\n",
              "instructor_notes": ""
            },
            {
              "id": 461499,
              "key": "a6d5748d-7863-495c-b23c-7b4ae6ee11a6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If we take a close look at the RNN neuron, we can see that we have simple linear combinations (with or without the use of an activation function). We can also see that we have a single addition.\n\nZooming in on the neuron, we can graphically see this in the following configuration:",
              "instructor_notes": ""
            },
            {
              "id": 465392,
              "key": "eb713793-1eaa-4208-82a3-8e2234e60c34",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1ca3f0_screen-shot-2017-11-27-at-3.46.35-pm/screen-shot-2017-11-27-at-3.46.35-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/eb713793-1eaa-4208-82a3-8e2234e60c34",
              "caption": "_Closeup Of The RNN Neuron_",
              "alt": "",
              "width": 250,
              "height": 250,
              "instructor_notes": null
            },
            {
              "id": 461505,
              "key": "849ee21d-d9de-4ad5-9687-659f948b54d5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The **LSTM** cell is a bit more complicated. If we zoom in on the cell, we can see that the mathematical configuration is the following:",
              "instructor_notes": ""
            },
            {
              "id": 465390,
              "key": "5a977d2b-55fa-429d-8bcb-3a632c03a382",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/November/5a1ca36a_screen-shot-2017-11-27-at-3.44.20-pm/screen-shot-2017-11-27-at-3.44.20-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5a977d2b-55fa-429d-8bcb-3a632c03a382",
              "caption": "_Closeup Of the LSTM Cell_",
              "alt": "",
              "width": 300,
              "height": 250,
              "instructor_notes": null
            },
            {
              "id": 461508,
              "key": "c2b95076-fb27-45b2-b934-e2be3907f69c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The LSTM cell allows a recurrent system to learn over many time steps without the fear of losing information due to the vanishing gradient problem. It is fully differentiable, therefore gives us the option of easily using backpropagation when updating the weights.\n\nIn our next set of videos Luis will help you understand LSTMs further.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 403179,
          "key": "80870483-e403-465c-9931-01d6fb175dee",
          "title": "Wrap Up",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "80870483-e403-465c-9931-01d6fb175dee",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 474543,
              "key": "3c356690-0bcd-4aba-88d8-3cf718dd2cb3",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/December/5a2e13e6_screen-shot-2017-12-10-at-9.12.16-pm/screen-shot-2017-12-10-at-9.12.16-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3c356690-0bcd-4aba-88d8-3cf718dd2cb3",
              "caption": "",
              "alt": "",
              "width": 500,
              "height": 500,
              "instructor_notes": null
            },
            {
              "id": 430428,
              "key": "bbf4301e-67f4-450d-ae12-0142549606d3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "\nIn this lesson we reviewed the FFNN and its training process. We dove into RNNs, understanding the motivation behind them, when they are used, and how are they designed. We also learned how to train them using BPTT. We reviewed current applications, such as machine translation, and gave intuition to why so many  other applications use RNNs, and in particular LSTMs. \n\nNext, Luis will guide you through the *LSTM* architecture. \n\n",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}